# Spark Memory Management: Execution vs Storage è¯¦ç»†åˆ†æ

## ğŸ“Š åŸºç¡€å‚æ•°

### æ•°æ®è§„æ¨¡
- **æ•°æ®æ€»é‡**: 200GB + 50GB (Parquetå‹ç¼©å)
- **è§£å‹åæ•°æ®**: 1000GB + 250GB = 1250GB
- **Shuffleè¾“å‡º**: 600GB (LZ4å‹ç¼©åï¼Œè†¨èƒ€å› å­0.5)
- **Joinç»“æœå› å­**: 1.2x
- **Shuffleåˆ†åŒºæ•°**: 500
- **æ¯ä¸ªExecutorå¹¶å‘ä»»åŠ¡æ•°**: 3

### æ¯ä¸ªTaskæ•°æ®è´Ÿè½½
- **è¾“å…¥æ•°æ®**: 1250GB Ã· 500 = 2.5GB/Task
- **Shuffleè¾“å‡º**: 600GB Ã· 500 = 1.2GB/Task

## ğŸ—ï¸ Sparkå†…å­˜ç®¡ç†æ¶æ„

### æ ¸å¿ƒé…ç½®å‚æ•°æºç ä¾æ®

```scala
// package.scala:389-397
private[spark] val MEMORY_FRACTION = ConfigBuilder("spark.memory.fraction")
  .doc("Fraction of (heap space - 300MB) used for execution and storage")
  .createWithDefault(0.6)

private[spark] val MEMORY_STORAGE_FRACTION = ConfigBuilder("spark.memory.storageFraction")
  .doc("Amount of storage memory immune to eviction, expressed as a fraction of the size of the region set aside by spark.memory.fraction")
  .createWithDefault(0.5)

// UnifiedMemoryManager.scala:198
private val RESERVED_SYSTEM_MEMORY_BYTES = 300 * 1024 * 1024  // 300MBä¿ç•™å†…å­˜
```

### å†…å­˜åˆ†é…è®¡ç®—é€»è¾‘

```scala
// UnifiedMemoryManager.scala:232-235
private def getMaxMemory(conf: SparkConf): Long = {
  val usableMemory = systemMemory - reservedMemory  // 4GB - 300MB = 3712MB
  val memoryFraction = conf.get(config.MEMORY_FRACTION)  // é»˜è®¤0.6
  (usableMemory * memoryFraction).toLong  // 3712MB Ã— 0.6 = 2227MB
}
```

## ğŸ’¾ MemoryOverhead è¯¦ç»†è®¡ç®—

### JVMéå †å†…å­˜ç»„ä»¶

| ç»„ä»¶                       | æºç ä¾æ®                        | è®¡ç®—æ–¹å¼                   | å†…å­˜å ç”¨  |
|--------------------------|-----------------------------|------------------------|-------|
| **Metaspace**            | JVMè§„èŒƒ                       | Spark 3.xç±»åŠ è½½ä¼˜åŒ–         | 256MB |
| **Code Cache**           | JVMè§„èŒƒ                       | JITç¼–è¯‘ç¼“å­˜                | 128MB |
| **çº¿ç¨‹æ ˆç©ºé—´**                | Executor.scala:113-119      | 25çº¿ç¨‹Ã—1MB               | 25MB  |
| **GCå·¥ä½œå†…å­˜**               | package.scala:335-339       | GCç®—æ³•å¼€é”€ï¼Œä¸å †å¤§å°æˆæ¯”ä¾‹         | 80MB  |
| **Parquetå‘é‡åŒ–ç¼“å†²åŒº**        | SQLConf.scala:1036          | 4096è¡ŒÃ—250å­—èŠ‚Ã—3tasks     | 3MB   |
| **Netty Arena Pool å†…å­˜æ± ** | NettyUtils.java:129         | min(3æ ¸, 2 * 3)Ã—16MB    | 48MB  |
| **Shuffleæ‹‰å–ç¼“å†²åŒº**         | reducer.maxSizeInFlight=48m | 3tasksÃ—48MB            | 144MB |

**çº¿ç¨‹æ ˆè¯¦ç»†åˆ†æ** (åŸºäºExecutor.scala:113-119):
```scala
// Executorä»»åŠ¡çº¿ç¨‹æ±  - newCachedThreadPool (åŠ¨æ€æ‰©å±•)
private[executor] val threadPool = Executors.newCachedThreadPool(threadFactory)

// Task reaperçº¿ç¨‹æ±  - ç›‘ç£ä»»åŠ¡å–æ¶ˆ
private val taskReaperPool = ThreadUtils.newDaemonCachedThreadPool("Task reaper")

// NettyRpcEnv RPCçº¿ç¨‹
val timeoutScheduler = ThreadUtils.newDaemonSingleThreadScheduledExecutor("netty-rpc-env-timeout")
private[netty] val clientConnectionExecutor = ThreadUtils.newDaemonCachedThreadPool("netty-rpc-client")

// NettyUtils.java:129 - Netty I/Oçº¿ç¨‹
NettyUtils.createEventLoop(IOMode.NIO, 3, "shuffle-chunk-fetch-handler")
```

**çº¿ç¨‹æ•°è®¡ç®—**:
- ä»»åŠ¡çº¿ç¨‹æ± : æœ€å¤š3ä¸ªå¹¶å‘ä»»åŠ¡ = 3çº¿ç¨‹
- Task reaperæ± : cachedçº¿ç¨‹æ± ï¼Œé»˜è®¤<5çº¿ç¨‹ = 5çº¿ç¨‹
- Netty RPC: 1ä¸ªtimeout scheduler + cached clientè¿æ¥æ±  = 6çº¿ç¨‹
- Netty I/O: 3ä¸ªäº‹ä»¶å¾ªç¯çº¿ç¨‹
- ç³»ç»Ÿçº¿ç¨‹: GCã€ç›‘æ§ç­‰ â‰ˆ 8çº¿ç¨‹
- **æ€»è®¡**: 3 + 5 + 6 + 3 + 8 = **25çº¿ç¨‹**

**Parquetè¯»å–ç¼“å†²**
   ```text
   VectorizedParquetRecordReader è¯»å–æ•°æ®åˆ°å†…å­˜ columnarBatch-> OnHeapColumnVector[]
   å‘é‡åŒ–æ‰¹æ¬¡: 4096è¡Œ Ã— 250å­—èŠ‚ = 1MB/æ‰¹æ¬¡ PARQUET_VECTORIZED_READER_BATCH_SIZE
   ```
- å‘é‡åŒ–ç¼“å†²: 1MB

**Netty Arena Pool å†…å­˜æ± **

æºç ä¾æ®: Nettyçš„å†…éƒ¨é…ç½®ï¼ˆSparkä¸­æ²¡æœ‰ç›´æ¥é…ç½®ï¼‰
Netty PooledByteBufAllocator é»˜è®¤é…ç½®ï¼š
- spark.network.sharedByteBufAllocators.enabled=true
- æ¯ä¸ª Arena: 16MB (Chunk size)
- é»˜è®¤ Arena æ•°é‡: min(availableProcessors=3, 3 Ã— 2)
- æ€»å†…å­˜: Arenaæ•°é‡ Ã— 16MB = 48MB

### **æœ€ç»ˆMemoryOverheadæ€»è®¡**
```
MemoryOverhead Total = 256 + 128 + 25 + 80 + 3 + 48 + 144 â‰ˆ 1GB
æ¨èé…ç½®: spark.executor.memoryOverhead=768m
```

**GCå†…å­˜ä½¿ç”¨è¯´æ˜**ï¼š
- **GCå·¥ä½œå†…å­˜**ï¼šåŒ…å«GCçº¿ç¨‹æ ˆã€ç®—æ³•å·¥ä½œåŒºï¼ˆå¦‚G1çš„Remembered Setsã€CMSçš„Mark Stackç­‰ï¼‰
- **ä¸å †å¤§å°æˆæ¯”ä¾‹**ï¼šå †è¶Šå¤§ï¼ŒGCè¶Šå¤æ‚ï¼Œéœ€è¦çš„non-heapå†…å­˜è¶Šå¤š
- **Sparkè®¾è®¡è€ƒè™‘**ï¼š`spark.executor.memoryOverheadFactor=0.1`é»˜è®¤10%è€ƒè™‘äº†GCå¼€é”€å¢é•¿

## ğŸ—„ï¸ Storage Memoryè¯¦ç»†ç»„ä»¶

**é…ç½®ä¾æ®** (package.scala:389-397):
```scala
private[spark] val MEMORY_FRACTION = ConfigBuilder("spark.memory.fraction")
  .createWithDefault(0.6)  // é»˜è®¤å€¼

private[spark] val MEMORY_STORAGE_FRACTION = ConfigBuilder("spark.memory.storageFraction")
  .createWithDefault(0.5)  // é»˜è®¤å€¼
```

**Storage Memoryå®é™…ä½¿ç”¨**: æ²¡æœ‰cacheå®é™…ä½¿ç”¨0MBï¼Œå…¨éƒ¨1139MBä¾›Execution Memoryå€Ÿç”¨

## âš™ï¸ Execution Memoryè¯¦ç»†ç»„ä»¶

### Spark Execution Memoryçš„åŠ¨æ€ç”³è¯·æœºåˆ¶

Execution Memoryé‡‡ç”¨**åŠ¨æ€ç”³è¯· + Spillæœºåˆ¶**ï¼Œæ— æ³•é¢„å…ˆç²¾ç¡®è®¡ç®—ï¼

#### é˜¶æ®µ1: ShuffleMapTask - Shuffleå†™ç«¯å†…å­˜åŠ¨æ€ç”³è¯·

**æºç ä¾æ®**: ExternalAppendOnlyMap.scala + TaskMemoryManager.java
```scala
// ExternalAppendOnlyMap.scala:29-30 - MemoryConsumeræœºåˆ¶
abstract class Spillable[C](taskMemoryManager: TaskMemoryManager)
  extends MemoryConsumer(taskMemoryManager, MemoryMode.ON_HEAP)

// TaskMemoryManager.java:306-323 - åŠ¨æ€é¡µé¢åˆ†é…
public MemoryBlock allocatePage(long size, MemoryConsumer consumer) {
  long acquired = acquireExecutionMemory(size, consumer);  // åŠ¨æ€ç”³è¯·Execution Memory
  if (acquired <= 0) {
    return null;  // ç”³è¯·å¤±è´¥ï¼Œä¼šè§¦å‘spillåˆ°ç£ç›˜
  }
  page = memoryManager.tungstenMemoryAllocator().allocate(acquired);
}
```

**ShuffleMapTaskå†…å­˜ä½¿ç”¨**ï¼š

2. **Shuffleå†™ç«¯å†…å­˜** - **çœŸæ­£çš„Execution Memoryç”¨æˆ·**
   ```scala
   // MemoryManager.scala:254-273 - é¡µé¢å¤§å°è®¡ç®—
   val pageSizeBytes = 32MB  // ç³»ç»Ÿè‡ªåŠ¨è®¡ç®—

   // ExternalAppendOnlyMapé€šè¿‡TaskMemoryManageråŠ¨æ€ç”³è¯·
   // æ’å…¥æ•°æ®æ—¶è°ƒç”¨acquirePage(32MB)
   ```
   - **ç”³è¯·æœºåˆ¶**: æŒ‰éœ€ç”³è¯·32MBé¡µé¢ï¼Œç”³è¯·å¤±è´¥å°±spill
   - **æœ€å¤§é™åˆ¶**: 1139MBåŸºç¡€é…é¢ + å¯å€Ÿç”¨Storageç©ºé—´
   - **å®é™…å ç”¨**: **æ— æ³•é¢„å…ˆç²¾ç¡®è®¡ç®—**ï¼Œå®Œå…¨æŒ‰éœ€åˆ†é…

**ShuffleMapTaskæ€»å†…å­˜**: **åŠ¨æ€ç”³è¯·ï¼Œä¸è¶…è¿‡é…é¢é™åˆ¶**

#### é˜¶æ®µ2: ResultTask - SortMergeJoinå¤„ç†å†…å­˜

**æºç ä¾æ®**: SortMergeJoinExec.scala:148 + ExternalAppendOnlyUnsafeRowArray.scala:135
```scala
// SortMergeJoinExec.scala:148
private[this] var currentRightMatches: ExternalAppendOnlyUnsafeRowArray = _

// ExternalAppendOnlyUnsafeRowArray.scala:135-144 - å†…å­˜å­˜å‚¨ç­–ç•¥
if (numRows < initialSizeOfInMemoryBuffer) {
  // å‰128è¡Œå­˜å‚¨åœ¨å†…å­˜ArrayBufferä¸­
  inMemoryBuffer.asInstanceOf[ArrayBuffer[UnsafeRow]] += row
} else {
  // è¶…è¿‡128è¡Œï¼Œä½¿ç”¨UnsafeExternalSorterï¼ˆéœ€è¦Execution Memoryï¼‰
  // æ•°æ®æº¢å‡ºåˆ°ç£ç›˜ï¼Œé€šè¿‡iteratoræŒ‰éœ€è¯»å–
}
```

**ResultTaskå†…å­˜ä½¿ç”¨çœŸå®æƒ…å†µ**ï¼š

**SortMergeJoinç¼“å†²**
   - **åˆå§‹ç¼“å†²**: 128è¡ŒArrayBuffer (çº¦12.8KB)
   - **è¶…è¿‡128è¡Œ**: ä½¿ç”¨UnsafeExternalSorteråŠ¨æ€ç”³è¯·Execution Memory
   - **Spillæœºåˆ¶**: å†…å­˜ä¸è¶³æ—¶æº¢å‡ºåˆ°ç£ç›˜
     - é»˜è®¤æƒ…å†µä¸‹spillableArray.insertRecord è§¦å‘split
     - UnsafeExternalSorter é»˜è®¤numElementsForSpillThreshold=Integer.MAX_VALUE
     - insertRecord split ä¸»è¦è§¦å‘ä¾é `growPointerArrayIfNecessary`ä¸­çš„catchä»£ç å—æ¥split

**ResultTaskæ€»å†…å­˜**: æ— æ³•é¢„å…ˆç²¾ç¡®è®¡ç®—ï¼Œå–å†³äºæ•°æ®åŒ¹é…æ¨¡å¼ï¼ŒåŠ¨æ€ç”³è¯·ï¼Œåˆå§‹æå°ï¼Œå¢é•¿æ—¶è§¦å‘spill

#### é˜¶æ®µ3: Executorå¹¶å‘å†…å­˜åˆ†æ

- **ShuffleMapTask**: 3ä¸ªTaskåŠ¨æ€ç”³è¯·Execution Memoryï¼Œå…±äº«1139MBé…é¢
- **ResultTask**: 3ä¸ªTaskåŠ¨æ€ç”³è¯·ï¼Œåˆå§‹å ç”¨æå°
- **å¹¶å‘é™åˆ¶**: æ€»å ç”¨ä¸è¶…è¿‡1139MBåŸºç¡€é…é¢ + å¯å€Ÿç”¨Storageç©ºé—´

**å€Ÿç”¨Storageæºç ä¾æ®**: UnifiedMemoryManager.scala:232-235 + TaskMemoryManager.java
```scala
// UnifiedMemoryManager.scala:232-235 - å†…å­˜é…é¢è®¡ç®—
private def getMaxMemory(conf: SparkConf): Long = {
  val usableMemory = systemMemory - reservedMemory  // 4096MB - 300MB = 3796MB
  val memoryFraction = conf.get(config.MEMORY_FRACTION)  // é»˜è®¤0.6
  (usableMemory * memoryFraction).toLong  // 3796MB Ã— 0.6 = 2278MB
}

// TaskMemoryManager.java - åŠ¨æ€ç”³è¯·æœºåˆ¶
public long acquireExecutionMemory(
    long numBytes, MemoryConsumer consumer) {
  // 1. å…ˆä»Execution Memoryæ± ç”³è¯·
  // 2. ä¸å¤Ÿæ—¶ï¼Œå€Ÿç”¨Storageç©ºé—²å†…å­˜
  // 3. ä»ç„¶ä¸å¤Ÿæ—¶ï¼Œè¿”å›0ï¼Œè§¦å‘spill
}
```
### Execution Memoryåˆ†é…è¡¨

| é˜¶æ®µ | ç»„ä»¶ | æºç ä¾æ® | çœŸå®æœºåˆ¶ | å†…å­˜å ç”¨ |
|------|------|----------|----------|----------|
| **ShuffleMapTask** | Shuffleå†™ç«¯ | ExternalAppendOnlyMap + TaskMemoryManager | åŠ¨æ€ç”³è¯·32MBé¡µé¢ï¼Œå¤±è´¥åˆ™spill | **æ— æ³•é¢„å…ˆç²¾ç¡®è®¡ç®—** |
| **ResultTask** | SortMergeJoin | ExternalAppendOnlyUnsafeRowArray | 128è¡Œåˆå§‹ç¼“å†²ï¼Œè¶…å‡ºç”¨UnsafeExternalSorter | **æ— æ³•é¢„å…ˆç²¾ç¡®è®¡ç®—** |
| **å¹¶å‘é™åˆ¶** | æ‰€æœ‰Task | TaskMemoryManager | å…±äº«1139MBé…é¢ + å¯å€Ÿç”¨Storage | **ä¸è¶…è¿‡é…é¢é™åˆ¶** |
| **åŸºç¡€é…é¢** | UnifiedMemoryManager | spark.memory.fraction=0.6 | 2278MBÃ—0.5 | 1139MB |
| **åŠ¨æ€å€Ÿç”¨** | ExecutionMemoryPool | å¯å€Ÿç”¨Storageç©ºé—²å†…å­˜ | æœ€å¤§1139MB | **åŠ¨æ€è®¡ç®—** |


## ğŸ“‹ æœ€ç»ˆå†…å­˜åˆ†é…è¯¦æƒ…

```text
Executor Memory: 4096MB
â”œâ”€â”€ ç³»ç»Ÿä¿ç•™å†…å­˜ (RESERVED_SYSTEM_MEMORY_BYTES): 300MB
â”œâ”€â”€ ç”¨æˆ·ä»£ç é¢„ç•™ (40%): 1638MB
â”œâ”€â”€ Unified Memory Pool (60%): 2278MB
â”‚   â”œâ”€â”€ Storage Memory (åŸºç¡€é…é¢50%): 1139MB
â”‚   â”‚   â””â”€â”€ å¯ç”¨å€Ÿç”¨ç©ºé—´: 1139MB (å…¨éƒ¨ä¾›Executionå€Ÿç”¨)
â”‚   â”‚
â”‚   â”œâ”€â”€ Execution Memory (åŸºç¡€é…é¢50%): 1139MB
â”‚   â”‚   â”œâ”€â”€ ShuffleMapTaské˜¶æ®µ: åŠ¨æ€ç”³è¯·ï¼Œå¤±è´¥åˆ™spill
â”‚   â”‚   â”œâ”€â”€ ResultTaské˜¶æ®µ: åŠ¨æ€ç”³è¯·ï¼Œåˆå§‹æå°
â”‚   â”‚   â””â”€â”€ æœ€å¤§å¯ç”¨: 1139MB + å¯å€Ÿç”¨Storageç©ºé—´
â”‚   â”‚
â”‚   â””â”€â”€ åŠ¨æ€å€Ÿç”¨æœºåˆ¶: Executionå¯å€Ÿç”¨Storageç©ºé—²å†…å­˜
â”‚
â””â”€â”€ MemoryOverhead: 1GB
    â”œâ”€â”€ Metaspace: 256MB
    â”œâ”€â”€ Code Cache: 128MB
    â”œâ”€â”€ çº¿ç¨‹æ ˆç©ºé—´: 25MB
    â”œâ”€â”€ GCå·¥ä½œå†…å­˜: 80MB
    â”œâ”€â”€ Parquetç¼“å†²: 3MB
    â”œâ”€â”€ Netty Arena Pool: 48MB
    â”œâ”€â”€ Shuffleç¼“å†²: 144MB
    â””â”€â”€ å…¶ä»–nativeå¼€é”€: 36MB
```

## ğŸ¯ é…ç½®å‚æ•°
```bash
# Executorå†…å­˜é…ç½®
spark.executor.memory=4g
spark.executor.memoryOverhead=768m
spark.executor.cores=3

# å†…å­˜ç®¡ç†é…ç½® (ä½¿ç”¨é»˜è®¤å€¼ï¼Œé€‚åˆSMJåœºæ™¯)
spark.memory.fraction=0.6           # é»˜è®¤å€¼ï¼Œ60%ç”¨äºUnified Memory Pool
spark.memory.storageFraction=0.5     # é»˜è®¤å€¼ï¼ŒSMJåœºæ™¯ä¸­Storageä½¿ç”¨å¾ˆå°‘ï¼Œå¤§éƒ¨åˆ†å¯å€Ÿç”¨

# é’ˆå¯¹Joinä¼˜åŒ–
spark.sql.sortMergeJoinExec.buffer.spill.threshold=1000000
spark.sql.autoBroadcastJoinThreshold=10m
spark.shuffle.file.buffer=256k
spark.reducer.maxSizeInFlight=48m     # ä½¿ç”¨é»˜è®¤å€¼

# Parquetä¼˜åŒ–
spark.sql.parquet.enableVectorizedReader=true
spark.sql.parquet.columnarReaderBatchSize=4096
```