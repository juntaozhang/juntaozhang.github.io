15:23:10.222 [ScalaTest-run] INFO  org.apache.spark.SparkContext - Running Spark version 3.3.1
15:23:10.286 [ScalaTest-run] WARN  org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
15:23:10.362 [ScalaTest-run] INFO  org.apache.spark.resource.ResourceUtils - ==============================================================
15:23:10.362 [ScalaTest-run] INFO  org.apache.spark.resource.ResourceUtils - No custom resources configured for spark.driver.
15:23:10.363 [ScalaTest-run] INFO  org.apache.spark.resource.ResourceUtils - ==============================================================
15:23:10.363 [ScalaTest-run] INFO  org.apache.spark.SparkContext - Submitted application: LogicalPlanTest
15:23:10.382 [ScalaTest-run] INFO  org.apache.spark.resource.ResourceProfile - Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
15:23:10.396 [ScalaTest-run] INFO  org.apache.spark.resource.ResourceProfile - Limiting resource is cpu
15:23:10.397 [ScalaTest-run] INFO  org.apache.spark.resource.ResourceProfileManager - Added ResourceProfile id: 0
15:23:10.443 [ScalaTest-run] INFO  org.apache.spark.SecurityManager - Changing view acls to: juntao
15:23:10.443 [ScalaTest-run] INFO  org.apache.spark.SecurityManager - Changing modify acls to: juntao
15:23:10.444 [ScalaTest-run] INFO  org.apache.spark.SecurityManager - Changing view acls groups to:
15:23:10.444 [ScalaTest-run] INFO  org.apache.spark.SecurityManager - Changing modify acls groups to:
15:23:10.444 [ScalaTest-run] INFO  org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(juntao); groups with view permissions: Set(); users  with modify permissions: Set(juntao); groups with modify permissions: Set()
15:23:10.513 [ScalaTest-run] DEBUG io.netty.util.internal.logging.InternalLoggerFactory - Using SLF4J as the default logging framework
15:23:10.519 [ScalaTest-run] DEBUG io.netty.util.internal.InternalThreadLocalMap - -Dio.netty.threadLocalMap.stringBuilder.initialSize: 1024
15:23:10.520 [ScalaTest-run] DEBUG io.netty.util.internal.InternalThreadLocalMap - -Dio.netty.threadLocalMap.stringBuilder.maxSize: 4096
15:23:10.530 [ScalaTest-run] DEBUG io.netty.channel.MultithreadEventLoopGroup - -Dio.netty.eventLoopThreads: 32
15:23:10.561 [ScalaTest-run] DEBUG io.netty.util.internal.PlatformDependent0 - -Dio.netty.noUnsafe: false
15:23:10.561 [ScalaTest-run] DEBUG io.netty.util.internal.PlatformDependent0 - Java version: 8
15:23:10.562 [ScalaTest-run] DEBUG io.netty.util.internal.PlatformDependent0 - sun.misc.Unsafe.theUnsafe: available
15:23:10.563 [ScalaTest-run] DEBUG io.netty.util.internal.PlatformDependent0 - sun.misc.Unsafe.copyMemory: available
15:23:10.563 [ScalaTest-run] DEBUG io.netty.util.internal.PlatformDependent0 - java.nio.Buffer.address: available
15:23:10.563 [ScalaTest-run] DEBUG io.netty.util.internal.PlatformDependent0 - direct buffer constructor: available
15:23:10.564 [ScalaTest-run] DEBUG io.netty.util.internal.PlatformDependent0 - java.nio.Bits.unaligned: available, true
15:23:10.564 [ScalaTest-run] DEBUG io.netty.util.internal.PlatformDependent0 - jdk.internal.misc.Unsafe.allocateUninitializedArray(int): unavailable prior to Java9
15:23:10.564 [ScalaTest-run] DEBUG io.netty.util.internal.PlatformDependent0 - java.nio.DirectByteBuffer.<init>(long, int): available
15:23:10.564 [ScalaTest-run] DEBUG io.netty.util.internal.PlatformDependent - sun.misc.Unsafe: available
15:23:10.564 [ScalaTest-run] DEBUG io.netty.util.internal.PlatformDependent - -Dio.netty.tmpdir: /var/folders/8w/3j0ns1bd6xjf_9n5_h2dyjt80000gn/T (java.io.tmpdir)
15:23:10.565 [ScalaTest-run] DEBUG io.netty.util.internal.PlatformDependent - -Dio.netty.bitMode: 64 (sun.arch.data.model)
15:23:10.565 [ScalaTest-run] DEBUG io.netty.util.internal.PlatformDependent - Platform: MacOS
15:23:10.565 [ScalaTest-run] DEBUG io.netty.util.internal.PlatformDependent - -Dio.netty.maxDirectMemory: 7635730432 bytes
15:23:10.565 [ScalaTest-run] DEBUG io.netty.util.internal.PlatformDependent - -Dio.netty.uninitializedArrayAllocationThreshold: -1
15:23:10.566 [ScalaTest-run] DEBUG io.netty.util.internal.CleanerJava6 - java.nio.ByteBuffer.cleaner(): available
15:23:10.566 [ScalaTest-run] DEBUG io.netty.util.internal.PlatformDependent - -Dio.netty.noPreferDirect: false
15:23:10.567 [ScalaTest-run] DEBUG io.netty.channel.nio.NioEventLoop - -Dio.netty.noKeySetOptimization: false
15:23:10.567 [ScalaTest-run] DEBUG io.netty.channel.nio.NioEventLoop - -Dio.netty.selectorAutoRebuildThreshold: 512
15:23:10.574 [ScalaTest-run] DEBUG io.netty.util.internal.PlatformDependent - org.jctools-core.MpscChunkedArrayQueue: available
15:23:10.593 [ScalaTest-run] DEBUG io.netty.util.ResourceLeakDetector - -Dio.netty.leakDetection.level: simple
15:23:10.593 [ScalaTest-run] DEBUG io.netty.util.ResourceLeakDetector - -Dio.netty.leakDetection.targetRecords: 4
15:23:10.595 [ScalaTest-run] DEBUG io.netty.buffer.PooledByteBufAllocator - -Dio.netty.allocator.numHeapArenas: 32
15:23:10.595 [ScalaTest-run] DEBUG io.netty.buffer.PooledByteBufAllocator - -Dio.netty.allocator.numDirectArenas: 32
15:23:10.595 [ScalaTest-run] DEBUG io.netty.buffer.PooledByteBufAllocator - -Dio.netty.allocator.pageSize: 8192
15:23:10.595 [ScalaTest-run] DEBUG io.netty.buffer.PooledByteBufAllocator - -Dio.netty.allocator.maxOrder: 11
15:23:10.596 [ScalaTest-run] DEBUG io.netty.buffer.PooledByteBufAllocator - -Dio.netty.allocator.chunkSize: 16777216
15:23:10.596 [ScalaTest-run] DEBUG io.netty.buffer.PooledByteBufAllocator - -Dio.netty.allocator.smallCacheSize: 256
15:23:10.596 [ScalaTest-run] DEBUG io.netty.buffer.PooledByteBufAllocator - -Dio.netty.allocator.normalCacheSize: 64
15:23:10.596 [ScalaTest-run] DEBUG io.netty.buffer.PooledByteBufAllocator - -Dio.netty.allocator.maxCachedBufferCapacity: 32768
15:23:10.596 [ScalaTest-run] DEBUG io.netty.buffer.PooledByteBufAllocator - -Dio.netty.allocator.cacheTrimInterval: 8192
15:23:10.596 [ScalaTest-run] DEBUG io.netty.buffer.PooledByteBufAllocator - -Dio.netty.allocator.cacheTrimIntervalMillis: 0
15:23:10.596 [ScalaTest-run] DEBUG io.netty.buffer.PooledByteBufAllocator - -Dio.netty.allocator.useCacheForAllThreads: true
15:23:10.596 [ScalaTest-run] DEBUG io.netty.buffer.PooledByteBufAllocator - -Dio.netty.allocator.maxCachedByteBuffersPerChunk: 1023
15:23:10.639 [ScalaTest-run] DEBUG io.netty.channel.DefaultChannelId - -Dio.netty.processId: 69449 (auto-detected)
15:23:10.641 [ScalaTest-run] DEBUG io.netty.util.NetUtil - -Djava.net.preferIPv4Stack: false
15:23:10.641 [ScalaTest-run] DEBUG io.netty.util.NetUtil - -Djava.net.preferIPv6Addresses: false
15:23:10.643 [ScalaTest-run] DEBUG io.netty.util.NetUtilInitializations - Loopback interface: lo0 (lo0, 0:0:0:0:0:0:0:1)
15:23:10.644 [ScalaTest-run] DEBUG io.netty.util.NetUtil - Failed to get SOMAXCONN from sysctl and file /proc/sys/net/core/somaxconn. Default: 128
15:23:10.646 [ScalaTest-run] DEBUG io.netty.channel.DefaultChannelId - -Dio.netty.machineId: ac:de:48:ff:fe:00:11:22 (auto-detected)
15:23:10.669 [ScalaTest-run] DEBUG io.netty.buffer.ByteBufUtil - -Dio.netty.allocator.type: pooled
15:23:10.669 [ScalaTest-run] DEBUG io.netty.buffer.ByteBufUtil - -Dio.netty.threadLocalDirectBufferSize: 0
15:23:10.669 [ScalaTest-run] DEBUG io.netty.buffer.ByteBufUtil - -Dio.netty.maxThreadLocalCharBufferSize: 16384
15:23:10.688 [ScalaTest-run] DEBUG org.apache.spark.network.server.TransportServer - Shuffle server started on port: 55537
15:23:10.693 [ScalaTest-run] INFO  org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 55537.
15:23:10.694 [ScalaTest-run] DEBUG org.apache.spark.SparkEnv - Using serializer: class org.apache.spark.serializer.JavaSerializer
15:23:10.717 [ScalaTest-run] INFO  org.apache.spark.SparkEnv - Registering MapOutputTracker
15:23:10.718 [ScalaTest-run] DEBUG org.apache.spark.MapOutputTrackerMasterEndpoint - init
15:23:10.748 [ScalaTest-run] INFO  org.apache.spark.SparkEnv - Registering BlockManagerMaster
15:23:10.793 [ScalaTest-run] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
15:23:10.794 [ScalaTest-run] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
15:23:10.797 [ScalaTest-run] INFO  org.apache.spark.SparkEnv - Registering BlockManagerMasterHeartbeat
15:23:10.833 [ScalaTest-run] INFO  org.apache.spark.storage.DiskBlockManager - Created local directory at /private/var/folders/8w/3j0ns1bd6xjf_9n5_h2dyjt80000gn/T/blockmgr-6a4a000f-ee9b-40b4-a0a7-22f874f1a0b8
15:23:10.836 [ScalaTest-run] DEBUG org.apache.spark.storage.DiskBlockManager - Adding shutdown hook
15:23:10.837 [ScalaTest-run] DEBUG org.apache.spark.util.ShutdownHookManager - Adding shutdown hook
15:23:10.877 [ScalaTest-run] INFO  org.apache.spark.storage.memory.MemoryStore - MemoryStore started with capacity 4.1 GiB
15:23:10.892 [ScalaTest-run] INFO  org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
15:23:10.893 [ScalaTest-run] DEBUG org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - init
15:23:10.903 [ScalaTest-run] DEBUG org.apache.spark.SecurityManager - Created SSL options for ui: SSLOptions{enabled=false, port=None, keyStore=None, keyStorePassword=None, trustStore=None, trustStorePassword=None, protocol=None, enabledAlgorithms=Set()}
15:23:11.077 [ScalaTest-run] DEBUG org.apache.spark.ui.JettyUtils - Using requestHeaderSize: 8192
15:23:11.112 [ScalaTest-run] INFO  org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
15:23:11.197 [ScalaTest-run] INFO  org.apache.spark.executor.Executor - Starting executor ID driver on host 192.168.31.150
15:23:11.205 [ScalaTest-run] INFO  org.apache.spark.executor.Executor - Starting executor with user classpath (userClassPathFirst = false): ''
15:23:11.222 [ScalaTest-run] DEBUG org.apache.spark.network.server.TransportServer - Shuffle server started on port: 55538
15:23:11.222 [ScalaTest-run] INFO  org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 55538.
15:23:11.222 [ScalaTest-run] INFO  org.apache.spark.network.netty.NettyBlockTransferService - Server created on 192.168.31.150:55538
15:23:11.224 [ScalaTest-run] INFO  org.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
15:23:11.229 [ScalaTest-run] INFO  org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.31.150, 55538, None)
15:23:11.231 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.DefaultTopologyMapper - Got a request for 192.168.31.150
15:23:11.232 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager 192.168.31.150:55538 with 4.1 GiB RAM, BlockManagerId(driver, 192.168.31.150, 55538, None)
15:23:11.235 [ScalaTest-run] INFO  org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.31.150, 55538, None)
15:23:11.236 [ScalaTest-run] INFO  org.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, 192.168.31.150, 55538, None)
15:23:11.441 [ScalaTest-run] DEBUG org.apache.spark.SparkContext - Adding shutdown hook



15:23:11.601 [ScalaTest-run-running-SortMergeJoinSpec] INFO  org.apache.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
15:23:11.603 [ScalaTest-run-running-SortMergeJoinSpec] DEBUG org.apache.spark.sql.internal.SharedState - Applying other initial session options to HadoopConf: spark.app.name -> LogicalPlanTest
15:23:11.603 [ScalaTest-run-running-SortMergeJoinSpec] DEBUG org.apache.spark.sql.internal.SharedState - Applying other initial session options to HadoopConf: spark.master -> local
15:23:11.603 [ScalaTest-run-running-SortMergeJoinSpec] DEBUG org.apache.spark.sql.internal.SharedState - Applying static initial session options to SparkConf: spark.sql.catalogImplementation -> hive
15:23:11.608 [ScalaTest-run-running-SortMergeJoinSpec] INFO  org.apache.spark.sql.internal.SharedState - Warehouse path is 'file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-warehouse'.
15:23:12.404 [ScalaTest-run-running-SortMergeJoinSpec] DEBUG org.apache.spark.sql.execution.SparkSqlParser - Parsing command:
SELECT t1.id, t1.name, t2.value
FROM t1
JOIN t2
ON t1.id = t2.id
where t2.value > 1

org.apache.spark.sql.catalyst.parser.SqlBaseParser$SingleStatementContext =>  SELECTt1.id,t1.name,t2.valueFROMt1JOINt2ONt1.id=t2.idwheret2.value>1 <EOF>
org.apache.spark.sql.catalyst.parser.SqlBaseParser$SingleStatementContext =>  SELECTt1.id,t1.name,t2.valueFROMt1JOINt2ONt1.id=t2.idwheret2.value>1 <EOF>
org.apache.spark.sql.catalyst.parser.SqlBaseParser$QueryContext =>  SELECTt1.id,t1.name,t2.valueFROMt1JOINt2ONt1.id=t2.idwheret2.value>1
org.apache.spark.sql.catalyst.parser.SqlBaseParser$RegularQuerySpecificationContext =>  SELECTt1.id,t1.name,t2.value FROMt1JOINt2ONt1.id=t2.id wheret2.value>1
org.apache.spark.sql.catalyst.parser.SqlBaseParser$FromClauseContext =>  FROM t1JOINt2ONt1.id=t2.id
org.apache.spark.sql.catalyst.parser.SqlBaseParser$TableNameContext =>  t1
org.apache.spark.sql.catalyst.parser.SqlBaseParser$MultipartIdentifierContext =>  t1
org.apache.spark.sql.catalyst.parser.SqlBaseParser$JoinRelationContext =>   JOIN t2 ONt1.id=t2.id
org.apache.spark.sql.catalyst.parser.SqlBaseParser$PredicatedContext =>  t1.id=t2.id
org.apache.spark.sql.catalyst.parser.SqlBaseParser$ComparisonContext =>  t1.id = t2.id
org.apache.spark.sql.catalyst.parser.SqlBaseParser$DereferenceContext =>  t1 . id
org.apache.spark.sql.catalyst.parser.SqlBaseParser$ColumnReferenceContext =>  t1
org.apache.spark.sql.catalyst.parser.SqlBaseParser$DereferenceContext =>  t2 . id
org.apache.spark.sql.catalyst.parser.SqlBaseParser$ColumnReferenceContext =>  t2
org.apache.spark.sql.catalyst.parser.SqlBaseParser$TableNameContext =>  t2
org.apache.spark.sql.catalyst.parser.SqlBaseParser$MultipartIdentifierContext =>  t2
org.apache.spark.sql.catalyst.parser.SqlBaseParser$RegularQuerySpecificationContext =>  SELECTt1.id,t1.name,t2.value FROMt1JOINt2ONt1.id=t2.id wheret2.value>1
org.apache.spark.sql.catalyst.parser.SqlBaseParser$NamedExpressionContext =>  t1.id
org.apache.spark.sql.catalyst.parser.SqlBaseParser$PredicatedContext =>  t1.id
org.apache.spark.sql.catalyst.parser.SqlBaseParser$DereferenceContext =>  t1 . id
org.apache.spark.sql.catalyst.parser.SqlBaseParser$ColumnReferenceContext =>  t1
org.apache.spark.sql.catalyst.parser.SqlBaseParser$NamedExpressionContext =>  t1.name
org.apache.spark.sql.catalyst.parser.SqlBaseParser$PredicatedContext =>  t1.name
org.apache.spark.sql.catalyst.parser.SqlBaseParser$DereferenceContext =>  t1 . name
org.apache.spark.sql.catalyst.parser.SqlBaseParser$ColumnReferenceContext =>  t1
org.apache.spark.sql.catalyst.parser.SqlBaseParser$NamedExpressionContext =>  t2.value
org.apache.spark.sql.catalyst.parser.SqlBaseParser$PredicatedContext =>  t2.value
org.apache.spark.sql.catalyst.parser.SqlBaseParser$DereferenceContext =>  t2 . value
org.apache.spark.sql.catalyst.parser.SqlBaseParser$ColumnReferenceContext =>  t2
org.apache.spark.sql.catalyst.parser.SqlBaseParser$PredicatedContext =>  t2.value>1
org.apache.spark.sql.catalyst.parser.SqlBaseParser$ComparisonContext =>  t2.value > 1
org.apache.spark.sql.catalyst.parser.SqlBaseParser$DereferenceContext =>  t2 . value
org.apache.spark.sql.catalyst.parser.SqlBaseParser$ColumnReferenceContext =>  t2
org.apache.spark.sql.catalyst.parser.SqlBaseParser$IntegerLiteralContext =>  1
org.apache.spark.sql.catalyst.parser.SqlBaseParser$QueryOrganizationContext =>
Unresolved Logical Plan ====>
'Project ['t1.id, 't1.name, 't2.value]
+- 'Filter ('t2.value > 1)
   +- 'Join Inner, ('t1.id = 't2.id)
      :- 'UnresolvedRelation [t1], [], false
      +- 'UnresolvedRelation [t2], [], false

15:23:12.690 [ScalaTest-run-running-SortMergeJoinSpec] DEBUG org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: spark_grouping_id
org.apache.spark.sql.catalyst.parser.SqlBaseParser$SingleMultipartIdentifierContext =>  spark_grouping_id <EOF>
org.apache.spark.sql.catalyst.parser.SqlBaseParser$MultipartIdentifierContext =>  spark_grouping_id
15:23:12.863 [ScalaTest-run-running-SortMergeJoinSpec] INFO  org.apache.spark.sql.hive.HiveUtils - Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.
15:23:13.284 [ScalaTest-run-running-SortMergeJoinSpec] INFO  org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 2.3.9) is file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-warehouse
15:23:13.456 [ScalaTest-run-running-SortMergeJoinSpec] WARN  org.apache.hadoop.hive.conf.HiveConf - HiveConf of name hive.stats.jdbc.timeout does not exist
15:23:13.456 [ScalaTest-run-running-SortMergeJoinSpec] WARN  org.apache.hadoop.hive.conf.HiveConf - HiveConf of name hive.stats.retries.wait does not exist
15:23:13.905 [ScalaTest-run-running-SortMergeJoinSpec] DEBUG com.jolbox.bonecp.BoneCPDataSource - JDBC URL = jdbc:derby:;databaseName=metastore_db;create=true, Username = APP, partitions = 1, max (per partition) = 10, min (per partition) = 0, idle max age = 60 min, idle test period = 240 min, strategy = DEFAULT
15:23:15.200 [ScalaTest-run-running-SortMergeJoinSpec] DEBUG com.jolbox.bonecp.BoneCPDataSource - JDBC URL = jdbc:derby:;databaseName=metastore_db;create=true, Username = APP, partitions = 1, max (per partition) = 10, min (per partition) = 0, idle max age = 60 min, idle test period = 240 min, strategy = DEFAULT
15:23:15.540 [ScalaTest-run-running-SortMergeJoinSpec] WARN  org.apache.hadoop.hive.metastore.ObjectStore - Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 2.3.0
15:23:15.540 [ScalaTest-run-running-SortMergeJoinSpec] WARN  org.apache.hadoop.hive.metastore.ObjectStore - setMetaStoreSchemaVersion called but recording version is disabled: version = 2.3.0, comment = Set by MetaStore juntao@127.0.0.1
15:23:15.903 [ScalaTest-run-running-SortMergeJoinSpec] DEBUG org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: int
org.apache.spark.sql.catalyst.parser.SqlBaseParser$SingleDataTypeContext =>  int <EOF>
org.apache.spark.sql.catalyst.parser.SqlBaseParser$PrimitiveDataTypeContext =>  int
15:23:15.909 [ScalaTest-run-running-SortMergeJoinSpec] DEBUG org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
org.apache.spark.sql.catalyst.parser.SqlBaseParser$SingleDataTypeContext =>  string <EOF>
org.apache.spark.sql.catalyst.parser.SqlBaseParser$PrimitiveDataTypeContext =>  string
15:23:15.910 [ScalaTest-run-running-SortMergeJoinSpec] DEBUG org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
org.apache.spark.sql.catalyst.parser.SqlBaseParser$SingleDataTypeContext =>  string <EOF>
org.apache.spark.sql.catalyst.parser.SqlBaseParser$PrimitiveDataTypeContext =>  string
15:23:15.926 [ScalaTest-run-running-SortMergeJoinSpec] DEBUG org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: array<string>
org.apache.spark.sql.catalyst.parser.SqlBaseParser$SingleDataTypeContext =>  array<string> <EOF>
org.apache.spark.sql.catalyst.parser.SqlBaseParser$ComplexDataTypeContext =>  array < string >
org.apache.spark.sql.catalyst.parser.SqlBaseParser$PrimitiveDataTypeContext =>  string
15:23:16.011 [ScalaTest-run-running-SortMergeJoinSpec] DEBUG org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: int
org.apache.spark.sql.catalyst.parser.SqlBaseParser$SingleDataTypeContext =>  int <EOF>
org.apache.spark.sql.catalyst.parser.SqlBaseParser$PrimitiveDataTypeContext =>  int
15:23:16.011 [ScalaTest-run-running-SortMergeJoinSpec] DEBUG org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: int
org.apache.spark.sql.catalyst.parser.SqlBaseParser$SingleDataTypeContext =>  int <EOF>
org.apache.spark.sql.catalyst.parser.SqlBaseParser$PrimitiveDataTypeContext =>  int
15:23:16.011 [ScalaTest-run-running-SortMergeJoinSpec] DEBUG org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
org.apache.spark.sql.catalyst.parser.SqlBaseParser$SingleDataTypeContext =>  string <EOF>
org.apache.spark.sql.catalyst.parser.SqlBaseParser$PrimitiveDataTypeContext =>  string
Analyzer(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$1@25f7cc38) org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations --> old【
'Project ['t1.id, 't1.name, 't2.value]
+- 'Filter ('t2.value > 1)
   +- 'Join Inner, ('t1.id = 't2.id)
      :- 'UnresolvedRelation [t1], [], false
      +- 'UnresolvedRelation [t2], [], false
】--> new【
'Project ['t1.id, 't1.name, 't2.value]
+- 'Filter ('t2.value > 1)
   +- 'Join Inner, ('t1.id = 't2.id)
      :- 'SubqueryAlias spark_catalog.default.t1
      :  +- 'UnresolvedCatalogRelation `default`.`t1`, org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, [], false
      +- 'SubqueryAlias spark_catalog.default.t2
         +- 'UnresolvedCatalogRelation `default`.`t2`, org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, [], false
】


Analyzer(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$1@25f7cc38) org.apache.spark.sql.execution.datasources.FindDataSourceTable --> old【
'Project ['t1.id, 't1.name, 't2.value]
+- 'Filter ('t2.value > 1)
   +- 'Join Inner, ('t1.id = 't2.id)
      :- 'SubqueryAlias spark_catalog.default.t1
      :  +- 'UnresolvedCatalogRelation `default`.`t1`, org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, [], false
      +- 'SubqueryAlias spark_catalog.default.t2
         +- 'UnresolvedCatalogRelation `default`.`t2`, org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, [], false
】--> new【
'Project ['t1.id, 't1.name, 't2.value]
+- 'Filter ('t2.value > 1)
   +- 'Join Inner, ('t1.id = 't2.id)
      :- SubqueryAlias spark_catalog.default.t1
      :  +- Relation default.t1[id#0,name#1,date#2] parquet
      +- SubqueryAlias spark_catalog.default.t2
         +- Relation default.t2[id#3,value#4,date#5] parquet
】


Analyzer(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$1@25f7cc38) org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences --> old【
'Project ['t1.id, 't1.name, 't2.value]
+- 'Filter ('t2.value > 1)
   +- 'Join Inner, ('t1.id = 't2.id)
      :- SubqueryAlias spark_catalog.default.t1
      :  +- Relation default.t1[id#0,name#1,date#2] parquet
      +- SubqueryAlias spark_catalog.default.t2
         +- Relation default.t2[id#3,value#4,date#5] parquet
】--> new【
Project [id#0, name#1, value#4]
+- Filter (value#4 > 1)
   +- Join Inner, (id#0 = id#3)
      :- SubqueryAlias spark_catalog.default.t1
      :  +- Relation default.t1[id#0,name#1,date#2] parquet
      +- SubqueryAlias spark_catalog.default.t2
         +- Relation default.t2[id#3,value#4,date#5] parquet
】


RuleExecutor.execute[Analyzer(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$1@25f7cc38)] final result ==> old【
'Project ['t1.id, 't1.name, 't2.value]
+- 'Filter ('t2.value > 1)
   +- 'Join Inner, ('t1.id = 't2.id)
      :- 'UnresolvedRelation [t1], [], false
      +- 'UnresolvedRelation [t2], [], false
】==> new【
Project [id#0, name#1, value#4]
+- Filter (value#4 > 1)
   +- Join Inner, (id#0 = id#3)
      :- SubqueryAlias spark_catalog.default.t1
      :  +- Relation default.t1[id#0,name#1,date#2] parquet
      +- SubqueryAlias spark_catalog.default.t2
         +- Relation default.t2[id#3,value#4,date#5] parquet
】


analyzed ====>
Project [id#0, name#1, value#4]
+- Filter (value#4 > 1)
   +- Join Inner, (id#0 = id#3)
      :- SubqueryAlias spark_catalog.default.t1
      :  +- Relation default.t1[id#0,name#1,date#2] parquet
      +- SubqueryAlias spark_catalog.default.t2
         +- Relation default.t2[id#3,value#4,date#5] parquet

===================== df.show() =============================
Analyzer(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$1@25f7cc38) org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveAliases --> old【
'Project [unresolvedalias(cast(id#0 as string), None), unresolvedalias(cast(name#1 as string), None), unresolvedalias(cast(value#4 as string), None)]
+- Project [id#0, name#1, value#4]
   +- Filter (value#4 > 1)
      +- Join Inner, (id#0 = id#3)
         :- SubqueryAlias spark_catalog.default.t1
         :  +- Relation default.t1[id#0,name#1,date#2] parquet
         +- SubqueryAlias spark_catalog.default.t2
            +- Relation default.t2[id#3,value#4,date#5] parquet
】--> new【
Project [cast(id#0 as string) AS id#14, cast(name#1 as string) AS name#15, cast(value#4 as string) AS value#16]
+- Project [id#0, name#1, value#4]
   +- Filter (value#4 > 1)
      +- Join Inner, (id#0 = id#3)
         :- SubqueryAlias spark_catalog.default.t1
         :  +- Relation default.t1[id#0,name#1,date#2] parquet
         +- SubqueryAlias spark_catalog.default.t2
            +- Relation default.t2[id#3,value#4,date#5] parquet
】


Analyzer(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$1@25f7cc38) org.apache.spark.sql.catalyst.analysis.ResolveTimeZone --> old【
Project [cast(id#0 as string) AS id#14, cast(name#1 as string) AS name#15, cast(value#4 as string) AS value#16]
+- Project [id#0, name#1, value#4]
   +- Filter (value#4 > 1)
      +- Join Inner, (id#0 = id#3)
         :- SubqueryAlias spark_catalog.default.t1
         :  +- Relation default.t1[id#0,name#1,date#2] parquet
         +- SubqueryAlias spark_catalog.default.t2
            +- Relation default.t2[id#3,value#4,date#5] parquet
】--> new【
Project [cast(id#0 as string) AS id#14, cast(name#1 as string) AS name#15, cast(value#4 as string) AS value#16]
+- Project [id#0, name#1, value#4]
   +- Filter (value#4 > 1)
      +- Join Inner, (id#0 = id#3)
         :- SubqueryAlias spark_catalog.default.t1
         :  +- Relation default.t1[id#0,name#1,date#2] parquet
         +- SubqueryAlias spark_catalog.default.t2
            +- Relation default.t2[id#3,value#4,date#5] parquet
】


RuleExecutor.execute[Analyzer(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$1@25f7cc38)] final result ==> old【
'Project [unresolvedalias(cast(id#0 as string), None), unresolvedalias(cast(name#1 as string), None), unresolvedalias(cast(value#4 as string), None)]
+- Project [id#0, name#1, value#4]
   +- Filter (value#4 > 1)
      +- Join Inner, (id#0 = id#3)
         :- SubqueryAlias spark_catalog.default.t1
         :  +- Relation default.t1[id#0,name#1,date#2] parquet
         +- SubqueryAlias spark_catalog.default.t2
            +- Relation default.t2[id#3,value#4,date#5] parquet
】==> new【
Project [cast(id#0 as string) AS id#14, cast(name#1 as string) AS name#15, cast(value#4 as string) AS value#16]
+- Project [id#0, name#1, value#4]
   +- Filter (value#4 > 1)
      +- Join Inner, (id#0 = id#3)
         :- SubqueryAlias spark_catalog.default.t1
         :  +- Relation default.t1[id#0,name#1,date#2] parquet
         +- SubqueryAlias spark_catalog.default.t2
            +- Relation default.t2[id#3,value#4,date#5] parquet
】


analyzed ====>
Project [cast(id#0 as string) AS id#14, cast(name#1 as string) AS name#15, cast(value#4 as string) AS value#16]
+- Project [id#0, name#1, value#4]
   +- Filter (value#4 > 1)
      +- Join Inner, (id#0 = id#3)
         :- SubqueryAlias spark_catalog.default.t1
         :  +- Relation default.t1[id#0,name#1,date#2] parquet
         +- SubqueryAlias spark_catalog.default.t2
            +- Relation default.t2[id#3,value#4,date#5] parquet

analyzed ====>
GlobalLimit 21
+- LocalLimit 21
   +- Project [cast(id#0 as string) AS id#14, cast(name#1 as string) AS name#15, cast(value#4 as string) AS value#16]
      +- Project [id#0, name#1, value#4]
         +- Filter (value#4 > 1)
            +- Join Inner, (id#0 = id#3)
               :- SubqueryAlias spark_catalog.default.t1
               :  +- Relation default.t1[id#0,name#1,date#2] parquet
               +- SubqueryAlias spark_catalog.default.t2
                  +- Relation default.t2[id#3,value#4,date#5] parquet

SparkOptimizer(org.apache.spark.sql.internal.BaseSessionStateBuilder$$anon$2@2ae5580) org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis --> old【
GlobalLimit 21
+- LocalLimit 21
   +- Project [cast(id#0 as string) AS id#14, cast(name#1 as string) AS name#15, cast(value#4 as string) AS value#16]
      +- Project [id#0, name#1, value#4]
         +- Filter (value#4 > 1)
            +- Join Inner, (id#0 = id#3)
               :- SubqueryAlias spark_catalog.default.t1
               :  +- Relation default.t1[id#0,name#1,date#2] parquet
               +- SubqueryAlias spark_catalog.default.t2
                  +- Relation default.t2[id#3,value#4,date#5] parquet
】--> new【
GlobalLimit 21
+- LocalLimit 21
   +- Project [cast(id#0 as string) AS id#14, cast(name#1 as string) AS name#15, cast(value#4 as string) AS value#16]
      +- Project [id#0, name#1, value#4]
         +- Filter (value#4 > 1)
            +- Join Inner, (id#0 = id#3)
               :- Relation default.t1[id#0,name#1,date#2] parquet
               +- Relation default.t2[id#3,value#4,date#5] parquet
】


SparkOptimizer(org.apache.spark.sql.internal.BaseSessionStateBuilder$$anon$2@2ae5580) org.apache.spark.sql.catalyst.optimizer.PushDownPredicates --> old【
GlobalLimit 21
+- LocalLimit 21
   +- Project [cast(id#0 as string) AS id#14, cast(name#1 as string) AS name#15, cast(value#4 as string) AS value#16]
      +- Project [id#0, name#1, value#4]
         +- Filter (value#4 > 1)
            +- Join Inner, (id#0 = id#3)
               :- Relation default.t1[id#0,name#1,date#2] parquet
               +- Relation default.t2[id#3,value#4,date#5] parquet
】--> new【
GlobalLimit 21
+- LocalLimit 21
   +- Project [cast(id#0 as string) AS id#14, cast(name#1 as string) AS name#15, cast(value#4 as string) AS value#16]
      +- Project [id#0, name#1, value#4]
         +- Join Inner, (id#0 = id#3)
            :- Relation default.t1[id#0,name#1,date#2] parquet
            +- Filter (value#4 > 1)
               +- Relation default.t2[id#3,value#4,date#5] parquet
】


SparkOptimizer(org.apache.spark.sql.internal.BaseSessionStateBuilder$$anon$2@2ae5580) org.apache.spark.sql.catalyst.optimizer.ColumnPruning --> old【
GlobalLimit 21
+- LocalLimit 21
   +- Project [cast(id#0 as string) AS id#14, cast(name#1 as string) AS name#15, cast(value#4 as string) AS value#16]
      +- Project [id#0, name#1, value#4]
         +- Join Inner, (id#0 = id#3)
            :- Relation default.t1[id#0,name#1,date#2] parquet
            +- Filter (value#4 > 1)
               +- Relation default.t2[id#3,value#4,date#5] parquet
】--> new【
GlobalLimit 21
+- LocalLimit 21
   +- Project [cast(id#0 as string) AS id#14, cast(name#1 as string) AS name#15, cast(value#4 as string) AS value#16]
      +- Project [id#0, name#1, value#4]
         +- Join Inner, (id#0 = id#3)
            :- Project [id#0, name#1]
            :  +- Relation default.t1[id#0,name#1,date#2] parquet
            +- Project [id#3, value#4]
               +- Filter (value#4 > 1)
                  +- Relation default.t2[id#3,value#4,date#5] parquet
】


SparkOptimizer(org.apache.spark.sql.internal.BaseSessionStateBuilder$$anon$2@2ae5580) org.apache.spark.sql.catalyst.optimizer.CollapseProject --> old【
GlobalLimit 21
+- LocalLimit 21
   +- Project [cast(id#0 as string) AS id#14, cast(name#1 as string) AS name#15, cast(value#4 as string) AS value#16]
      +- Project [id#0, name#1, value#4]
         +- Join Inner, (id#0 = id#3)
            :- Project [id#0, name#1]
            :  +- Relation default.t1[id#0,name#1,date#2] parquet
            +- Project [id#3, value#4]
               +- Filter (value#4 > 1)
                  +- Relation default.t2[id#3,value#4,date#5] parquet
】--> new【
GlobalLimit 21
+- LocalLimit 21
   +- Project [cast(id#0 as string) AS id#14, cast(name#1 as string) AS name#15, cast(value#4 as string) AS value#16]
      +- Join Inner, (id#0 = id#3)
         :- Project [id#0, name#1]
         :  +- Relation default.t1[id#0,name#1,date#2] parquet
         +- Project [id#3, value#4]
            +- Filter (value#4 > 1)
               +- Relation default.t2[id#3,value#4,date#5] parquet
】


SparkOptimizer(org.apache.spark.sql.internal.BaseSessionStateBuilder$$anon$2@2ae5580) org.apache.spark.sql.catalyst.optimizer.SimplifyCasts --> old【
GlobalLimit 21
+- LocalLimit 21
   +- Project [cast(id#0 as string) AS id#14, cast(name#1 as string) AS name#15, cast(value#4 as string) AS value#16]
      +- Join Inner, (id#0 = id#3)
         :- Project [id#0, name#1]
         :  +- Relation default.t1[id#0,name#1,date#2] parquet
         +- Project [id#3, value#4]
            +- Filter (value#4 > 1)
               +- Relation default.t2[id#3,value#4,date#5] parquet
】--> new【
GlobalLimit 21
+- LocalLimit 21
   +- Project [cast(id#0 as string) AS id#14, name#1 AS name#15, cast(value#4 as string) AS value#16]
      +- Join Inner, (id#0 = id#3)
         :- Project [id#0, name#1]
         :  +- Relation default.t1[id#0,name#1,date#2] parquet
         +- Project [id#3, value#4]
            +- Filter (value#4 > 1)
               +- Relation default.t2[id#3,value#4,date#5] parquet
】


SparkOptimizer(org.apache.spark.sql.internal.BaseSessionStateBuilder$$anon$2@2ae5580) org.apache.spark.sql.catalyst.optimizer.RemoveRedundantAliases --> old【
GlobalLimit 21
+- LocalLimit 21
   +- Project [cast(id#0 as string) AS id#14, name#1 AS name#15, cast(value#4 as string) AS value#16]
      +- Join Inner, (id#0 = id#3)
         :- Project [id#0, name#1]
         :  +- Relation default.t1[id#0,name#1,date#2] parquet
         +- Project [id#3, value#4]
            +- Filter (value#4 > 1)
               +- Relation default.t2[id#3,value#4,date#5] parquet
】--> new【
GlobalLimit 21
+- LocalLimit 21
   +- Project [cast(id#0 as string) AS id#14, name#1, cast(value#4 as string) AS value#16]
      +- Join Inner, (id#0 = id#3)
         :- Project [id#0, name#1]
         :  +- Relation default.t1[id#0,name#1,date#2] parquet
         +- Project [id#3, value#4]
            +- Filter (value#4 > 1)
               +- Relation default.t2[id#3,value#4,date#5] parquet
】


SparkOptimizer(org.apache.spark.sql.internal.BaseSessionStateBuilder$$anon$2@2ae5580) org.apache.spark.sql.catalyst.optimizer.InferFiltersFromConstraints --> old【
GlobalLimit 21
+- LocalLimit 21
   +- Project [cast(id#0 as string) AS id#14, name#1, cast(value#4 as string) AS value#16]
      +- Join Inner, (id#0 = id#3)
         :- Project [id#0, name#1]
         :  +- Relation default.t1[id#0,name#1,date#2] parquet
         +- Project [id#3, value#4]
            +- Filter (value#4 > 1)
               +- Relation default.t2[id#3,value#4,date#5] parquet
】--> new【
GlobalLimit 21
+- LocalLimit 21
   +- Project [cast(id#0 as string) AS id#14, name#1, cast(value#4 as string) AS value#16]
      +- Join Inner, (id#0 = id#3)
         :- Filter isnotnull(id#0)
         :  +- Project [id#0, name#1]
         :     +- Relation default.t1[id#0,name#1,date#2] parquet
         +- Filter isnotnull(id#3)
            +- Project [id#3, value#4]
               +- Filter (isnotnull(value#4) AND (value#4 > 1))
                  +- Relation default.t2[id#3,value#4,date#5] parquet
】


SparkOptimizer(org.apache.spark.sql.internal.BaseSessionStateBuilder$$anon$2@2ae5580) org.apache.spark.sql.catalyst.optimizer.PushDownPredicates --> old【
GlobalLimit 21
+- LocalLimit 21
   +- Project [cast(id#0 as string) AS id#14, name#1, cast(value#4 as string) AS value#16]
      +- Join Inner, (id#0 = id#3)
         :- Filter isnotnull(id#0)
         :  +- Project [id#0, name#1]
         :     +- Relation default.t1[id#0,name#1,date#2] parquet
         +- Filter isnotnull(id#3)
            +- Project [id#3, value#4]
               +- Filter (isnotnull(value#4) AND (value#4 > 1))
                  +- Relation default.t2[id#3,value#4,date#5] parquet
】--> new【
GlobalLimit 21
+- LocalLimit 21
   +- Project [cast(id#0 as string) AS id#14, name#1, cast(value#4 as string) AS value#16]
      +- Join Inner, (id#0 = id#3)
         :- Project [id#0, name#1]
         :  +- Filter isnotnull(id#0)
         :     +- Relation default.t1[id#0,name#1,date#2] parquet
         +- Project [id#3, value#4]
            +- Filter ((isnotnull(value#4) AND (value#4 > 1)) AND isnotnull(id#3))
               +- Relation default.t2[id#3,value#4,date#5] parquet
】


15:23:17.736 [ScalaTest-run-running-SortMergeJoinSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - Considering join on: Some((id#0 = id#3))
15:23:17.737 [ScalaTest-run-running-SortMergeJoinSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - leftKeys:List(id#0) | rightKeys:List(id#3)
15:23:17.739 [ScalaTest-run-running-SortMergeJoinSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - Considering join on: Some((id#0 = id#3))
15:23:17.739 [ScalaTest-run-running-SortMergeJoinSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - leftKeys:List(id#0) | rightKeys:List(id#3)
RuleExecutor.execute[SparkOptimizer(org.apache.spark.sql.internal.BaseSessionStateBuilder$$anon$2@2ae5580)] final result ==> old【
GlobalLimit 21
+- LocalLimit 21
   +- Project [cast(id#0 as string) AS id#14, cast(name#1 as string) AS name#15, cast(value#4 as string) AS value#16]
      +- Project [id#0, name#1, value#4]
         +- Filter (value#4 > 1)
            +- Join Inner, (id#0 = id#3)
               :- SubqueryAlias spark_catalog.default.t1
               :  +- Relation default.t1[id#0,name#1,date#2] parquet
               +- SubqueryAlias spark_catalog.default.t2
                  +- Relation default.t2[id#3,value#4,date#5] parquet
】==> new【
GlobalLimit 21
+- LocalLimit 21
   +- Project [cast(id#0 as string) AS id#14, name#1, cast(value#4 as string) AS value#16]
      +- Join Inner, (id#0 = id#3)
         :- Project [id#0, name#1]
         :  +- Filter isnotnull(id#0)
         :     +- Relation default.t1[id#0,name#1,date#2] parquet
         +- Project [id#3, value#4]
            +- Filter ((isnotnull(value#4) AND (value#4 > 1)) AND isnotnull(id#3))
               +- Relation default.t2[id#3,value#4,date#5] parquet
】


optimizedPlan ====>
GlobalLimit 21
+- LocalLimit 21
   +- Project [cast(id#0 as string) AS id#14, name#1, cast(value#4 as string) AS value#16]
      +- Join Inner, (id#0 = id#3)
         :- Project [id#0, name#1]
         :  +- Filter isnotnull(id#0)
         :     +- Relation default.t1[id#0,name#1,date#2] parquet
         +- Project [id#3, value#4]
            +- Filter ((isnotnull(value#4) AND (value#4 > 1)) AND isnotnull(id#3))
               +- Relation default.t2[id#3,value#4,date#5] parquet

SparkPlanner(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2@a2fb8a5) - org.apache.spark.sql.execution.SparkStrategies$SpecialLimits$ -->【
ReturnAnswer
+- GlobalLimit 21
   +- LocalLimit 21
      +- Project [cast(id#0 as string) AS id#14, name#1, cast(value#4 as string) AS value#16]
         +- Join Inner, (id#0 = id#3)
            :- Project [id#0, name#1]
            :  +- Filter isnotnull(id#0)
            :     +- Relation default.t1[id#0,name#1,date#2] parquet
            +- Project [id#3, value#4]
               +- Filter ((isnotnull(value#4) AND (value#4 > 1)) AND isnotnull(id#3))
                  +- Relation default.t2[id#3,value#4,date#5] parquet
】 --> 【
CollectLimit 21
+- PlanLater Project [cast(id#0 as string) AS id#14, name#1, cast(value#4 as string) AS value#16]
】

SparkPlanner(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2@a2fb8a5) - org.apache.spark.sql.execution.SparkStrategies$BasicOperators$ -->【
Project [cast(id#0 as string) AS id#14, name#1, cast(value#4 as string) AS value#16]
+- Join Inner, (id#0 = id#3)
   :- Project [id#0, name#1]
   :  +- Filter isnotnull(id#0)
   :     +- Relation default.t1[id#0,name#1,date#2] parquet
   +- Project [id#3, value#4]
      +- Filter ((isnotnull(value#4) AND (value#4 > 1)) AND isnotnull(id#3))
         +- Relation default.t2[id#3,value#4,date#5] parquet
】 --> 【
Project [cast(id#0 as string) AS id#14, name#1, cast(value#4 as string) AS value#16]
+- PlanLater Join Inner, (id#0 = id#3)
】

15:23:17.838 [ScalaTest-run-running-SortMergeJoinSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - Considering join on: Some((id#0 = id#3))
15:23:17.838 [ScalaTest-run-running-SortMergeJoinSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - leftKeys:List(id#0) | rightKeys:List(id#3)
15:23:17.839 [ScalaTest-run-running-SortMergeJoinSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - Considering join on: Some((id#0 = id#3))
15:23:17.839 [ScalaTest-run-running-SortMergeJoinSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - leftKeys:List(id#0) | rightKeys:List(id#3)
SparkPlanner(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2@a2fb8a5) - org.apache.spark.sql.execution.SparkStrategies$JoinSelection$ -->【
Join Inner, (id#0 = id#3)
:- Project [id#0, name#1]
:  +- Filter isnotnull(id#0)
:     +- Relation default.t1[id#0,name#1,date#2] parquet
+- Project [id#3, value#4]
   +- Filter ((isnotnull(value#4) AND (value#4 > 1)) AND isnotnull(id#3))
      +- Relation default.t2[id#3,value#4,date#5] parquet
】 --> 【
SortMergeJoin [id#0], [id#3], Inner
:- PlanLater Project [id#0, name#1]
+- PlanLater Project [id#3, value#4]
】

15:23:17.859 [ScalaTest-run-running-SortMergeJoinSpec] INFO  org.apache.spark.sql.execution.datasources.DataSourceStrategy - Pruning directories with:
15:23:17.863 [ScalaTest-run-running-SortMergeJoinSpec] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: IsNotNull(id)
15:23:17.864 [ScalaTest-run-running-SortMergeJoinSpec] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(id#0)
15:23:17.865 [ScalaTest-run-running-SortMergeJoinSpec] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<id: int, name: string>
SparkPlanner(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2@a2fb8a5) - org.apache.spark.sql.execution.datasources.FileSourceStrategy$ -->【
Project [id#0, name#1]
+- Filter isnotnull(id#0)
   +- Relation default.t1[id#0,name#1,date#2] parquet
】 --> 【
Project [id#0, name#1]
+- Filter isnotnull(id#0)
   +- FileScan parquet default.t1[id#0,name#1,date#2] Batched: true, DataFilters: [isnotnull(id#0)], Format: Parquet, Location: CatalogFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-wareh..., PartitionFilters: [], PushedFilters: [IsNotNull(id)], ReadSchema: struct<id:int,name:string>
】

15:23:17.889 [ScalaTest-run-running-SortMergeJoinSpec] INFO  org.apache.spark.sql.execution.datasources.DataSourceStrategy - Pruning directories with:
15:23:17.890 [ScalaTest-run-running-SortMergeJoinSpec] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: IsNotNull(value),GreaterThan(value,1),IsNotNull(id)
15:23:17.891 [ScalaTest-run-running-SortMergeJoinSpec] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(value#4),(value#4 > 1),isnotnull(id#3)
15:23:17.891 [ScalaTest-run-running-SortMergeJoinSpec] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<id: int, value: int>
SparkPlanner(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2@a2fb8a5) - org.apache.spark.sql.execution.datasources.FileSourceStrategy$ -->【
Project [id#3, value#4]
+- Filter ((isnotnull(value#4) AND (value#4 > 1)) AND isnotnull(id#3))
   +- Relation default.t2[id#3,value#4,date#5] parquet
】 --> 【
Project [id#3, value#4]
+- Filter ((isnotnull(value#4) AND (value#4 > 1)) AND isnotnull(id#3))
   +- FileScan parquet default.t2[id#3,value#4,date#5] Batched: true, DataFilters: [isnotnull(value#4), (value#4 > 1), isnotnull(id#3)], Format: Parquet, Location: CatalogFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-wareh..., PartitionFilters: [], PushedFilters: [IsNotNull(value), GreaterThan(value,1), IsNotNull(id)], ReadSchema: struct<id:int,value:int>
】

sparkPlan ====>
CollectLimit 21
+- Project [cast(id#0 as string) AS id#14, name#1, cast(value#4 as string) AS value#16]
   +- SortMergeJoin [id#0], [id#3], Inner
      :- Project [id#0, name#1]
      :  +- Filter isnotnull(id#0)
      :     +- FileScan parquet default.t1[id#0,name#1,date#2] Batched: true, DataFilters: [isnotnull(id#0)], Format: Parquet, Location: CatalogFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-wareh..., PartitionFilters: [], PushedFilters: [IsNotNull(id)], ReadSchema: struct<id:int,name:string>
      +- Project [id#3, value#4]
         +- Filter ((isnotnull(value#4) AND (value#4 > 1)) AND isnotnull(id#3))
            +- FileScan parquet default.t2[id#3,value#4,date#5] Batched: true, DataFilters: [isnotnull(value#4), (value#4 > 1), isnotnull(id#3)], Format: Parquet, Location: CatalogFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-wareh..., PartitionFilters: [], PushedFilters: [IsNotNull(value), GreaterThan(value,1), IsNotNull(id)], ReadSchema: struct<id:int,value:int>

15:23:17.908 [ScalaTest-run-running-SortMergeJoinSpec] DEBUG org.apache.spark.sql.execution.adaptive.InsertAdaptiveSparkPlan - Adaptive execution enabled for plan:
CollectLimit 21
+- Project [cast(id#0 as string) AS id#14, name#1, cast(value#4 as string) AS value#16]
   +- SortMergeJoin [id#0], [id#3], Inner
      :- Project [id#0, name#1]
      :  +- Filter isnotnull(id#0)
      :     +- FileScan parquet default.t1[id#0,name#1,date#2] Batched: true, DataFilters: [isnotnull(id#0)], Format: Parquet, Location: CatalogFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-wareh..., PartitionFilters: [], PushedFilters: [IsNotNull(id)], ReadSchema: struct<id:int,name:string>
      +- Project [id#3, value#4]
         +- Filter ((isnotnull(value#4) AND (value#4 > 1)) AND isnotnull(id#3))
            +- FileScan parquet default.t2[id#3,value#4,date#5] Batched: true, DataFilters: [isnotnull(value#4), (value#4 > 1), isnotnull(id#3)], Format: Parquet, Location: CatalogFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-wareh..., PartitionFilters: [], PushedFilters: [IsNotNull(value), GreaterThan(value,1), IsNotNull(id)], ReadSchema: struct<id:int,value:int>

org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec$ - applyPhysicalRules - org.apache.spark.sql.execution.exchange.EnsureRequirements -->【
CollectLimit 21
+- Project [cast(id#0 as string) AS id#14, name#1, cast(value#4 as string) AS value#16]
   +- SortMergeJoin [id#0], [id#3], Inner
      :- Project [id#0, name#1]
      :  +- Filter isnotnull(id#0)
      :     +- FileScan parquet default.t1[id#0,name#1,date#2] Batched: true, DataFilters: [isnotnull(id#0)], Format: Parquet, Location: CatalogFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-wareh..., PartitionFilters: [], PushedFilters: [IsNotNull(id)], ReadSchema: struct<id:int,name:string>
      +- Project [id#3, value#4]
         +- Filter ((isnotnull(value#4) AND (value#4 > 1)) AND isnotnull(id#3))
            +- FileScan parquet default.t2[id#3,value#4,date#5] Batched: true, DataFilters: [isnotnull(value#4), (value#4 > 1), isnotnull(id#3)], Format: Parquet, Location: CatalogFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-wareh..., PartitionFilters: [], PushedFilters: [IsNotNull(value), GreaterThan(value,1), IsNotNull(id)], ReadSchema: struct<id:int,value:int>
】AQE --> 【
CollectLimit 21
+- Project [cast(id#0 as string) AS id#14, name#1, cast(value#4 as string) AS value#16]
   +- SortMergeJoin [id#0], [id#3], Inner
      :- Sort [id#0 ASC NULLS FIRST], false, 0
      :  +- Exchange hashpartitioning(id#0, 200), ENSURE_REQUIREMENTS, [plan_id=28]
      :     +- Project [id#0, name#1]
      :        +- Filter isnotnull(id#0)
      :           +- FileScan parquet default.t1[id#0,name#1,date#2] Batched: true, DataFilters: [isnotnull(id#0)], Format: Parquet, Location: CatalogFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-wareh..., PartitionFilters: [], PushedFilters: [IsNotNull(id)], ReadSchema: struct<id:int,name:string>
      +- Sort [id#3 ASC NULLS FIRST], false, 0
         +- Exchange hashpartitioning(id#3, 200), ENSURE_REQUIREMENTS, [plan_id=29]
            +- Project [id#3, value#4]
               +- Filter ((isnotnull(value#4) AND (value#4 > 1)) AND isnotnull(id#3))
                  +- FileScan parquet default.t2[id#3,value#4,date#5] Batched: true, DataFilters: [isnotnull(value#4), (value#4 > 1), isnotnull(id#3)], Format: Parquet, Location: CatalogFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-wareh..., PartitionFilters: [], PushedFilters: [IsNotNull(value), GreaterThan(value,1), IsNotNull(id)], ReadSchema: struct<id:int,value:int>
】

org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec$ final result ==>【
CollectLimit 21
+- Project [cast(id#0 as string) AS id#14, name#1, cast(value#4 as string) AS value#16]
   +- SortMergeJoin [id#0], [id#3], Inner
      :- Project [id#0, name#1]
      :  +- Filter isnotnull(id#0)
      :     +- FileScan parquet default.t1[id#0,name#1,date#2] Batched: true, DataFilters: [isnotnull(id#0)], Format: Parquet, Location: CatalogFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-wareh..., PartitionFilters: [], PushedFilters: [IsNotNull(id)], ReadSchema: struct<id:int,name:string>
      +- Project [id#3, value#4]
         +- Filter ((isnotnull(value#4) AND (value#4 > 1)) AND isnotnull(id#3))
            +- FileScan parquet default.t2[id#3,value#4,date#5] Batched: true, DataFilters: [isnotnull(value#4), (value#4 > 1), isnotnull(id#3)], Format: Parquet, Location: CatalogFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-wareh..., PartitionFilters: [], PushedFilters: [IsNotNull(value), GreaterThan(value,1), IsNotNull(id)], ReadSchema: struct<id:int,value:int>
】AQE ==> 【
CollectLimit 21
+- Project [cast(id#0 as string) AS id#14, name#1, cast(value#4 as string) AS value#16]
   +- SortMergeJoin [id#0], [id#3], Inner
      :- Sort [id#0 ASC NULLS FIRST], false, 0
      :  +- Exchange hashpartitioning(id#0, 200), ENSURE_REQUIREMENTS, [plan_id=28]
      :     +- Project [id#0, name#1]
      :        +- Filter isnotnull(id#0)
      :           +- FileScan parquet default.t1[id#0,name#1,date#2] Batched: true, DataFilters: [isnotnull(id#0)], Format: Parquet, Location: CatalogFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-wareh..., PartitionFilters: [], PushedFilters: [IsNotNull(id)], ReadSchema: struct<id:int,name:string>
      +- Sort [id#3 ASC NULLS FIRST], false, 0
         +- Exchange hashpartitioning(id#3, 200), ENSURE_REQUIREMENTS, [plan_id=29]
            +- Project [id#3, value#4]
               +- Filter ((isnotnull(value#4) AND (value#4 > 1)) AND isnotnull(id#3))
                  +- FileScan parquet default.t2[id#3,value#4,date#5] Batched: true, DataFilters: [isnotnull(value#4), (value#4 > 1), isnotnull(id#3)], Format: Parquet, Location: CatalogFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-wareh..., PartitionFilters: [], PushedFilters: [IsNotNull(value), GreaterThan(value,1), IsNotNull(id)], ReadSchema: struct<id:int,value:int>
】

org.apache.spark.sql.execution.QueryExecution$ - org.apache.spark.sql.execution.adaptive.InsertAdaptiveSparkPlan -->【
CollectLimit 21
+- Project [cast(id#0 as string) AS id#14, name#1, cast(value#4 as string) AS value#16]
   +- SortMergeJoin [id#0], [id#3], Inner
      :- Project [id#0, name#1]
      :  +- Filter isnotnull(id#0)
      :     +- FileScan parquet default.t1[id#0,name#1,date#2] Batched: true, DataFilters: [isnotnull(id#0)], Format: Parquet, Location: CatalogFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-wareh..., PartitionFilters: [], PushedFilters: [IsNotNull(id)], ReadSchema: struct<id:int,name:string>
      +- Project [id#3, value#4]
         +- Filter ((isnotnull(value#4) AND (value#4 > 1)) AND isnotnull(id#3))
            +- FileScan parquet default.t2[id#3,value#4,date#5] Batched: true, DataFilters: [isnotnull(value#4), (value#4 > 1), isnotnull(id#3)], Format: Parquet, Location: CatalogFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-wareh..., PartitionFilters: [], PushedFilters: [IsNotNull(value), GreaterThan(value,1), IsNotNull(id)], ReadSchema: struct<id:int,value:int>
】 --> 【
AdaptiveSparkPlan isFinalPlan=false
+- CollectLimit 21
   +- Project [cast(id#0 as string) AS id#14, name#1, cast(value#4 as string) AS value#16]
      +- SortMergeJoin [id#0], [id#3], Inner
         :- Sort [id#0 ASC NULLS FIRST], false, 0
         :  +- Exchange hashpartitioning(id#0, 200), ENSURE_REQUIREMENTS, [plan_id=28]
         :     +- Project [id#0, name#1]
         :        +- Filter isnotnull(id#0)
         :           +- FileScan parquet default.t1[id#0,name#1,date#2] Batched: true, DataFilters: [isnotnull(id#0)], Format: Parquet, Location: CatalogFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-wareh..., PartitionFilters: [], PushedFilters: [IsNotNull(id)], ReadSchema: struct<id:int,name:string>
         +- Sort [id#3 ASC NULLS FIRST], false, 0
            +- Exchange hashpartitioning(id#3, 200), ENSURE_REQUIREMENTS, [plan_id=29]
               +- Project [id#3, value#4]
                  +- Filter ((isnotnull(value#4) AND (value#4 > 1)) AND isnotnull(id#3))
                     +- FileScan parquet default.t2[id#3,value#4,date#5] Batched: true, DataFilters: [isnotnull(value#4), (value#4 > 1), isnotnull(id#3)], Format: Parquet, Location: CatalogFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-wareh..., PartitionFilters: [], PushedFilters: [IsNotNull(value), GreaterThan(value,1), IsNotNull(id)], ReadSchema: struct<id:int,value:int>
】

org.apache.spark.sql.execution.QueryExecution$ prepareForExecution final result ==>【
CollectLimit 21
+- Project [cast(id#0 as string) AS id#14, name#1, cast(value#4 as string) AS value#16]
   +- SortMergeJoin [id#0], [id#3], Inner
      :- Project [id#0, name#1]
      :  +- Filter isnotnull(id#0)
      :     +- FileScan parquet default.t1[id#0,name#1,date#2] Batched: true, DataFilters: [isnotnull(id#0)], Format: Parquet, Location: CatalogFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-wareh..., PartitionFilters: [], PushedFilters: [IsNotNull(id)], ReadSchema: struct<id:int,name:string>
      +- Project [id#3, value#4]
         +- Filter ((isnotnull(value#4) AND (value#4 > 1)) AND isnotnull(id#3))
            +- FileScan parquet default.t2[id#3,value#4,date#5] Batched: true, DataFilters: [isnotnull(value#4), (value#4 > 1), isnotnull(id#3)], Format: Parquet, Location: CatalogFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-wareh..., PartitionFilters: [], PushedFilters: [IsNotNull(value), GreaterThan(value,1), IsNotNull(id)], ReadSchema: struct<id:int,value:int>
】 ==> 【
AdaptiveSparkPlan isFinalPlan=false
+- CollectLimit 21
   +- Project [cast(id#0 as string) AS id#14, name#1, cast(value#4 as string) AS value#16]
      +- SortMergeJoin [id#0], [id#3], Inner
         :- Sort [id#0 ASC NULLS FIRST], false, 0
         :  +- Exchange hashpartitioning(id#0, 200), ENSURE_REQUIREMENTS, [plan_id=28]
         :     +- Project [id#0, name#1]
         :        +- Filter isnotnull(id#0)
         :           +- FileScan parquet default.t1[id#0,name#1,date#2] Batched: true, DataFilters: [isnotnull(id#0)], Format: Parquet, Location: CatalogFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-wareh..., PartitionFilters: [], PushedFilters: [IsNotNull(id)], ReadSchema: struct<id:int,name:string>
         +- Sort [id#3 ASC NULLS FIRST], false, 0
            +- Exchange hashpartitioning(id#3, 200), ENSURE_REQUIREMENTS, [plan_id=29]
               +- Project [id#3, value#4]
                  +- Filter ((isnotnull(value#4) AND (value#4 > 1)) AND isnotnull(id#3))
                     +- FileScan parquet default.t2[id#3,value#4,date#5] Batched: true, DataFilters: [isnotnull(value#4), (value#4 > 1), isnotnull(id#3)], Format: Parquet, Location: CatalogFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-wareh..., PartitionFilters: [], PushedFilters: [IsNotNull(value), GreaterThan(value,1), IsNotNull(id)], ReadSchema: struct<id:int,value:int>
】

executedPlan ====>
AdaptiveSparkPlan isFinalPlan=false
+- CollectLimit 21
   +- Project [cast(id#0 as string) AS id#14, name#1, cast(value#4 as string) AS value#16]
      +- SortMergeJoin [id#0], [id#3], Inner
         :- Sort [id#0 ASC NULLS FIRST], false, 0
         :  +- Exchange hashpartitioning(id#0, 200), ENSURE_REQUIREMENTS, [plan_id=28]
         :     +- Project [id#0, name#1]
         :        +- Filter isnotnull(id#0)
         :           +- FileScan parquet default.t1[id#0,name#1,date#2] Batched: true, DataFilters: [isnotnull(id#0)], Format: Parquet, Location: CatalogFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-wareh..., PartitionFilters: [], PushedFilters: [IsNotNull(id)], ReadSchema: struct<id:int,name:string>
         +- Sort [id#3 ASC NULLS FIRST], false, 0
            +- Exchange hashpartitioning(id#3, 200), ENSURE_REQUIREMENTS, [plan_id=29]
               +- Project [id#3, value#4]
                  +- Filter ((isnotnull(value#4) AND (value#4 > 1)) AND isnotnull(id#3))
                     +- FileScan parquet default.t2[id#3,value#4,date#5] Batched: true, DataFilters: [isnotnull(value#4), (value#4 > 1), isnotnull(id#3)], Format: Parquet, Location: CatalogFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-wareh..., PartitionFilters: [], PushedFilters: [IsNotNull(value), GreaterThan(value,1), IsNotNull(id)], ReadSchema: struct<id:int,value:int>

Analyzer(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$1@25f7cc38) org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveDeserializer --> old【
'DeserializeToObject unresolveddeserializer(createexternalrow(getcolumnbyordinal(0, StructField(id,StringType,true), StructField(name,StringType,true), StructField(value,StringType,true)).toString, getcolumnbyordinal(1, StructField(id,StringType,true), StructField(name,StringType,true), StructField(value,StringType,true)).toString, getcolumnbyordinal(2, StructField(id,StringType,true), StructField(name,StringType,true), StructField(value,StringType,true)).toString, StructField(id,StringType,true), StructField(name,StringType,true), StructField(value,StringType,true))), obj#20: org.apache.spark.sql.Row
+- LocalRelation <empty>, [id#14, name#15, value#16]
】--> new【
DeserializeToObject createexternalrow(id#14.toString, name#15.toString, value#16.toString, StructField(id,StringType,true), StructField(name,StringType,true), StructField(value,StringType,true)), obj#20: org.apache.spark.sql.Row
+- LocalRelation <empty>, [id#14, name#15, value#16]
】


RuleExecutor.execute[Analyzer(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$1@25f7cc38)] final result ==> old【
'DeserializeToObject unresolveddeserializer(createexternalrow(getcolumnbyordinal(0, StructField(id,StringType,true), StructField(name,StringType,true), StructField(value,StringType,true)).toString, getcolumnbyordinal(1, StructField(id,StringType,true), StructField(name,StringType,true), StructField(value,StringType,true)).toString, getcolumnbyordinal(2, StructField(id,StringType,true), StructField(name,StringType,true), StructField(value,StringType,true)).toString, StructField(id,StringType,true), StructField(name,StringType,true), StructField(value,StringType,true))), obj#20: org.apache.spark.sql.Row
+- LocalRelation <empty>, [id#14, name#15, value#16]
】==> new【
DeserializeToObject createexternalrow(id#14.toString, name#15.toString, value#16.toString, StructField(id,StringType,true), StructField(name,StringType,true), StructField(value,StringType,true)), obj#20: org.apache.spark.sql.Row
+- LocalRelation <empty>, [id#14, name#15, value#16]
】


====== collectFromPlan ======
AdaptiveSparkPlan isFinalPlan=false
+- CollectLimit 21
   +- Project [cast(id#0 as string) AS id#14, name#1, cast(value#4 as string) AS value#16]
      +- SortMergeJoin [id#0], [id#3], Inner
         :- Sort [id#0 ASC NULLS FIRST], false, 0
         :  +- Exchange hashpartitioning(id#0, 200), ENSURE_REQUIREMENTS, [plan_id=28]
         :     +- Project [id#0, name#1]
         :        +- Filter isnotnull(id#0)
         :           +- FileScan parquet default.t1[id#0,name#1,date#2] Batched: true, DataFilters: [isnotnull(id#0)], Format: Parquet, Location: CatalogFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-wareh..., PartitionFilters: [], PushedFilters: [IsNotNull(id)], ReadSchema: struct<id:int,name:string>
         +- Sort [id#3 ASC NULLS FIRST], false, 0
            +- Exchange hashpartitioning(id#3, 200), ENSURE_REQUIREMENTS, [plan_id=29]
               +- Project [id#3, value#4]
                  +- Filter ((isnotnull(value#4) AND (value#4 > 1)) AND isnotnull(id#3))
                     +- FileScan parquet default.t2[id#3,value#4,date#5] Batched: true, DataFilters: [isnotnull(value#4), (value#4 > 1), isnotnull(id#3)], Format: Parquet, Location: CatalogFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-wareh..., PartitionFilters: [], PushedFilters: [IsNotNull(value), GreaterThan(value,1), IsNotNull(id)], ReadSchema: struct<id:int,value:int>

15:23:18.028 [ScalaTest-run-running-SortMergeJoinSpec] DEBUG org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec - withFinalPlanUpdate.executeCollect ==> before get final plan:
CollectLimit 21
+- Project [cast(id#0 as string) AS id#14, name#1, cast(value#4 as string) AS value#16]
   +- SortMergeJoin [id#0], [id#3], Inner
      :- Sort [id#0 ASC NULLS FIRST], false, 0
      :  +- Exchange hashpartitioning(id#0, 200), ENSURE_REQUIREMENTS, [plan_id=28]
      :     +- Project [id#0, name#1]
      :        +- Filter isnotnull(id#0)
      :           +- FileScan parquet default.t1[id#0,name#1,date#2] Batched: true, DataFilters: [isnotnull(id#0)], Format: Parquet, Location: CatalogFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-wareh..., PartitionFilters: [], PushedFilters: [IsNotNull(id)], ReadSchema: struct<id:int,name:string>
      +- Sort [id#3 ASC NULLS FIRST], false, 0
         +- Exchange hashpartitioning(id#3, 200), ENSURE_REQUIREMENTS, [plan_id=29]
            +- Project [id#3, value#4]
               +- Filter ((isnotnull(value#4) AND (value#4 > 1)) AND isnotnull(id#3))
                  +- FileScan parquet default.t2[id#3,value#4,date#5] Batched: true, DataFilters: [isnotnull(value#4), (value#4 > 1), isnotnull(id#3)], Format: Parquet, Location: CatalogFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-wareh..., PartitionFilters: [], PushedFilters: [IsNotNull(value), GreaterThan(value,1), IsNotNull(id)], ReadSchema: struct<id:int,value:int>

org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec$ - applyPhysicalRules - org.apache.spark.sql.execution.ApplyColumnarRulesAndInsertTransitions -->【
Exchange hashpartitioning(id#0, 200), ENSURE_REQUIREMENTS, [plan_id=28]
+- Project [id#0, name#1]
   +- Filter isnotnull(id#0)
      +- FileScan parquet default.t1[id#0,name#1,date#2] Batched: true, DataFilters: [isnotnull(id#0)], Format: Parquet, Location: CatalogFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-wareh..., PartitionFilters: [], PushedFilters: [IsNotNull(id)], ReadSchema: struct<id:int,name:string>
】AQE --> 【
Exchange hashpartitioning(id#0, 200), ENSURE_REQUIREMENTS, [plan_id=45]
+- Project [id#0, name#1]
   +- Filter isnotnull(id#0)
      +- ColumnarToRow
         +- FileScan parquet default.t1[id#0,name#1,date#2] Batched: true, DataFilters: [isnotnull(id#0)], Format: Parquet, Location: CatalogFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-wareh..., PartitionFilters: [], PushedFilters: [IsNotNull(id)], ReadSchema: struct<id:int,name:string>
】

org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec$ - applyPhysicalRules - org.apache.spark.sql.execution.CollapseCodegenStages -->【
Exchange hashpartitioning(id#0, 200), ENSURE_REQUIREMENTS, [plan_id=45]
+- Project [id#0, name#1]
   +- Filter isnotnull(id#0)
      +- ColumnarToRow
         +- FileScan parquet default.t1[id#0,name#1,date#2] Batched: true, DataFilters: [isnotnull(id#0)], Format: Parquet, Location: CatalogFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-wareh..., PartitionFilters: [], PushedFilters: [IsNotNull(id)], ReadSchema: struct<id:int,name:string>
】AQE --> 【
Exchange hashpartitioning(id#0, 200), ENSURE_REQUIREMENTS, [plan_id=51]
+- *(1) Project [id#0, name#1]
   +- *(1) Filter isnotnull(id#0)
      +- *(1) ColumnarToRow
         +- FileScan parquet default.t1[id#0,name#1,date#2] Batched: true, DataFilters: [isnotnull(id#0)], Format: Parquet, Location: CatalogFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-wareh..., PartitionFilters: [], PushedFilters: [IsNotNull(id)], ReadSchema: struct<id:int,name:string>
】

org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec$ final result ==>【
Exchange hashpartitioning(id#0, 200), ENSURE_REQUIREMENTS, [plan_id=28]
+- Project [id#0, name#1]
   +- Filter isnotnull(id#0)
      +- FileScan parquet default.t1[id#0,name#1,date#2] Batched: true, DataFilters: [isnotnull(id#0)], Format: Parquet, Location: CatalogFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-wareh..., PartitionFilters: [], PushedFilters: [IsNotNull(id)], ReadSchema: struct<id:int,name:string>
】AQE ==> 【
Exchange hashpartitioning(id#0, 200), ENSURE_REQUIREMENTS, [plan_id=51]
+- *(1) Project [id#0, name#1]
   +- *(1) Filter isnotnull(id#0)
      +- *(1) ColumnarToRow
         +- FileScan parquet default.t1[id#0,name#1,date#2] Batched: true, DataFilters: [isnotnull(id#0)], Format: Parquet, Location: CatalogFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-wareh..., PartitionFilters: [], PushedFilters: [IsNotNull(id)], ReadSchema: struct<id:int,name:string>
】

org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec$ - applyPhysicalRules - org.apache.spark.sql.execution.ApplyColumnarRulesAndInsertTransitions -->【
Exchange hashpartitioning(id#3, 200), ENSURE_REQUIREMENTS, [plan_id=29]
+- Project [id#3, value#4]
   +- Filter ((isnotnull(value#4) AND (value#4 > 1)) AND isnotnull(id#3))
      +- FileScan parquet default.t2[id#3,value#4,date#5] Batched: true, DataFilters: [isnotnull(value#4), (value#4 > 1), isnotnull(id#3)], Format: Parquet, Location: CatalogFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-wareh..., PartitionFilters: [], PushedFilters: [IsNotNull(value), GreaterThan(value,1), IsNotNull(id)], ReadSchema: struct<id:int,value:int>
】AQE --> 【
Exchange hashpartitioning(id#3, 200), ENSURE_REQUIREMENTS, [plan_id=73]
+- Project [id#3, value#4]
   +- Filter ((isnotnull(value#4) AND (value#4 > 1)) AND isnotnull(id#3))
      +- ColumnarToRow
         +- FileScan parquet default.t2[id#3,value#4,date#5] Batched: true, DataFilters: [isnotnull(value#4), (value#4 > 1), isnotnull(id#3)], Format: Parquet, Location: CatalogFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-wareh..., PartitionFilters: [], PushedFilters: [IsNotNull(value), GreaterThan(value,1), IsNotNull(id)], ReadSchema: struct<id:int,value:int>
】

org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec$ - applyPhysicalRules - org.apache.spark.sql.execution.CollapseCodegenStages -->【
Exchange hashpartitioning(id#3, 200), ENSURE_REQUIREMENTS, [plan_id=73]
+- Project [id#3, value#4]
   +- Filter ((isnotnull(value#4) AND (value#4 > 1)) AND isnotnull(id#3))
      +- ColumnarToRow
         +- FileScan parquet default.t2[id#3,value#4,date#5] Batched: true, DataFilters: [isnotnull(value#4), (value#4 > 1), isnotnull(id#3)], Format: Parquet, Location: CatalogFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-wareh..., PartitionFilters: [], PushedFilters: [IsNotNull(value), GreaterThan(value,1), IsNotNull(id)], ReadSchema: struct<id:int,value:int>
】AQE --> 【
Exchange hashpartitioning(id#3, 200), ENSURE_REQUIREMENTS, [plan_id=79]
+- *(2) Project [id#3, value#4]
   +- *(2) Filter ((isnotnull(value#4) AND (value#4 > 1)) AND isnotnull(id#3))
      +- *(2) ColumnarToRow
         +- FileScan parquet default.t2[id#3,value#4,date#5] Batched: true, DataFilters: [isnotnull(value#4), (value#4 > 1), isnotnull(id#3)], Format: Parquet, Location: CatalogFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-wareh..., PartitionFilters: [], PushedFilters: [IsNotNull(value), GreaterThan(value,1), IsNotNull(id)], ReadSchema: struct<id:int,value:int>
】

org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec$ final result ==>【
Exchange hashpartitioning(id#3, 200), ENSURE_REQUIREMENTS, [plan_id=29]
+- Project [id#3, value#4]
   +- Filter ((isnotnull(value#4) AND (value#4 > 1)) AND isnotnull(id#3))
      +- FileScan parquet default.t2[id#3,value#4,date#5] Batched: true, DataFilters: [isnotnull(value#4), (value#4 > 1), isnotnull(id#3)], Format: Parquet, Location: CatalogFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-wareh..., PartitionFilters: [], PushedFilters: [IsNotNull(value), GreaterThan(value,1), IsNotNull(id)], ReadSchema: struct<id:int,value:int>
】AQE ==> 【
Exchange hashpartitioning(id#3, 200), ENSURE_REQUIREMENTS, [plan_id=79]
+- *(2) Project [id#3, value#4]
   +- *(2) Filter ((isnotnull(value#4) AND (value#4 > 1)) AND isnotnull(id#3))
      +- *(2) ColumnarToRow
         +- FileScan parquet default.t2[id#3,value#4,date#5] Batched: true, DataFilters: [isnotnull(value#4), (value#4 > 1), isnotnull(id#3)], Format: Parquet, Location: CatalogFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-wareh..., PartitionFilters: [], PushedFilters: [IsNotNull(value), GreaterThan(value,1), IsNotNull(id)], ReadSchema: struct<id:int,value:int>
】

15:23:18.069 [ScalaTest-run-running-SortMergeJoinSpec] DEBUG org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec - Materialize query stage ShuffleQueryStageExec: 0, query stage plan:
Exchange hashpartitioning(id#0, 200), ENSURE_REQUIREMENTS, [plan_id=51]
+- *(1) Project [id#0, name#1]
   +- *(1) Filter isnotnull(id#0)
      +- *(1) ColumnarToRow
         +- FileScan parquet default.t1[id#0,name#1,date#2] Batched: true, DataFilters: [isnotnull(id#0)], Format: Parquet, Location: CatalogFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-wareh..., PartitionFilters: [], PushedFilters: [IsNotNull(id)], ReadSchema: struct<id:int,name:string>

15:23:18.392 [ScalaTest-run-running-SortMergeJoinSpec] DEBUG org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator -
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage1(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=1
/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private int columnartorow_batchIdx_0;
/* 010 */   private org.apache.spark.sql.execution.vectorized.OnHeapColumnVector[] columnartorow_mutableStateArray_2 = new org.apache.spark.sql.execution.vectorized.OnHeapColumnVector[3];
/* 011 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] columnartorow_mutableStateArray_3 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[3];
/* 012 */   private org.apache.spark.sql.vectorized.ColumnarBatch[] columnartorow_mutableStateArray_1 = new org.apache.spark.sql.vectorized.ColumnarBatch[1];
/* 013 */   private scala.collection.Iterator[] columnartorow_mutableStateArray_0 = new scala.collection.Iterator[1];
/* 014 */
/* 015 */   public GeneratedIteratorForCodegenStage1(Object[] references) {
/* 016 */     this.references = references;
/* 017 */   }
/* 018 */
/* 019 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 020 */     partitionIndex = index;
/* 021 */     this.inputs = inputs;
/* 022 */     columnartorow_mutableStateArray_0[0] = inputs[0];
/* 023 */
/* 024 */     columnartorow_mutableStateArray_3[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(3, 64);
/* 025 */     columnartorow_mutableStateArray_3[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(3, 64);
/* 026 */     columnartorow_mutableStateArray_3[2] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 027 */
/* 028 */   }
/* 029 */
/* 030 */   private void columnartorow_nextBatch_0() throws java.io.IOException {
/* 031 */     if (columnartorow_mutableStateArray_0[0].hasNext()) {
/* 032 */       columnartorow_mutableStateArray_1[0] = (org.apache.spark.sql.vectorized.ColumnarBatch)columnartorow_mutableStateArray_0[0].next();
/* 033 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* numInputBatches */).add(1);
/* 034 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(columnartorow_mutableStateArray_1[0].numRows());
/* 035 */       columnartorow_batchIdx_0 = 0;
/* 036 */       columnartorow_mutableStateArray_2[0] = (org.apache.spark.sql.execution.vectorized.OnHeapColumnVector) columnartorow_mutableStateArray_1[0].column(0);
/* 037 */       columnartorow_mutableStateArray_2[1] = (org.apache.spark.sql.execution.vectorized.OnHeapColumnVector) columnartorow_mutableStateArray_1[0].column(1);
/* 038 */       columnartorow_mutableStateArray_2[2] = (org.apache.spark.sql.execution.vectorized.OnHeapColumnVector) columnartorow_mutableStateArray_1[0].column(2);
/* 039 */
/* 040 */     }
/* 041 */   }
/* 042 */
/* 043 */   protected void processNext() throws java.io.IOException {
/* 044 */     if (columnartorow_mutableStateArray_1[0] == null) {
/* 045 */       columnartorow_nextBatch_0();
/* 046 */     }
/* 047 */     while ( columnartorow_mutableStateArray_1[0] != null) {
/* 048 */       int columnartorow_numRows_0 = columnartorow_mutableStateArray_1[0].numRows();
/* 049 */       int columnartorow_localEnd_0 = columnartorow_numRows_0 - columnartorow_batchIdx_0;
/* 050 */       for (int columnartorow_localIdx_0 = 0; columnartorow_localIdx_0 < columnartorow_localEnd_0; columnartorow_localIdx_0++) {
/* 051 */         int columnartorow_rowIdx_0 = columnartorow_batchIdx_0 + columnartorow_localIdx_0;
/* 052 */         do {
/* 053 */           boolean columnartorow_isNull_0 = columnartorow_mutableStateArray_2[0].isNullAt(columnartorow_rowIdx_0);
/* 054 */           int columnartorow_value_0 = columnartorow_isNull_0 ? -1 : (columnartorow_mutableStateArray_2[0].getInt(columnartorow_rowIdx_0));
/* 055 */
/* 056 */           boolean filter_value_2 = !columnartorow_isNull_0;
/* 057 */           if (!filter_value_2) continue;
/* 058 */
/* 059 */           ((org.apache.spark.sql.execution.metric.SQLMetric) references[2] /* numOutputRows */).add(1);
/* 060 */
/* 061 */           // common sub-expressions
/* 062 */
/* 063 */           boolean columnartorow_isNull_1 = columnartorow_mutableStateArray_2[1].isNullAt(columnartorow_rowIdx_0);
/* 064 */           UTF8String columnartorow_value_1 = columnartorow_isNull_1 ? null : (columnartorow_mutableStateArray_2[1].getUTF8String(columnartorow_rowIdx_0));
/* 065 */           columnartorow_mutableStateArray_3[2].reset();
/* 066 */
/* 067 */           columnartorow_mutableStateArray_3[2].zeroOutNullBytes();
/* 068 */
/* 069 */           if (false) {
/* 070 */             columnartorow_mutableStateArray_3[2].setNullAt(0);
/* 071 */           } else {
/* 072 */             columnartorow_mutableStateArray_3[2].write(0, columnartorow_value_0);
/* 073 */           }
/* 074 */
/* 075 */           if (columnartorow_isNull_1) {
/* 076 */             columnartorow_mutableStateArray_3[2].setNullAt(1);
/* 077 */           } else {
/* 078 */             columnartorow_mutableStateArray_3[2].write(1, columnartorow_value_1);
/* 079 */           }
/* 080 */           append((columnartorow_mutableStateArray_3[2].getRow()));
/* 081 */
/* 082 */         } while(false);
/* 083 */         if (shouldStop()) { columnartorow_batchIdx_0 = columnartorow_rowIdx_0 + 1; return; }
/* 084 */       }
/* 085 */       columnartorow_batchIdx_0 = columnartorow_numRows_0;
/* 086 */       columnartorow_mutableStateArray_1[0] = null;
/* 087 */       columnartorow_nextBatch_0();
/* 088 */     }
/* 089 */   }
/* 090 */
/* 091 */ }

15:23:18.576 [ScalaTest-run-running-SortMergeJoinSpec] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 208.746885 ms
15:23:18.640 [ScalaTest-run-running-SortMergeJoinSpec] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 355.4 KiB, free 4.1 GiB)
15:23:18.641 [ScalaTest-run-running-SortMergeJoinSpec] DEBUG org.apache.spark.storage.BlockManager - Put block broadcast_0 locally took 45 ms
15:23:18.642 [ScalaTest-run-running-SortMergeJoinSpec] DEBUG org.apache.spark.storage.BlockManager - Putting block broadcast_0 without replication took 46 ms
15:23:18.970 [ScalaTest-run-running-SortMergeJoinSpec] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.4 KiB, free 4.1 GiB)
15:23:18.971 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_0_piece0 for BlockManagerId(driver, 192.168.31.150, 55538, None)
15:23:18.973 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on 192.168.31.150:55538 (size: 34.4 KiB, free: 4.1 GiB)
15:23:18.974 [ScalaTest-run-running-SortMergeJoinSpec] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_0_piece0
15:23:18.975 [ScalaTest-run-running-SortMergeJoinSpec] DEBUG org.apache.spark.storage.BlockManager - Told master about block broadcast_0_piece0
15:23:18.975 [ScalaTest-run-running-SortMergeJoinSpec] DEBUG org.apache.spark.storage.BlockManager - Put block broadcast_0_piece0 locally took 6 ms
15:23:18.975 [ScalaTest-run-running-SortMergeJoinSpec] DEBUG org.apache.spark.storage.BlockManager - Putting block broadcast_0_piece0 without replication took 6 ms
15:23:18.976 [ScalaTest-run-running-SortMergeJoinSpec] INFO  org.apache.spark.SparkContext - Created broadcast 0 from show at SortMergeJoinSpec.scala:132
15:23:19.011 [ScalaTest-run-running-SortMergeJoinSpec] DEBUG org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: int
org.apache.spark.sql.catalyst.parser.SqlBaseParser$SingleDataTypeContext =>  int <EOF>
org.apache.spark.sql.catalyst.parser.SqlBaseParser$PrimitiveDataTypeContext =>  int
15:23:19.011 [ScalaTest-run-running-SortMergeJoinSpec] DEBUG org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
org.apache.spark.sql.catalyst.parser.SqlBaseParser$SingleDataTypeContext =>  string <EOF>
org.apache.spark.sql.catalyst.parser.SqlBaseParser$PrimitiveDataTypeContext =>  string
15:23:19.011 [ScalaTest-run-running-SortMergeJoinSpec] DEBUG org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
org.apache.spark.sql.catalyst.parser.SqlBaseParser$SingleDataTypeContext =>  string <EOF>
org.apache.spark.sql.catalyst.parser.SqlBaseParser$PrimitiveDataTypeContext =>  string
15:23:19.119 [ScalaTest-run-running-SortMergeJoinSpec] INFO  org.apache.spark.sql.execution.datasources.InMemoryFileIndex - It took 20 ms to list leaf files for 5 paths.
15:23:19.128 [ScalaTest-run-running-SortMergeJoinSpec] INFO  org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 20975025 bytes, open cost is considered as scanning 4194304 bytes.
15:23:19.173 [ScalaTest-run-running-SortMergeJoinSpec] DEBUG org.apache.spark.util.ClosureCleaner - Cleaning indylambda closure: $anonfun$doExecute$4$adapted
15:23:19.185 [ScalaTest-run-running-SortMergeJoinSpec] DEBUG org.apache.spark.util.ClosureCleaner -  +++ indylambda closure ($anonfun$doExecute$4$adapted) is now cleaned +++
15:23:19.243 [ScalaTest-run-running-SortMergeJoinSpec] DEBUG org.apache.spark.scheduler.DAGScheduler - eagerlyComputePartitionsForRddAndAncestors for RDD 3 took 0.001039 seconds
15:23:19.248 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - Merging stage rdd profiles: Set()
15:23:19.249 [ScalaTest-run-running-SortMergeJoinSpec] DEBUG org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec - Materialize query stage ShuffleQueryStageExec: 1, query stage plan:
Exchange hashpartitioning(id#3, 200), ENSURE_REQUIREMENTS, [plan_id=79]
+- *(2) Project [id#3, value#4]
   +- *(2) Filter ((isnotnull(value#4) AND (value#4 > 1)) AND isnotnull(id#3))
      +- *(2) ColumnarToRow
         +- FileScan parquet default.t2[id#3,value#4,date#5] Batched: true, DataFilters: [isnotnull(value#4), (value#4 > 1), isnotnull(id#3)], Format: Parquet, Location: CatalogFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-wareh..., PartitionFilters: [], PushedFilters: [IsNotNull(value), GreaterThan(value,1), IsNotNull(id)], ReadSchema: struct<id:int,value:int>

15:23:19.260 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Registering RDD 3 (show at SortMergeJoinSpec.scala:132) as input to shuffle 0
15:23:19.262 [ScalaTest-run-running-SortMergeJoinSpec] DEBUG org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator -
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage2(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=2
/* 006 */ final class GeneratedIteratorForCodegenStage2 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private int columnartorow_batchIdx_0;
/* 010 */   private org.apache.spark.sql.execution.vectorized.OnHeapColumnVector[] columnartorow_mutableStateArray_2 = new org.apache.spark.sql.execution.vectorized.OnHeapColumnVector[3];
/* 011 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] columnartorow_mutableStateArray_3 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[3];
/* 012 */   private org.apache.spark.sql.vectorized.ColumnarBatch[] columnartorow_mutableStateArray_1 = new org.apache.spark.sql.vectorized.ColumnarBatch[1];
/* 013 */   private scala.collection.Iterator[] columnartorow_mutableStateArray_0 = new scala.collection.Iterator[1];
/* 014 */
/* 015 */   public GeneratedIteratorForCodegenStage2(Object[] references) {
/* 016 */     this.references = references;
/* 017 */   }
/* 018 */
/* 019 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 020 */     partitionIndex = index;
/* 021 */     this.inputs = inputs;
/* 022 */     columnartorow_mutableStateArray_0[0] = inputs[0];
/* 023 */
/* 024 */     columnartorow_mutableStateArray_3[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(3, 32);
/* 025 */     columnartorow_mutableStateArray_3[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(3, 32);
/* 026 */     columnartorow_mutableStateArray_3[2] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 0);
/* 027 */
/* 028 */   }
/* 029 */
/* 030 */   private void columnartorow_nextBatch_0() throws java.io.IOException {
/* 031 */     if (columnartorow_mutableStateArray_0[0].hasNext()) {
/* 032 */       columnartorow_mutableStateArray_1[0] = (org.apache.spark.sql.vectorized.ColumnarBatch)columnartorow_mutableStateArray_0[0].next();
/* 033 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* numInputBatches */).add(1);
/* 034 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(columnartorow_mutableStateArray_1[0].numRows());
/* 035 */       columnartorow_batchIdx_0 = 0;
/* 036 */       columnartorow_mutableStateArray_2[0] = (org.apache.spark.sql.execution.vectorized.OnHeapColumnVector) columnartorow_mutableStateArray_1[0].column(0);
/* 037 */       columnartorow_mutableStateArray_2[1] = (org.apache.spark.sql.execution.vectorized.OnHeapColumnVector) columnartorow_mutableStateArray_1[0].column(1);
/* 038 */       columnartorow_mutableStateArray_2[2] = (org.apache.spark.sql.execution.vectorized.OnHeapColumnVector) columnartorow_mutableStateArray_1[0].column(2);
/* 039 */
/* 040 */     }
/* 041 */   }
/* 042 */
/* 043 */   protected void processNext() throws java.io.IOException {
/* 044 */     if (columnartorow_mutableStateArray_1[0] == null) {
/* 045 */       columnartorow_nextBatch_0();
/* 046 */     }
/* 047 */     while ( columnartorow_mutableStateArray_1[0] != null) {
/* 048 */       int columnartorow_numRows_0 = columnartorow_mutableStateArray_1[0].numRows();
/* 049 */       int columnartorow_localEnd_0 = columnartorow_numRows_0 - columnartorow_batchIdx_0;
/* 050 */       for (int columnartorow_localIdx_0 = 0; columnartorow_localIdx_0 < columnartorow_localEnd_0; columnartorow_localIdx_0++) {
/* 051 */         int columnartorow_rowIdx_0 = columnartorow_batchIdx_0 + columnartorow_localIdx_0;
/* 052 */         do {
/* 053 */           boolean columnartorow_isNull_1 = columnartorow_mutableStateArray_2[1].isNullAt(columnartorow_rowIdx_0);
/* 054 */           int columnartorow_value_1 = columnartorow_isNull_1 ? -1 : (columnartorow_mutableStateArray_2[1].getInt(columnartorow_rowIdx_0));
/* 055 */
/* 056 */           boolean filter_value_2 = !columnartorow_isNull_1;
/* 057 */           if (!filter_value_2) continue;
/* 058 */
/* 059 */           boolean filter_value_3 = false;
/* 060 */           filter_value_3 = columnartorow_value_1 > 1;
/* 061 */           if (!filter_value_3) continue;
/* 062 */
/* 063 */           boolean columnartorow_isNull_0 = columnartorow_mutableStateArray_2[0].isNullAt(columnartorow_rowIdx_0);
/* 064 */           int columnartorow_value_0 = columnartorow_isNull_0 ? -1 : (columnartorow_mutableStateArray_2[0].getInt(columnartorow_rowIdx_0));
/* 065 */
/* 066 */           boolean filter_value_8 = !columnartorow_isNull_0;
/* 067 */           if (!filter_value_8) continue;
/* 068 */
/* 069 */           ((org.apache.spark.sql.execution.metric.SQLMetric) references[2] /* numOutputRows */).add(1);
/* 070 */
/* 071 */           // common sub-expressions
/* 072 */
/* 073 */           columnartorow_mutableStateArray_3[2].reset();
/* 074 */
/* 075 */           if (false) {
/* 076 */             columnartorow_mutableStateArray_3[2].setNullAt(0);
/* 077 */           } else {
/* 078 */             columnartorow_mutableStateArray_3[2].write(0, columnartorow_value_0);
/* 079 */           }
/* 080 */
/* 081 */           if (false) {
/* 082 */             columnartorow_mutableStateArray_3[2].setNullAt(1);
/* 083 */           } else {
/* 084 */             columnartorow_mutableStateArray_3[2].write(1, columnartorow_value_1);
/* 085 */           }
/* 086 */           append((columnartorow_mutableStateArray_3[2].getRow()));
/* 087 */
/* 088 */         } while(false);
/* 089 */         if (shouldStop()) { columnartorow_batchIdx_0 = columnartorow_rowIdx_0 + 1; return; }
/* 090 */       }
/* 091 */       columnartorow_batchIdx_0 = columnartorow_numRows_0;
/* 092 */       columnartorow_mutableStateArray_1[0] = null;
/* 093 */       columnartorow_nextBatch_0();
/* 094 */     }
/* 095 */   }
/* 096 */
/* 097 */ }

15:23:19.266 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got map stage job 0 (show at SortMergeJoinSpec.scala:132) with 1 output partitions
15:23:19.266 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ShuffleMapStage 0 (show at SortMergeJoinSpec.scala:132)
15:23:19.267 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
15:23:19.268 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
15:23:19.271 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - submitStage(ShuffleMapStage 0 (name=show at SortMergeJoinSpec.scala:132;jobs=0))
15:23:19.272 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - missing: List()
15:23:19.273 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at show at SortMergeJoinSpec.scala:132), which has no missing parents
15:23:19.274 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - submitMissingTasks(ShuffleMapStage 0)
15:23:19.283 [ScalaTest-run-running-SortMergeJoinSpec] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 26.134728 ms
15:23:19.289 [ScalaTest-run-running-SortMergeJoinSpec] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 355.4 KiB, free 4.1 GiB)
15:23:19.289 [ScalaTest-run-running-SortMergeJoinSpec] DEBUG org.apache.spark.storage.BlockManager - Put block broadcast_1 locally took 4 ms
15:23:19.289 [ScalaTest-run-running-SortMergeJoinSpec] DEBUG org.apache.spark.storage.BlockManager - Putting block broadcast_1 without replication took 4 ms
15:23:19.307 [ScalaTest-run-running-SortMergeJoinSpec] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 34.4 KiB, free 4.1 GiB)
15:23:19.308 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_1_piece0 for BlockManagerId(driver, 192.168.31.150, 55538, None)
15:23:19.308 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on 192.168.31.150:55538 (size: 34.4 KiB, free: 4.1 GiB)
15:23:19.308 [ScalaTest-run-running-SortMergeJoinSpec] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_1_piece0
15:23:19.308 [ScalaTest-run-running-SortMergeJoinSpec] DEBUG org.apache.spark.storage.BlockManager - Told master about block broadcast_1_piece0
15:23:19.308 [ScalaTest-run-running-SortMergeJoinSpec] DEBUG org.apache.spark.storage.BlockManager - Put block broadcast_1_piece0 locally took 1 ms
15:23:19.308 [ScalaTest-run-running-SortMergeJoinSpec] DEBUG org.apache.spark.storage.BlockManager - Putting block broadcast_1_piece0 without replication took 1 ms
15:23:19.309 [ScalaTest-run-running-SortMergeJoinSpec] INFO  org.apache.spark.SparkContext - Created broadcast 1 from show at SortMergeJoinSpec.scala:132
15:23:19.332 [ScalaTest-run-running-SortMergeJoinSpec] DEBUG org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: int
org.apache.spark.sql.catalyst.parser.SqlBaseParser$SingleDataTypeContext =>  int <EOF>
org.apache.spark.sql.catalyst.parser.SqlBaseParser$PrimitiveDataTypeContext =>  int
15:23:19.333 [ScalaTest-run-running-SortMergeJoinSpec] DEBUG org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: int
org.apache.spark.sql.catalyst.parser.SqlBaseParser$SingleDataTypeContext =>  int <EOF>
org.apache.spark.sql.catalyst.parser.SqlBaseParser$PrimitiveDataTypeContext =>  int
15:23:19.333 [ScalaTest-run-running-SortMergeJoinSpec] DEBUG org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
org.apache.spark.sql.catalyst.parser.SqlBaseParser$SingleDataTypeContext =>  string <EOF>
org.apache.spark.sql.catalyst.parser.SqlBaseParser$PrimitiveDataTypeContext =>  string
15:23:19.377 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 16.9 KiB, free 4.1 GiB)
15:23:19.377 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager - Put block broadcast_2 locally took 0 ms
15:23:19.377 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager - Putting block broadcast_2 without replication took 0 ms
15:23:19.379 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 4.1 GiB)
15:23:19.380 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_2_piece0 for BlockManagerId(driver, 192.168.31.150, 55538, None)
15:23:19.380 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on 192.168.31.150:55538 (size: 7.7 KiB, free: 4.1 GiB)
15:23:19.380 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_2_piece0
15:23:19.380 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager - Told master about block broadcast_2_piece0
15:23:19.380 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager - Put block broadcast_2_piece0 locally took 1 ms
15:23:19.380 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager - Putting block broadcast_2_piece0 without replication took 1 ms
15:23:19.381 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1513
15:23:19.387 [ScalaTest-run-running-SortMergeJoinSpec] INFO  org.apache.spark.sql.execution.datasources.InMemoryFileIndex - It took 5 ms to list leaf files for 5 paths.
15:23:19.388 [ScalaTest-run-running-SortMergeJoinSpec] INFO  org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 20974984 bytes, open cost is considered as scanning 4194304 bytes.
15:23:19.390 [ScalaTest-run-running-SortMergeJoinSpec] DEBUG org.apache.spark.util.ClosureCleaner - Cleaning indylambda closure: $anonfun$doExecute$4$adapted
15:23:19.392 [ScalaTest-run-running-SortMergeJoinSpec] DEBUG org.apache.spark.util.ClosureCleaner -  +++ indylambda closure ($anonfun$doExecute$4$adapted) is now cleaned +++
15:23:19.396 [ScalaTest-run-running-SortMergeJoinSpec] DEBUG org.apache.spark.scheduler.DAGScheduler - eagerlyComputePartitionsForRddAndAncestors for RDD 7 took 0.000050 seconds
15:23:19.397 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at show at SortMergeJoinSpec.scala:132) (first 15 tasks are for partitions Vector(0))
15:23:19.398 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks resource profile 0
15:23:19.418 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.TaskSetManager - Epoch for TaskSet 0.0: 0
15:23:19.420 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.TaskSetManager - Adding pending tasks took 1 ms
15:23:19.421 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.TaskSetManager - Valid locality levels for TaskSet 0.0: NO_PREF, ANY
15:23:19.423 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - Merging stage rdd profiles: Set()
15:23:19.424 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Registering RDD 7 (show at SortMergeJoinSpec.scala:132) as input to shuffle 1
15:23:19.424 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got map stage job 1 (show at SortMergeJoinSpec.scala:132) with 1 output partitions
15:23:19.424 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ShuffleMapStage 1 (show at SortMergeJoinSpec.scala:132)
15:23:19.424 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
15:23:19.424 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
15:23:19.425 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - submitStage(ShuffleMapStage 1 (name=show at SortMergeJoinSpec.scala:132;jobs=1))
15:23:19.425 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - missing: List()
15:23:19.425 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 1 (MapPartitionsRDD[7] at show at SortMergeJoinSpec.scala:132), which has no missing parents
15:23:19.425 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - submitMissingTasks(ShuffleMapStage 1)
15:23:19.433 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_3 stored as values in memory (estimated size 17.1 KiB, free 4.1 GiB)
15:23:19.434 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager - Put block broadcast_3 locally took 1 ms
15:23:19.434 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager - Putting block broadcast_3 without replication took 1 ms
15:23:19.435 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 4.1 GiB)
15:23:19.436 [dispatcher-event-loop-0] DEBUG org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0.0, runningTasks: 0
15:23:19.436 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_3_piece0 for BlockManagerId(driver, 192.168.31.150, 55538, None)
15:23:19.436 [dispatcher-event-loop-0] DEBUG org.apache.spark.scheduler.TaskSetManager - Valid locality levels for TaskSet 0.0: NO_PREF, ANY
15:23:19.436 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_3_piece0 in memory on 192.168.31.150:55538 (size: 7.7 KiB, free: 4.1 GiB)
15:23:19.436 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_3_piece0
15:23:19.436 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager - Told master about block broadcast_3_piece0
15:23:19.436 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager - Put block broadcast_3_piece0 locally took 1 ms
15:23:19.436 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager - Putting block broadcast_3_piece0 without replication took 1 ms
15:23:19.437 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 3 from broadcast at DAGScheduler.scala:1513
15:23:19.437 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[7] at show at SortMergeJoinSpec.scala:132) (first 15 tasks are for partitions Vector(0))
15:23:19.437 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks resource profile 0
15:23:19.449 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0) (192.168.31.150, executor driver, partition 0, PROCESS_LOCAL, 6074 bytes) taskResourceAssignments Map()
15:23:19.453 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.TaskSetManager - Epoch for TaskSet 1.0: 0
15:23:19.453 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.TaskSetManager - Adding pending tasks took 0 ms
15:23:19.453 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.TaskSetManager - Valid locality levels for TaskSet 1.0: NO_PREF, ANY
15:23:19.457 [dispatcher-event-loop-0] DEBUG org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0.0, runningTasks: 1
15:23:19.457 [dispatcher-event-loop-0] DEBUG org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_1.0, runningTasks: 0
15:23:19.462 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
15:23:19.469 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] DEBUG org.apache.spark.executor.ExecutorMetricsPoller - stageTCMP: (0, 0) -> 1
15:23:19.491 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] DEBUG org.apache.spark.storage.BlockManager - Getting local block broadcast_2
15:23:19.493 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] DEBUG org.apache.spark.storage.BlockManager - Level for block broadcast_2 is StorageLevel(disk, memory, deserialized, 1 replicas)
15:23:19.690 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] DEBUG org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeProjection - code for pmod(hash(input[0, int, true], 42), 200):
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = false;
/* 032 */     int value_0 = -1;
/* 033 */     if (200 == 0) {
/* 034 */       isNull_0 = true;
/* 035 */     } else {
/* 036 */       int value_1 = 42;
/* 037 */       boolean isNull_2 = i.isNullAt(0);
/* 038 */       int value_2 = isNull_2 ?
/* 039 */       -1 : (i.getInt(0));
/* 040 */       if (!isNull_2) {
/* 041 */         value_1 = org.apache.spark.unsafe.hash.Murmur3_x86_32.hashInt(value_2, value_1);
/* 042 */       }
/* 043 */
/* 044 */       int remainder_0 = value_1 % 200;
/* 045 */       if (remainder_0 < 0) {
/* 046 */         value_0=(remainder_0 + 200) % 200;
/* 047 */       } else {
/* 048 */         value_0=remainder_0;
/* 049 */       }
/* 050 */
/* 051 */     }
/* 052 */     if (isNull_0) {
/* 053 */       mutableStateArray_0[0].setNullAt(0);
/* 054 */     } else {
/* 055 */       mutableStateArray_0[0].write(0, value_0);
/* 056 */     }
/* 057 */     return (mutableStateArray_0[0].getRow());
/* 058 */   }
/* 059 */
/* 060 */
/* 061 */ }

15:23:19.691 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] DEBUG org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator -
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = false;
/* 032 */     int value_0 = -1;
/* 033 */     if (200 == 0) {
/* 034 */       isNull_0 = true;
/* 035 */     } else {
/* 036 */       int value_1 = 42;
/* 037 */       boolean isNull_2 = i.isNullAt(0);
/* 038 */       int value_2 = isNull_2 ?
/* 039 */       -1 : (i.getInt(0));
/* 040 */       if (!isNull_2) {
/* 041 */         value_1 = org.apache.spark.unsafe.hash.Murmur3_x86_32.hashInt(value_2, value_1);
/* 042 */       }
/* 043 */
/* 044 */       int remainder_0 = value_1 % 200;
/* 045 */       if (remainder_0 < 0) {
/* 046 */         value_0=(remainder_0 + 200) % 200;
/* 047 */       } else {
/* 048 */         value_0=remainder_0;
/* 049 */       }
/* 050 */
/* 051 */     }
/* 052 */     if (isNull_0) {
/* 053 */       mutableStateArray_0[0].setNullAt(0);
/* 054 */     } else {
/* 055 */       mutableStateArray_0[0].write(0, value_0);
/* 056 */     }
/* 057 */     return (mutableStateArray_0[0].getRow());
/* 058 */   }
/* 059 */
/* 060 */
/* 061 */ }

15:23:19.707 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 17.419027 ms
15:23:19.716 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-warehouse/t1/date=2023-01-00/part-00000-8b528cad-38e6-470c-85a2-61fc0a5f4d84.c000.snappy.parquet, range: 0-702, partition values: [2023-01-00]
15:23:19.717 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] DEBUG org.apache.spark.storage.BlockManager - Getting local block broadcast_0
15:23:19.717 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] DEBUG org.apache.spark.storage.BlockManager - Level for block broadcast_0 is StorageLevel(disk, memory, deserialized, 1 replicas)
15:23:20.073 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] DEBUG org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport - Going to read the following fields from the Parquet file with the following schema:
Parquet file schema:
message spark_schema {
  required int32 id;
  required binary name (STRING);
}

Parquet clipped schema:
message spark_schema {
  required int32 id;
  required binary name (STRING);
}

Parquet requested schema:
message spark_schema {
  required int32 id;
  required binary name (STRING);
}

Catalyst requested schema:
root
-- id: integer (nullable = true)
-- name: string (nullable = true)


15:23:20.130 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] DEBUG org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat - Appending StructType(StructField(date,StringType,true)) [2023-01-00]
15:23:20.618 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-warehouse/t1/date=2023-01-02/part-00000-8b528cad-38e6-470c-85a2-61fc0a5f4d84.c000.snappy.parquet, range: 0-702, partition values: [2023-01-02]
15:23:20.624 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] DEBUG org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport - Going to read the following fields from the Parquet file with the following schema:
Parquet file schema:
message spark_schema {
  required int32 id;
  required binary name (STRING);
}

Parquet clipped schema:
message spark_schema {
  required int32 id;
  required binary name (STRING);
}

Parquet requested schema:
message spark_schema {
  required int32 id;
  required binary name (STRING);
}

Catalyst requested schema:
root
-- id: integer (nullable = true)
-- name: string (nullable = true)


15:23:20.624 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] DEBUG org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat - Appending StructType(StructField(date,StringType,true)) [2023-01-02]
15:23:20.627 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-warehouse/t1/date=2023-01-03/part-00000-8b528cad-38e6-470c-85a2-61fc0a5f4d84.c000.snappy.parquet, range: 0-701, partition values: [2023-01-03]
15:23:20.630 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] DEBUG org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport - Going to read the following fields from the Parquet file with the following schema:
Parquet file schema:
message spark_schema {
  required int32 id;
  required binary name (STRING);
}

Parquet clipped schema:
message spark_schema {
  required int32 id;
  required binary name (STRING);
}

Parquet requested schema:
message spark_schema {
  required int32 id;
  required binary name (STRING);
}

Catalyst requested schema:
root
-- id: integer (nullable = true)
-- name: string (nullable = true)


15:23:20.630 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] DEBUG org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat - Appending StructType(StructField(date,StringType,true)) [2023-01-03]
15:23:20.632 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-warehouse/t1/date=2023-01-04/part-00000-8b528cad-38e6-470c-85a2-61fc0a5f4d84.c000.snappy.parquet, range: 0-701, partition values: [2023-01-04]
15:23:20.636 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] DEBUG org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport - Going to read the following fields from the Parquet file with the following schema:
Parquet file schema:
message spark_schema {
  required int32 id;
  required binary name (STRING);
}

Parquet clipped schema:
message spark_schema {
  required int32 id;
  required binary name (STRING);
}

Parquet requested schema:
message spark_schema {
  required int32 id;
  required binary name (STRING);
}

Catalyst requested schema:
root
-- id: integer (nullable = true)
-- name: string (nullable = true)


15:23:20.636 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] DEBUG org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat - Appending StructType(StructField(date,StringType,true)) [2023-01-04]
15:23:20.637 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-warehouse/t1/date=2023-01-01/part-00000-8b528cad-38e6-470c-85a2-61fc0a5f4d84.c000.snappy.parquet, range: 0-699, partition values: [2023-01-01]
15:23:20.641 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] DEBUG org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport - Going to read the following fields from the Parquet file with the following schema:
Parquet file schema:
message spark_schema {
  required int32 id;
  required binary name (STRING);
}

Parquet clipped schema:
message spark_schema {
  required int32 id;
  required binary name (STRING);
}

Parquet requested schema:
message spark_schema {
  required int32 id;
  required binary name (STRING);
}

Catalyst requested schema:
root
-- id: integer (nullable = true)
-- name: string (nullable = true)


15:23:20.641 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] DEBUG org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat - Appending StructType(StructField(date,StringType,true)) [2023-01-01]
15:23:20.662 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] DEBUG org.apache.spark.shuffle.sort.io.LocalDiskShuffleMapOutputWriter - Writing shuffle index file for mapId 0 with length 200
15:23:20.667 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] DEBUG org.apache.spark.shuffle.IndexShuffleBlockResolver - Shuffle index for mapId 0: [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,72,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,72,0,0,0,0,72,89,0,72,0,72,0,0,0,0,0,0,0,0,0,0,0,0,67,0,0,72,0,0,0,0,0,0,0,72,0,0,0,0,0,0,0,0,0,0,0,90,0,0,0,0,0,0,0,0,0,0,0,0,72,72,0,0,0,72,0,0,0,0,0,0,0,0,0,0,0,0,0,0,72,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,72,0,0,0,0,72,0,0,0,0,0,72,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,72,0,0,0,0,0,0,0]
15:23:20.680 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 2294 bytes result sent to driver
15:23:20.681 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] DEBUG org.apache.spark.executor.ExecutorMetricsPoller - stageTCMP: (0, 0) -> 0
15:23:20.682 [dispatcher-event-loop-0] DEBUG org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0.0, runningTasks: 0
15:23:20.682 [dispatcher-event-loop-0] DEBUG org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_1.0, runningTasks: 0
15:23:20.682 [dispatcher-event-loop-0] DEBUG org.apache.spark.scheduler.TaskSetManager - No tasks for locality level NO_PREF, so moving to locality level ANY
15:23:20.684 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 1) (192.168.31.150, executor driver, partition 0, PROCESS_LOCAL, 6074 bytes) taskResourceAssignments Map()
15:23:20.684 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 1)
15:23:20.685 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] DEBUG org.apache.spark.executor.ExecutorMetricsPoller - stageTCMP: (1, 0) -> 1
15:23:20.686 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] DEBUG org.apache.spark.storage.BlockManager - Getting local block broadcast_3
15:23:20.686 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] DEBUG org.apache.spark.storage.BlockManager - Level for block broadcast_3 is StorageLevel(disk, memory, deserialized, 1 replicas)
15:23:20.687 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 1246 ms on 192.168.31.150 (executor driver) (1/1)
15:23:20.688 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool
15:23:20.693 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] DEBUG org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeProjection - code for pmod(hash(input[0, int, true], 42), 200):
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = false;
/* 032 */     int value_0 = -1;
/* 033 */     if (200 == 0) {
/* 034 */       isNull_0 = true;
/* 035 */     } else {
/* 036 */       int value_1 = 42;
/* 037 */       boolean isNull_2 = i.isNullAt(0);
/* 038 */       int value_2 = isNull_2 ?
/* 039 */       -1 : (i.getInt(0));
/* 040 */       if (!isNull_2) {
/* 041 */         value_1 = org.apache.spark.unsafe.hash.Murmur3_x86_32.hashInt(value_2, value_1);
/* 042 */       }
/* 043 */
/* 044 */       int remainder_0 = value_1 % 200;
/* 045 */       if (remainder_0 < 0) {
/* 046 */         value_0=(remainder_0 + 200) % 200;
/* 047 */       } else {
/* 048 */         value_0=remainder_0;
/* 049 */       }
/* 050 */
/* 051 */     }
/* 052 */     if (isNull_0) {
/* 053 */       mutableStateArray_0[0].setNullAt(0);
/* 054 */     } else {
/* 055 */       mutableStateArray_0[0].write(0, value_0);
/* 056 */     }
/* 057 */     return (mutableStateArray_0[0].getRow());
/* 058 */   }
/* 059 */
/* 060 */
/* 061 */ }

15:23:20.693 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-warehouse/t2/date=2023-01-00/part-00000-55b8f9b8-4fba-4159-9494-537c1102dfa1.c000.snappy.parquet, range: 0-693, partition values: [2023-01-00]
15:23:20.694 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] DEBUG org.apache.spark.storage.BlockManager - Getting local block broadcast_1
15:23:20.694 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] DEBUG org.apache.spark.storage.BlockManager - Level for block broadcast_1 is StorageLevel(disk, memory, deserialized, 1 replicas)
15:23:20.694 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - ShuffleMapTask finished on driver
15:23:20.695 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 0 (show at SortMergeJoinSpec.scala:132) finished in 1.406 s
15:23:20.695 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
15:23:20.695 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - running: Set(ShuffleMapStage 1)
15:23:20.696 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - waiting: Set()
15:23:20.696 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - failed: Set()
15:23:20.696 [dag-scheduler-event-loop] DEBUG org.apache.spark.MapOutputTrackerMaster - Increasing epoch to 1
15:23:20.701 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - After removal of stage 0, remaining stages = 1
15:23:20.704 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] DEBUG org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport - Going to read the following fields from the Parquet file with the following schema:
Parquet file schema:
message spark_schema {
  required int32 id;
  required int32 value;
}

Parquet clipped schema:
message spark_schema {
  required int32 id;
  required int32 value;
}

Parquet requested schema:
message spark_schema {
  required int32 id;
  required int32 value;
}

Catalyst requested schema:
root
-- id: integer (nullable = true)
-- value: integer (nullable = true)


15:23:20.705 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] DEBUG org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat - Appending StructType(StructField(date,StringType,true)) [2023-01-00]
15:23:20.710 [ScalaTest-run-running-SortMergeJoinSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - Considering join on: Some((id#0 = id#3))
15:23:20.710 [ScalaTest-run-running-SortMergeJoinSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - leftKeys:List(id#0) | rightKeys:List(id#3)
15:23:20.713 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-warehouse/t2/date=2023-01-01/part-00000-55b8f9b8-4fba-4159-9494-537c1102dfa1.c000.snappy.parquet, range: 0-693, partition values: [2023-01-01]
org.apache.spark.sql.execution.adaptive.AQEOptimizer@6e53bb4f org.apache.spark.sql.execution.adaptive.DynamicJoinSelection --> old【
GlobalLimit 21
+- LocalLimit 21
   +- Project [cast(id#0 as string) AS id#14, name#1, cast(value#4 as string) AS value#16]
      +- Join Inner, (id#0 = id#3)
         :- LogicalQueryStage Project [id#0, name#1], ShuffleQueryStage 0
         +- LogicalQueryStage Project [id#3, value#4], ShuffleQueryStage 1
】--> new【
GlobalLimit 21
+- LocalLimit 21
   +- Project [cast(id#0 as string) AS id#14, name#1, cast(value#4 as string) AS value#16]
      +- Join Inner, (id#0 = id#3), leftHint=(strategy=no_broadcast_hash)
         :- LogicalQueryStage Project [id#0, name#1], ShuffleQueryStage 0
         +- LogicalQueryStage Project [id#3, value#4], ShuffleQueryStage 1
】


15:23:20.718 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] DEBUG org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport - Going to read the following fields from the Parquet file with the following schema:
Parquet file schema:
message spark_schema {
  required int32 id;
  required int32 value;
}

Parquet clipped schema:
message spark_schema {
  required int32 id;
  required int32 value;
}

Parquet requested schema:
message spark_schema {
  required int32 id;
  required int32 value;
}

Catalyst requested schema:
root
-- id: integer (nullable = true)
-- value: integer (nullable = true)


15:23:20.719 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] DEBUG org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat - Appending StructType(StructField(date,StringType,true)) [2023-01-01]
15:23:20.720 [ScalaTest-run-running-SortMergeJoinSpec] DEBUG org.apache.spark.sql.execution.adaptive.LogicalQueryStage - Physical stats available as Statistics(sizeInBytes=640.0 B, rowCount=20) for plan: ShuffleQueryStage 0
+- Exchange hashpartitioning(id#0, 200), ENSURE_REQUIREMENTS, [plan_id=51]
   +- *(1) Project [id#0, name#1]
      +- *(1) Filter isnotnull(id#0)
         +- *(1) ColumnarToRow
            +- FileScan parquet default.t1[id#0,name#1,date#2] Batched: true, DataFilters: [isnotnull(id#0)], Format: Parquet, Location: CatalogFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-wareh..., PartitionFilters: [], PushedFilters: [IsNotNull(id)], ReadSchema: struct<id:int,name:string>

15:23:20.720 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-warehouse/t2/date=2023-01-02/part-00000-55b8f9b8-4fba-4159-9494-537c1102dfa1.c000.snappy.parquet, range: 0-693, partition values: [2023-01-02]
15:23:20.721 [ScalaTest-run-running-SortMergeJoinSpec] DEBUG org.apache.spark.sql.execution.adaptive.LogicalQueryStage - Physical stats not available for plan: ShuffleQueryStage 1
+- Exchange hashpartitioning(id#3, 200), ENSURE_REQUIREMENTS, [plan_id=79]
   +- *(2) Project [id#3, value#4]
      +- *(2) Filter ((isnotnull(value#4) AND (value#4 > 1)) AND isnotnull(id#3))
         +- *(2) ColumnarToRow
            +- FileScan parquet default.t2[id#3,value#4,date#5] Batched: true, DataFilters: [isnotnull(value#4), (value#4 > 1), isnotnull(id#3)], Format: Parquet, Location: CatalogFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-wareh..., PartitionFilters: [], PushedFilters: [IsNotNull(value), GreaterThan(value,1), IsNotNull(id)], ReadSchema: struct<id:int,value:int>

RuleExecutor.execute[org.apache.spark.sql.execution.adaptive.AQEOptimizer@6e53bb4f] final result ==> old【
GlobalLimit 21
+- LocalLimit 21
   +- Project [cast(id#0 as string) AS id#14, name#1, cast(value#4 as string) AS value#16]
      +- Join Inner, (id#0 = id#3)
         :- LogicalQueryStage Project [id#0, name#1], ShuffleQueryStage 0
         +- LogicalQueryStage Project [id#3, value#4], ShuffleQueryStage 1
】==> new【
GlobalLimit 21
+- LocalLimit 21
   +- Project [cast(id#0 as string) AS id#14, name#1, cast(value#4 as string) AS value#16]
      +- Join Inner, (id#0 = id#3), leftHint=(strategy=no_broadcast_hash)
         :- LogicalQueryStage Project [id#0, name#1], ShuffleQueryStage 0
         +- LogicalQueryStage Project [id#3, value#4], ShuffleQueryStage 1
】


SparkPlanner(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2@a2fb8a5) - org.apache.spark.sql.execution.SparkStrategies$SpecialLimits$ -->【
ReturnAnswer
+- GlobalLimit 21
   +- LocalLimit 21
      +- Project [cast(id#0 as string) AS id#14, name#1, cast(value#4 as string) AS value#16]
         +- Join Inner, (id#0 = id#3), leftHint=(strategy=no_broadcast_hash)
            :- LogicalQueryStage Project [id#0, name#1], ShuffleQueryStage 0
            +- LogicalQueryStage Project [id#3, value#4], ShuffleQueryStage 1
】 --> 【
CollectLimit 21
+- PlanLater Project [cast(id#0 as string) AS id#14, name#1, cast(value#4 as string) AS value#16]
】

SparkPlanner(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2@a2fb8a5) - org.apache.spark.sql.execution.SparkStrategies$BasicOperators$ -->【
Project [cast(id#0 as string) AS id#14, name#1, cast(value#4 as string) AS value#16]
+- Join Inner, (id#0 = id#3), leftHint=(strategy=no_broadcast_hash)
   :- LogicalQueryStage Project [id#0, name#1], ShuffleQueryStage 0
   +- LogicalQueryStage Project [id#3, value#4], ShuffleQueryStage 1
】 --> 【
Project [cast(id#0 as string) AS id#14, name#1, cast(value#4 as string) AS value#16]
+- PlanLater Join Inner, (id#0 = id#3), leftHint=(strategy=no_broadcast_hash)
】

15:23:20.724 [ScalaTest-run-running-SortMergeJoinSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - Considering join on: Some((id#0 = id#3))
15:23:20.724 [ScalaTest-run-running-SortMergeJoinSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - leftKeys:List(id#0) | rightKeys:List(id#3)
15:23:20.724 [ScalaTest-run-running-SortMergeJoinSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - Considering join on: Some((id#0 = id#3))
15:23:20.724 [ScalaTest-run-running-SortMergeJoinSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - leftKeys:List(id#0) | rightKeys:List(id#3)
15:23:20.724 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] DEBUG org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport - Going to read the following fields from the Parquet file with the following schema:
Parquet file schema:
message spark_schema {
  required int32 id;
  required int32 value;
}

Parquet clipped schema:
message spark_schema {
  required int32 id;
  required int32 value;
}

Parquet requested schema:
message spark_schema {
  required int32 id;
  required int32 value;
}

Catalyst requested schema:
root
-- id: integer (nullable = true)
-- value: integer (nullable = true)


15:23:20.725 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] DEBUG org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat - Appending StructType(StructField(date,StringType,true)) [2023-01-02]
15:23:20.727 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-warehouse/t2/date=2023-01-03/part-00000-55b8f9b8-4fba-4159-9494-537c1102dfa1.c000.snappy.parquet, range: 0-693, partition values: [2023-01-03]
SparkPlanner(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2@a2fb8a5) - org.apache.spark.sql.execution.SparkStrategies$JoinSelection$ -->【
Join Inner, (id#0 = id#3), leftHint=(strategy=no_broadcast_hash)
:- LogicalQueryStage Project [id#0, name#1], ShuffleQueryStage 0
+- LogicalQueryStage Project [id#3, value#4], ShuffleQueryStage 1
】 --> 【
SortMergeJoin [id#0], [id#3], Inner
:- PlanLater LogicalQueryStage Project [id#0, name#1], ShuffleQueryStage 0
+- PlanLater LogicalQueryStage Project [id#3, value#4], ShuffleQueryStage 1
】

SparkPlanner(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2@a2fb8a5) - org.apache.spark.sql.execution.adaptive.LogicalQueryStageStrategy$ -->【
LogicalQueryStage Project [id#0, name#1], ShuffleQueryStage 0
】 --> 【
ShuffleQueryStage 0
+- Exchange hashpartitioning(id#0, 200), ENSURE_REQUIREMENTS, [plan_id=51]
   +- *(1) Project [id#0, name#1]
      +- *(1) Filter isnotnull(id#0)
         +- *(1) ColumnarToRow
            +- FileScan parquet default.t1[id#0,name#1,date#2] Batched: true, DataFilters: [isnotnull(id#0)], Format: Parquet, Location: CatalogFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-wareh..., PartitionFilters: [], PushedFilters: [IsNotNull(id)], ReadSchema: struct<id:int,name:string>
】

SparkPlanner(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2@a2fb8a5) - org.apache.spark.sql.execution.adaptive.LogicalQueryStageStrategy$ -->【
LogicalQueryStage Project [id#3, value#4], ShuffleQueryStage 1
】 --> 【
ShuffleQueryStage 1
+- Exchange hashpartitioning(id#3, 200), ENSURE_REQUIREMENTS, [plan_id=79]
   +- *(2) Project [id#3, value#4]
      +- *(2) Filter ((isnotnull(value#4) AND (value#4 > 1)) AND isnotnull(id#3))
         +- *(2) ColumnarToRow
            +- FileScan parquet default.t2[id#3,value#4,date#5] Batched: true, DataFilters: [isnotnull(value#4), (value#4 > 1), isnotnull(id#3)], Format: Parquet, Location: CatalogFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-wareh..., PartitionFilters: [], PushedFilters: [IsNotNull(value), GreaterThan(value,1), IsNotNull(id)], ReadSchema: struct<id:int,value:int>
】

15:23:20.731 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] DEBUG org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport - Going to read the following fields from the Parquet file with the following schema:
Parquet file schema:
message spark_schema {
  required int32 id;
  required int32 value;
}

Parquet clipped schema:
message spark_schema {
  required int32 id;
  required int32 value;
}

Parquet requested schema:
message spark_schema {
  required int32 id;
  required int32 value;
}

Catalyst requested schema:
root
-- id: integer (nullable = true)
-- value: integer (nullable = true)


15:23:20.731 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] DEBUG org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat - Appending StructType(StructField(date,StringType,true)) [2023-01-03]
15:23:20.733 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-warehouse/t2/date=2023-01-04/part-00000-55b8f9b8-4fba-4159-9494-537c1102dfa1.c000.snappy.parquet, range: 0-692, partition values: [2023-01-04]
org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec$ - applyPhysicalRules - org.apache.spark.sql.execution.exchange.EnsureRequirements -->【
CollectLimit 21
+- Project [cast(id#0 as string) AS id#14, name#1, cast(value#4 as string) AS value#16]
   +- SortMergeJoin [id#0], [id#3], Inner
      :- ShuffleQueryStage 0
      :  +- Exchange hashpartitioning(id#0, 200), ENSURE_REQUIREMENTS, [plan_id=51]
      :     +- *(1) Project [id#0, name#1]
      :        +- *(1) Filter isnotnull(id#0)
      :           +- *(1) ColumnarToRow
      :              +- FileScan parquet default.t1[id#0,name#1,date#2] Batched: true, DataFilters: [isnotnull(id#0)], Format: Parquet, Location: CatalogFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-wareh..., PartitionFilters: [], PushedFilters: [IsNotNull(id)], ReadSchema: struct<id:int,name:string>
      +- ShuffleQueryStage 1
         +- Exchange hashpartitioning(id#3, 200), ENSURE_REQUIREMENTS, [plan_id=79]
            +- *(2) Project [id#3, value#4]
               +- *(2) Filter ((isnotnull(value#4) AND (value#4 > 1)) AND isnotnull(id#3))
                  +- *(2) ColumnarToRow
                     +- FileScan parquet default.t2[id#3,value#4,date#5] Batched: true, DataFilters: [isnotnull(value#4), (value#4 > 1), isnotnull(id#3)], Format: Parquet, Location: CatalogFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-wareh..., PartitionFilters: [], PushedFilters: [IsNotNull(value), GreaterThan(value,1), IsNotNull(id)], ReadSchema: struct<id:int,value:int>
】AQE --> 【
CollectLimit 21
+- Project [cast(id#0 as string) AS id#14, name#1, cast(value#4 as string) AS value#16]
   +- SortMergeJoin [id#0], [id#3], Inner
      :- Sort [id#0 ASC NULLS FIRST], false, 0
      :  +- ShuffleQueryStage 0
      :     +- Exchange hashpartitioning(id#0, 200), ENSURE_REQUIREMENTS, [plan_id=51]
      :        +- *(1) Project [id#0, name#1]
      :           +- *(1) Filter isnotnull(id#0)
      :              +- *(1) ColumnarToRow
      :                 +- FileScan parquet default.t1[id#0,name#1,date#2] Batched: true, DataFilters: [isnotnull(id#0)], Format: Parquet, Location: CatalogFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-wareh..., PartitionFilters: [], PushedFilters: [IsNotNull(id)], ReadSchema: struct<id:int,name:string>
      +- Sort [id#3 ASC NULLS FIRST], false, 0
         +- ShuffleQueryStage 1
            +- Exchange hashpartitioning(id#3, 200), ENSURE_REQUIREMENTS, [plan_id=79]
               +- *(2) Project [id#3, value#4]
                  +- *(2) Filter ((isnotnull(value#4) AND (value#4 > 1)) AND isnotnull(id#3))
                     +- *(2) ColumnarToRow
                        +- FileScan parquet default.t2[id#3,value#4,date#5] Batched: true, DataFilters: [isnotnull(value#4), (value#4 > 1), isnotnull(id#3)], Format: Parquet, Location: CatalogFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-wareh..., PartitionFilters: [], PushedFilters: [IsNotNull(value), GreaterThan(value,1), IsNotNull(id)], ReadSchema: struct<id:int,value:int>
】

15:23:20.736 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] DEBUG org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport - Going to read the following fields from the Parquet file with the following schema:
Parquet file schema:
message spark_schema {
  required int32 id;
  required int32 value;
}

Parquet clipped schema:
message spark_schema {
  required int32 id;
  required int32 value;
}

Parquet requested schema:
message spark_schema {
  required int32 id;
  required int32 value;
}

Catalyst requested schema:
root
-- id: integer (nullable = true)
-- value: integer (nullable = true)


15:23:20.737 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] DEBUG org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat - Appending StructType(StructField(date,StringType,true)) [2023-01-04]
org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec$ final result ==>【
CollectLimit 21
+- Project [cast(id#0 as string) AS id#14, name#1, cast(value#4 as string) AS value#16]
   +- SortMergeJoin [id#0], [id#3], Inner
      :- ShuffleQueryStage 0
      :  +- Exchange hashpartitioning(id#0, 200), ENSURE_REQUIREMENTS, [plan_id=51]
      :     +- *(1) Project [id#0, name#1]
      :        +- *(1) Filter isnotnull(id#0)
      :           +- *(1) ColumnarToRow
      :              +- FileScan parquet default.t1[id#0,name#1,date#2] Batched: true, DataFilters: [isnotnull(id#0)], Format: Parquet, Location: CatalogFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-wareh..., PartitionFilters: [], PushedFilters: [IsNotNull(id)], ReadSchema: struct<id:int,name:string>
      +- ShuffleQueryStage 1
         +- Exchange hashpartitioning(id#3, 200), ENSURE_REQUIREMENTS, [plan_id=79]
            +- *(2) Project [id#3, value#4]
               +- *(2) Filter ((isnotnull(value#4) AND (value#4 > 1)) AND isnotnull(id#3))
                  +- *(2) ColumnarToRow
                     +- FileScan parquet default.t2[id#3,value#4,date#5] Batched: true, DataFilters: [isnotnull(value#4), (value#4 > 1), isnotnull(id#3)], Format: Parquet, Location: CatalogFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-wareh..., PartitionFilters: [], PushedFilters: [IsNotNull(value), GreaterThan(value,1), IsNotNull(id)], ReadSchema: struct<id:int,value:int>
】AQE ==> 【
CollectLimit 21
+- Project [cast(id#0 as string) AS id#14, name#1, cast(value#4 as string) AS value#16]
   +- SortMergeJoin [id#0], [id#3], Inner
      :- Sort [id#0 ASC NULLS FIRST], false, 0
      :  +- ShuffleQueryStage 0
      :     +- Exchange hashpartitioning(id#0, 200), ENSURE_REQUIREMENTS, [plan_id=51]
      :        +- *(1) Project [id#0, name#1]
      :           +- *(1) Filter isnotnull(id#0)
      :              +- *(1) ColumnarToRow
      :                 +- FileScan parquet default.t1[id#0,name#1,date#2] Batched: true, DataFilters: [isnotnull(id#0)], Format: Parquet, Location: CatalogFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-wareh..., PartitionFilters: [], PushedFilters: [IsNotNull(id)], ReadSchema: struct<id:int,name:string>
      +- Sort [id#3 ASC NULLS FIRST], false, 0
         +- ShuffleQueryStage 1
            +- Exchange hashpartitioning(id#3, 200), ENSURE_REQUIREMENTS, [plan_id=79]
               +- *(2) Project [id#3, value#4]
                  +- *(2) Filter ((isnotnull(value#4) AND (value#4 > 1)) AND isnotnull(id#3))
                     +- *(2) ColumnarToRow
                        +- FileScan parquet default.t2[id#3,value#4,date#5] Batched: true, DataFilters: [isnotnull(value#4), (value#4 > 1), isnotnull(id#3)], Format: Parquet, Location: CatalogFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-wareh..., PartitionFilters: [], PushedFilters: [IsNotNull(value), GreaterThan(value,1), IsNotNull(id)], ReadSchema: struct<id:int,value:int>
】

15:23:20.747 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] DEBUG org.apache.spark.shuffle.sort.io.LocalDiskShuffleMapOutputWriter - Writing shuffle index file for mapId 1 with length 200
15:23:20.749 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] DEBUG org.apache.spark.shuffle.IndexShuffleBlockResolver - Shuffle index for mapId 1: [0,0,0,0,0,0,0,0,0,0,0,63,0,0,63,0,0,0,0,63,0,0,0,0,63,0,0,0,0,0,63,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,76,0,0,0,76,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,63,63,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,63,0,0,0,0,0,63,0,0,0,0,0,0,62,0,0,0,0,0,0,0,0,0,0,0,0,0,0,63,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,63,0,0,0,0,0,0,0,0,0,0,0,0,63,0,0,0,0,0,0,0,0,0,0,63,0,0,0,0,0,63,0]
15:23:20.750 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 1). 2251 bytes result sent to driver
15:23:20.750 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] DEBUG org.apache.spark.executor.ExecutorMetricsPoller - stageTCMP: (1, 0) -> 0
15:23:20.751 [dispatcher-event-loop-0] DEBUG org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_1.0, runningTasks: 0
15:23:20.751 [dispatcher-event-loop-0] DEBUG org.apache.spark.scheduler.TaskSetManager - No tasks for locality level NO_PREF, so moving to locality level ANY
15:23:20.751 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 1) in 68 ms on 192.168.31.150 (executor driver) (1/1)
15:23:20.751 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool
15:23:20.752 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - ShuffleMapTask finished on driver
15:23:20.752 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 1 (show at SortMergeJoinSpec.scala:132) finished in 1.326 s
15:23:20.752 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
15:23:20.752 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - running: Set()
15:23:20.752 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - waiting: Set()
15:23:20.752 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - failed: Set()
15:23:20.752 [dag-scheduler-event-loop] DEBUG org.apache.spark.MapOutputTrackerMaster - Increasing epoch to 2
15:23:20.753 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - After removal of stage 1, remaining stages = 0
15:23:20.754 [ScalaTest-run-running-SortMergeJoinSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - Considering join on: Some((id#0 = id#3))
15:23:20.754 [ScalaTest-run-running-SortMergeJoinSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - leftKeys:List(id#0) | rightKeys:List(id#3)
org.apache.spark.sql.execution.adaptive.AQEOptimizer@6e53bb4f org.apache.spark.sql.execution.adaptive.DynamicJoinSelection --> old【
GlobalLimit 21
+- LocalLimit 21
   +- Project [cast(id#0 as string) AS id#14, name#1, cast(value#4 as string) AS value#16]
      +- Join Inner, (id#0 = id#3)
         :- LogicalQueryStage Project [id#0, name#1], ShuffleQueryStage 0
         +- LogicalQueryStage Project [id#3, value#4], ShuffleQueryStage 1
】--> new【
GlobalLimit 21
+- LocalLimit 21
   +- Project [cast(id#0 as string) AS id#14, name#1, cast(value#4 as string) AS value#16]
      +- Join Inner, (id#0 = id#3), leftHint=(strategy=no_broadcast_hash), rightHint=(strategy=no_broadcast_hash)
         :- LogicalQueryStage Project [id#0, name#1], ShuffleQueryStage 0
         +- LogicalQueryStage Project [id#3, value#4], ShuffleQueryStage 1
】


15:23:20.756 [ScalaTest-run-running-SortMergeJoinSpec] DEBUG org.apache.spark.sql.execution.adaptive.LogicalQueryStage - Physical stats available as Statistics(sizeInBytes=640.0 B, rowCount=20) for plan: ShuffleQueryStage 0
+- Exchange hashpartitioning(id#0, 200), ENSURE_REQUIREMENTS, [plan_id=51]
   +- *(1) Project [id#0, name#1]
      +- *(1) Filter isnotnull(id#0)
         +- *(1) ColumnarToRow
            +- FileScan parquet default.t1[id#0,name#1,date#2] Batched: true, DataFilters: [isnotnull(id#0)], Format: Parquet, Location: CatalogFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-wareh..., PartitionFilters: [], PushedFilters: [IsNotNull(id)], ReadSchema: struct<id:int,name:string>

15:23:20.756 [ScalaTest-run-running-SortMergeJoinSpec] DEBUG org.apache.spark.sql.execution.adaptive.LogicalQueryStage - Physical stats available as Statistics(sizeInBytes=456.0 B, rowCount=19) for plan: ShuffleQueryStage 1
+- Exchange hashpartitioning(id#3, 200), ENSURE_REQUIREMENTS, [plan_id=79]
   +- *(2) Project [id#3, value#4]
      +- *(2) Filter ((isnotnull(value#4) AND (value#4 > 1)) AND isnotnull(id#3))
         +- *(2) ColumnarToRow
            +- FileScan parquet default.t2[id#3,value#4,date#5] Batched: true, DataFilters: [isnotnull(value#4), (value#4 > 1), isnotnull(id#3)], Format: Parquet, Location: CatalogFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-wareh..., PartitionFilters: [], PushedFilters: [IsNotNull(value), GreaterThan(value,1), IsNotNull(id)], ReadSchema: struct<id:int,value:int>

RuleExecutor.execute[org.apache.spark.sql.execution.adaptive.AQEOptimizer@6e53bb4f] final result ==> old【
GlobalLimit 21
+- LocalLimit 21
   +- Project [cast(id#0 as string) AS id#14, name#1, cast(value#4 as string) AS value#16]
      +- Join Inner, (id#0 = id#3)
         :- LogicalQueryStage Project [id#0, name#1], ShuffleQueryStage 0
         +- LogicalQueryStage Project [id#3, value#4], ShuffleQueryStage 1
】==> new【
GlobalLimit 21
+- LocalLimit 21
   +- Project [cast(id#0 as string) AS id#14, name#1, cast(value#4 as string) AS value#16]
      +- Join Inner, (id#0 = id#3), leftHint=(strategy=no_broadcast_hash), rightHint=(strategy=no_broadcast_hash)
         :- LogicalQueryStage Project [id#0, name#1], ShuffleQueryStage 0
         +- LogicalQueryStage Project [id#3, value#4], ShuffleQueryStage 1
】


SparkPlanner(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2@a2fb8a5) - org.apache.spark.sql.execution.SparkStrategies$SpecialLimits$ -->【
ReturnAnswer
+- GlobalLimit 21
   +- LocalLimit 21
      +- Project [cast(id#0 as string) AS id#14, name#1, cast(value#4 as string) AS value#16]
         +- Join Inner, (id#0 = id#3), leftHint=(strategy=no_broadcast_hash), rightHint=(strategy=no_broadcast_hash)
            :- LogicalQueryStage Project [id#0, name#1], ShuffleQueryStage 0
            +- LogicalQueryStage Project [id#3, value#4], ShuffleQueryStage 1
】 --> 【
CollectLimit 21
+- PlanLater Project [cast(id#0 as string) AS id#14, name#1, cast(value#4 as string) AS value#16]
】

SparkPlanner(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2@a2fb8a5) - org.apache.spark.sql.execution.SparkStrategies$BasicOperators$ -->【
Project [cast(id#0 as string) AS id#14, name#1, cast(value#4 as string) AS value#16]
+- Join Inner, (id#0 = id#3), leftHint=(strategy=no_broadcast_hash), rightHint=(strategy=no_broadcast_hash)
   :- LogicalQueryStage Project [id#0, name#1], ShuffleQueryStage 0
   +- LogicalQueryStage Project [id#3, value#4], ShuffleQueryStage 1
】 --> 【
Project [cast(id#0 as string) AS id#14, name#1, cast(value#4 as string) AS value#16]
+- PlanLater Join Inner, (id#0 = id#3), leftHint=(strategy=no_broadcast_hash), rightHint=(strategy=no_broadcast_hash)
】

15:23:20.759 [ScalaTest-run-running-SortMergeJoinSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - Considering join on: Some((id#0 = id#3))
15:23:20.759 [ScalaTest-run-running-SortMergeJoinSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - leftKeys:List(id#0) | rightKeys:List(id#3)
15:23:20.759 [ScalaTest-run-running-SortMergeJoinSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - Considering join on: Some((id#0 = id#3))
15:23:20.759 [ScalaTest-run-running-SortMergeJoinSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - leftKeys:List(id#0) | rightKeys:List(id#3)
SparkPlanner(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2@a2fb8a5) - org.apache.spark.sql.execution.SparkStrategies$JoinSelection$ -->【
Join Inner, (id#0 = id#3), leftHint=(strategy=no_broadcast_hash), rightHint=(strategy=no_broadcast_hash)
:- LogicalQueryStage Project [id#0, name#1], ShuffleQueryStage 0
+- LogicalQueryStage Project [id#3, value#4], ShuffleQueryStage 1
】 --> 【
SortMergeJoin [id#0], [id#3], Inner
:- PlanLater LogicalQueryStage Project [id#0, name#1], ShuffleQueryStage 0
+- PlanLater LogicalQueryStage Project [id#3, value#4], ShuffleQueryStage 1
】

SparkPlanner(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2@a2fb8a5) - org.apache.spark.sql.execution.adaptive.LogicalQueryStageStrategy$ -->【
LogicalQueryStage Project [id#0, name#1], ShuffleQueryStage 0
】 --> 【
ShuffleQueryStage 0
+- Exchange hashpartitioning(id#0, 200), ENSURE_REQUIREMENTS, [plan_id=51]
   +- *(1) Project [id#0, name#1]
      +- *(1) Filter isnotnull(id#0)
         +- *(1) ColumnarToRow
            +- FileScan parquet default.t1[id#0,name#1,date#2] Batched: true, DataFilters: [isnotnull(id#0)], Format: Parquet, Location: CatalogFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-wareh..., PartitionFilters: [], PushedFilters: [IsNotNull(id)], ReadSchema: struct<id:int,name:string>
】

SparkPlanner(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2@a2fb8a5) - org.apache.spark.sql.execution.adaptive.LogicalQueryStageStrategy$ -->【
LogicalQueryStage Project [id#3, value#4], ShuffleQueryStage 1
】 --> 【
ShuffleQueryStage 1
+- Exchange hashpartitioning(id#3, 200), ENSURE_REQUIREMENTS, [plan_id=79]
   +- *(2) Project [id#3, value#4]
      +- *(2) Filter ((isnotnull(value#4) AND (value#4 > 1)) AND isnotnull(id#3))
         +- *(2) ColumnarToRow
            +- FileScan parquet default.t2[id#3,value#4,date#5] Batched: true, DataFilters: [isnotnull(value#4), (value#4 > 1), isnotnull(id#3)], Format: Parquet, Location: CatalogFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-wareh..., PartitionFilters: [], PushedFilters: [IsNotNull(value), GreaterThan(value,1), IsNotNull(id)], ReadSchema: struct<id:int,value:int>
】

org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec$ - applyPhysicalRules - org.apache.spark.sql.execution.exchange.EnsureRequirements -->【
CollectLimit 21
+- Project [cast(id#0 as string) AS id#14, name#1, cast(value#4 as string) AS value#16]
   +- SortMergeJoin [id#0], [id#3], Inner
      :- ShuffleQueryStage 0
      :  +- Exchange hashpartitioning(id#0, 200), ENSURE_REQUIREMENTS, [plan_id=51]
      :     +- *(1) Project [id#0, name#1]
      :        +- *(1) Filter isnotnull(id#0)
      :           +- *(1) ColumnarToRow
      :              +- FileScan parquet default.t1[id#0,name#1,date#2] Batched: true, DataFilters: [isnotnull(id#0)], Format: Parquet, Location: CatalogFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-wareh..., PartitionFilters: [], PushedFilters: [IsNotNull(id)], ReadSchema: struct<id:int,name:string>
      +- ShuffleQueryStage 1
         +- Exchange hashpartitioning(id#3, 200), ENSURE_REQUIREMENTS, [plan_id=79]
            +- *(2) Project [id#3, value#4]
               +- *(2) Filter ((isnotnull(value#4) AND (value#4 > 1)) AND isnotnull(id#3))
                  +- *(2) ColumnarToRow
                     +- FileScan parquet default.t2[id#3,value#4,date#5] Batched: true, DataFilters: [isnotnull(value#4), (value#4 > 1), isnotnull(id#3)], Format: Parquet, Location: CatalogFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-wareh..., PartitionFilters: [], PushedFilters: [IsNotNull(value), GreaterThan(value,1), IsNotNull(id)], ReadSchema: struct<id:int,value:int>
】AQE --> 【
CollectLimit 21
+- Project [cast(id#0 as string) AS id#14, name#1, cast(value#4 as string) AS value#16]
   +- SortMergeJoin [id#0], [id#3], Inner
      :- Sort [id#0 ASC NULLS FIRST], false, 0
      :  +- ShuffleQueryStage 0
      :     +- Exchange hashpartitioning(id#0, 200), ENSURE_REQUIREMENTS, [plan_id=51]
      :        +- *(1) Project [id#0, name#1]
      :           +- *(1) Filter isnotnull(id#0)
      :              +- *(1) ColumnarToRow
      :                 +- FileScan parquet default.t1[id#0,name#1,date#2] Batched: true, DataFilters: [isnotnull(id#0)], Format: Parquet, Location: CatalogFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-wareh..., PartitionFilters: [], PushedFilters: [IsNotNull(id)], ReadSchema: struct<id:int,name:string>
      +- Sort [id#3 ASC NULLS FIRST], false, 0
         +- ShuffleQueryStage 1
            +- Exchange hashpartitioning(id#3, 200), ENSURE_REQUIREMENTS, [plan_id=79]
               +- *(2) Project [id#3, value#4]
                  +- *(2) Filter ((isnotnull(value#4) AND (value#4 > 1)) AND isnotnull(id#3))
                     +- *(2) ColumnarToRow
                        +- FileScan parquet default.t2[id#3,value#4,date#5] Batched: true, DataFilters: [isnotnull(value#4), (value#4 > 1), isnotnull(id#3)], Format: Parquet, Location: CatalogFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-wareh..., PartitionFilters: [], PushedFilters: [IsNotNull(value), GreaterThan(value,1), IsNotNull(id)], ReadSchema: struct<id:int,value:int>
】

15:23:20.765 [ScalaTest-run-running-SortMergeJoinSpec] DEBUG org.apache.spark.sql.execution.adaptive.OptimizeSkewedJoin -
Optimizing skewed join.
Left side partitions size info:
median size: 1, max size: 97, min size: 0, avg size: 6
Right side partitions size info:
median size: 1, max size: 80, min size: 0, avg size: 5

15:23:20.770 [ScalaTest-run-running-SortMergeJoinSpec] DEBUG org.apache.spark.sql.execution.adaptive.OptimizeSkewedJoin - number of skewed partitions: left 0, right 0
org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec$ final result ==>【
CollectLimit 21
+- Project [cast(id#0 as string) AS id#14, name#1, cast(value#4 as string) AS value#16]
   +- SortMergeJoin [id#0], [id#3], Inner
      :- ShuffleQueryStage 0
      :  +- Exchange hashpartitioning(id#0, 200), ENSURE_REQUIREMENTS, [plan_id=51]
      :     +- *(1) Project [id#0, name#1]
      :        +- *(1) Filter isnotnull(id#0)
      :           +- *(1) ColumnarToRow
      :              +- FileScan parquet default.t1[id#0,name#1,date#2] Batched: true, DataFilters: [isnotnull(id#0)], Format: Parquet, Location: CatalogFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-wareh..., PartitionFilters: [], PushedFilters: [IsNotNull(id)], ReadSchema: struct<id:int,name:string>
      +- ShuffleQueryStage 1
         +- Exchange hashpartitioning(id#3, 200), ENSURE_REQUIREMENTS, [plan_id=79]
            +- *(2) Project [id#3, value#4]
               +- *(2) Filter ((isnotnull(value#4) AND (value#4 > 1)) AND isnotnull(id#3))
                  +- *(2) ColumnarToRow
                     +- FileScan parquet default.t2[id#3,value#4,date#5] Batched: true, DataFilters: [isnotnull(value#4), (value#4 > 1), isnotnull(id#3)], Format: Parquet, Location: CatalogFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-wareh..., PartitionFilters: [], PushedFilters: [IsNotNull(value), GreaterThan(value,1), IsNotNull(id)], ReadSchema: struct<id:int,value:int>
】AQE ==> 【
CollectLimit 21
+- Project [cast(id#0 as string) AS id#14, name#1, cast(value#4 as string) AS value#16]
   +- SortMergeJoin [id#0], [id#3], Inner
      :- Sort [id#0 ASC NULLS FIRST], false, 0
      :  +- ShuffleQueryStage 0
      :     +- Exchange hashpartitioning(id#0, 200), ENSURE_REQUIREMENTS, [plan_id=51]
      :        +- *(1) Project [id#0, name#1]
      :           +- *(1) Filter isnotnull(id#0)
      :              +- *(1) ColumnarToRow
      :                 +- FileScan parquet default.t1[id#0,name#1,date#2] Batched: true, DataFilters: [isnotnull(id#0)], Format: Parquet, Location: CatalogFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-wareh..., PartitionFilters: [], PushedFilters: [IsNotNull(id)], ReadSchema: struct<id:int,name:string>
      +- Sort [id#3 ASC NULLS FIRST], false, 0
         +- ShuffleQueryStage 1
            +- Exchange hashpartitioning(id#3, 200), ENSURE_REQUIREMENTS, [plan_id=79]
               +- *(2) Project [id#3, value#4]
                  +- *(2) Filter ((isnotnull(value#4) AND (value#4 > 1)) AND isnotnull(id#3))
                     +- *(2) ColumnarToRow
                        +- FileScan parquet default.t2[id#3,value#4,date#5] Batched: true, DataFilters: [isnotnull(value#4), (value#4 > 1), isnotnull(id#3)], Format: Parquet, Location: CatalogFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-wareh..., PartitionFilters: [], PushedFilters: [IsNotNull(value), GreaterThan(value,1), IsNotNull(id)], ReadSchema: struct<id:int,value:int>
】

15:23:20.777 [ScalaTest-run-running-SortMergeJoinSpec] INFO  org.apache.spark.sql.execution.adaptive.ShufflePartitionsUtil - For shuffle(0, 1), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
optimizeQueryStage - org.apache.spark.sql.execution.adaptive.CoalesceShufflePartitions => old【
CollectLimit 21
+- Project [cast(id#0 as string) AS id#14, name#1, cast(value#4 as string) AS value#16]
   +- SortMergeJoin [id#0], [id#3], Inner
      :- Sort [id#0 ASC NULLS FIRST], false, 0
      :  +- ShuffleQueryStage 0
      :     +- Exchange hashpartitioning(id#0, 200), ENSURE_REQUIREMENTS, [plan_id=51]
      :        +- *(1) Project [id#0, name#1]
      :           +- *(1) Filter isnotnull(id#0)
      :              +- *(1) ColumnarToRow
      :                 +- FileScan parquet default.t1[id#0,name#1,date#2] Batched: true, DataFilters: [isnotnull(id#0)], Format: Parquet, Location: CatalogFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-wareh..., PartitionFilters: [], PushedFilters: [IsNotNull(id)], ReadSchema: struct<id:int,name:string>
      +- Sort [id#3 ASC NULLS FIRST], false, 0
         +- ShuffleQueryStage 1
            +- Exchange hashpartitioning(id#3, 200), ENSURE_REQUIREMENTS, [plan_id=79]
               +- *(2) Project [id#3, value#4]
                  +- *(2) Filter ((isnotnull(value#4) AND (value#4 > 1)) AND isnotnull(id#3))
                     +- *(2) ColumnarToRow
                        +- FileScan parquet default.t2[id#3,value#4,date#5] Batched: true, DataFilters: [isnotnull(value#4), (value#4 > 1), isnotnull(id#3)], Format: Parquet, Location: CatalogFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-wareh..., PartitionFilters: [], PushedFilters: [IsNotNull(value), GreaterThan(value,1), IsNotNull(id)], ReadSchema: struct<id:int,value:int>
】AQE ==> new【
CollectLimit 21
+- Project [cast(id#0 as string) AS id#14, name#1, cast(value#4 as string) AS value#16]
   +- SortMergeJoin [id#0], [id#3], Inner
      :- Sort [id#0 ASC NULLS FIRST], false, 0
      :  +- AQEShuffleRead coalesced
      :     +- ShuffleQueryStage 0
      :        +- Exchange hashpartitioning(id#0, 200), ENSURE_REQUIREMENTS, [plan_id=51]
      :           +- *(1) Project [id#0, name#1]
      :              +- *(1) Filter isnotnull(id#0)
      :                 +- *(1) ColumnarToRow
      :                    +- FileScan parquet default.t1[id#0,name#1,date#2] Batched: true, DataFilters: [isnotnull(id#0)], Format: Parquet, Location: CatalogFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-wareh..., PartitionFilters: [], PushedFilters: [IsNotNull(id)], ReadSchema: struct<id:int,name:string>
      +- Sort [id#3 ASC NULLS FIRST], false, 0
         +- AQEShuffleRead coalesced
            +- ShuffleQueryStage 1
               +- Exchange hashpartitioning(id#3, 200), ENSURE_REQUIREMENTS, [plan_id=79]
                  +- *(2) Project [id#3, value#4]
                     +- *(2) Filter ((isnotnull(value#4) AND (value#4 > 1)) AND isnotnull(id#3))
                        +- *(2) ColumnarToRow
                           +- FileScan parquet default.t2[id#3,value#4,date#5] Batched: true, DataFilters: [isnotnull(value#4), (value#4 > 1), isnotnull(id#3)], Format: Parquet, Location: CatalogFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-wareh..., PartitionFilters: [], PushedFilters: [IsNotNull(value), GreaterThan(value,1), IsNotNull(id)], ReadSchema: struct<id:int,value:int>
】


org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec$ - applyPhysicalRules - org.apache.spark.sql.execution.CollapseCodegenStages -->【
CollectLimit 21
+- Project [cast(id#0 as string) AS id#14, name#1, cast(value#4 as string) AS value#16]
   +- SortMergeJoin [id#0], [id#3], Inner
      :- Sort [id#0 ASC NULLS FIRST], false, 0
      :  +- AQEShuffleRead coalesced
      :     +- ShuffleQueryStage 0
      :        +- Exchange hashpartitioning(id#0, 200), ENSURE_REQUIREMENTS, [plan_id=51]
      :           +- *(1) Project [id#0, name#1]
      :              +- *(1) Filter isnotnull(id#0)
      :                 +- *(1) ColumnarToRow
      :                    +- FileScan parquet default.t1[id#0,name#1,date#2] Batched: true, DataFilters: [isnotnull(id#0)], Format: Parquet, Location: CatalogFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-wareh..., PartitionFilters: [], PushedFilters: [IsNotNull(id)], ReadSchema: struct<id:int,name:string>
      +- Sort [id#3 ASC NULLS FIRST], false, 0
         +- AQEShuffleRead coalesced
            +- ShuffleQueryStage 1
               +- Exchange hashpartitioning(id#3, 200), ENSURE_REQUIREMENTS, [plan_id=79]
                  +- *(2) Project [id#3, value#4]
                     +- *(2) Filter ((isnotnull(value#4) AND (value#4 > 1)) AND isnotnull(id#3))
                        +- *(2) ColumnarToRow
                           +- FileScan parquet default.t2[id#3,value#4,date#5] Batched: true, DataFilters: [isnotnull(value#4), (value#4 > 1), isnotnull(id#3)], Format: Parquet, Location: CatalogFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-wareh..., PartitionFilters: [], PushedFilters: [IsNotNull(value), GreaterThan(value,1), IsNotNull(id)], ReadSchema: struct<id:int,value:int>
】AQE --> 【
CollectLimit 21
+- *(5) Project [cast(id#0 as string) AS id#14, name#1, cast(value#4 as string) AS value#16]
   +- *(5) SortMergeJoin [id#0], [id#3], Inner
      :- *(3) Sort [id#0 ASC NULLS FIRST], false, 0
      :  +- AQEShuffleRead coalesced
      :     +- ShuffleQueryStage 0
      :        +- Exchange hashpartitioning(id#0, 200), ENSURE_REQUIREMENTS, [plan_id=51]
      :           +- *(1) Project [id#0, name#1]
      :              +- *(1) Filter isnotnull(id#0)
      :                 +- *(1) ColumnarToRow
      :                    +- FileScan parquet default.t1[id#0,name#1,date#2] Batched: true, DataFilters: [isnotnull(id#0)], Format: Parquet, Location: CatalogFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-wareh..., PartitionFilters: [], PushedFilters: [IsNotNull(id)], ReadSchema: struct<id:int,name:string>
      +- *(4) Sort [id#3 ASC NULLS FIRST], false, 0
         +- AQEShuffleRead coalesced
            +- ShuffleQueryStage 1
               +- Exchange hashpartitioning(id#3, 200), ENSURE_REQUIREMENTS, [plan_id=79]
                  +- *(2) Project [id#3, value#4]
                     +- *(2) Filter ((isnotnull(value#4) AND (value#4 > 1)) AND isnotnull(id#3))
                        +- *(2) ColumnarToRow
                           +- FileScan parquet default.t2[id#3,value#4,date#5] Batched: true, DataFilters: [isnotnull(value#4), (value#4 > 1), isnotnull(id#3)], Format: Parquet, Location: CatalogFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-wareh..., PartitionFilters: [], PushedFilters: [IsNotNull(value), GreaterThan(value,1), IsNotNull(id)], ReadSchema: struct<id:int,value:int>
】

org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec$ final result ==>【
CollectLimit 21
+- Project [cast(id#0 as string) AS id#14, name#1, cast(value#4 as string) AS value#16]
   +- SortMergeJoin [id#0], [id#3], Inner
      :- Sort [id#0 ASC NULLS FIRST], false, 0
      :  +- AQEShuffleRead coalesced
      :     +- ShuffleQueryStage 0
      :        +- Exchange hashpartitioning(id#0, 200), ENSURE_REQUIREMENTS, [plan_id=51]
      :           +- *(1) Project [id#0, name#1]
      :              +- *(1) Filter isnotnull(id#0)
      :                 +- *(1) ColumnarToRow
      :                    +- FileScan parquet default.t1[id#0,name#1,date#2] Batched: true, DataFilters: [isnotnull(id#0)], Format: Parquet, Location: CatalogFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-wareh..., PartitionFilters: [], PushedFilters: [IsNotNull(id)], ReadSchema: struct<id:int,name:string>
      +- Sort [id#3 ASC NULLS FIRST], false, 0
         +- AQEShuffleRead coalesced
            +- ShuffleQueryStage 1
               +- Exchange hashpartitioning(id#3, 200), ENSURE_REQUIREMENTS, [plan_id=79]
                  +- *(2) Project [id#3, value#4]
                     +- *(2) Filter ((isnotnull(value#4) AND (value#4 > 1)) AND isnotnull(id#3))
                        +- *(2) ColumnarToRow
                           +- FileScan parquet default.t2[id#3,value#4,date#5] Batched: true, DataFilters: [isnotnull(value#4), (value#4 > 1), isnotnull(id#3)], Format: Parquet, Location: CatalogFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-wareh..., PartitionFilters: [], PushedFilters: [IsNotNull(value), GreaterThan(value,1), IsNotNull(id)], ReadSchema: struct<id:int,value:int>
】AQE ==> 【
CollectLimit 21
+- *(5) Project [cast(id#0 as string) AS id#14, name#1, cast(value#4 as string) AS value#16]
   +- *(5) SortMergeJoin [id#0], [id#3], Inner
      :- *(3) Sort [id#0 ASC NULLS FIRST], false, 0
      :  +- AQEShuffleRead coalesced
      :     +- ShuffleQueryStage 0
      :        +- Exchange hashpartitioning(id#0, 200), ENSURE_REQUIREMENTS, [plan_id=51]
      :           +- *(1) Project [id#0, name#1]
      :              +- *(1) Filter isnotnull(id#0)
      :                 +- *(1) ColumnarToRow
      :                    +- FileScan parquet default.t1[id#0,name#1,date#2] Batched: true, DataFilters: [isnotnull(id#0)], Format: Parquet, Location: CatalogFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-wareh..., PartitionFilters: [], PushedFilters: [IsNotNull(id)], ReadSchema: struct<id:int,name:string>
      +- *(4) Sort [id#3 ASC NULLS FIRST], false, 0
         +- AQEShuffleRead coalesced
            +- ShuffleQueryStage 1
               +- Exchange hashpartitioning(id#3, 200), ENSURE_REQUIREMENTS, [plan_id=79]
                  +- *(2) Project [id#3, value#4]
                     +- *(2) Filter ((isnotnull(value#4) AND (value#4 > 1)) AND isnotnull(id#3))
                        +- *(2) ColumnarToRow
                           +- FileScan parquet default.t2[id#3,value#4,date#5] Batched: true, DataFilters: [isnotnull(value#4), (value#4 > 1), isnotnull(id#3)], Format: Parquet, Location: CatalogFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-wareh..., PartitionFilters: [], PushedFilters: [IsNotNull(value), GreaterThan(value,1), IsNotNull(id)], ReadSchema: struct<id:int,value:int>
】

15:23:20.799 [ScalaTest-run-running-SortMergeJoinSpec] DEBUG org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec - withFinalPlanUpdate.executeCollect ==> after get final plan:
CollectLimit 21
+- *(5) Project [cast(id#0 as string) AS id#14, name#1, cast(value#4 as string) AS value#16]
   +- *(5) SortMergeJoin [id#0], [id#3], Inner
      :- *(3) Sort [id#0 ASC NULLS FIRST], false, 0
      :  +- AQEShuffleRead coalesced
      :     +- ShuffleQueryStage 0
      :        +- Exchange hashpartitioning(id#0, 200), ENSURE_REQUIREMENTS, [plan_id=51]
      :           +- *(1) Project [id#0, name#1]
      :              +- *(1) Filter isnotnull(id#0)
      :                 +- *(1) ColumnarToRow
      :                    +- FileScan parquet default.t1[id#0,name#1,date#2] Batched: true, DataFilters: [isnotnull(id#0)], Format: Parquet, Location: CatalogFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-wareh..., PartitionFilters: [], PushedFilters: [IsNotNull(id)], ReadSchema: struct<id:int,name:string>
      +- *(4) Sort [id#3 ASC NULLS FIRST], false, 0
         +- AQEShuffleRead coalesced
            +- ShuffleQueryStage 1
               +- Exchange hashpartitioning(id#3, 200), ENSURE_REQUIREMENTS, [plan_id=79]
                  +- *(2) Project [id#3, value#4]
                     +- *(2) Filter ((isnotnull(value#4) AND (value#4 > 1)) AND isnotnull(id#3))
                        +- *(2) ColumnarToRow
                           +- FileScan parquet default.t2[id#3,value#4,date#5] Batched: true, DataFilters: [isnotnull(value#4), (value#4 > 1), isnotnull(id#3)], Format: Parquet, Location: CatalogFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-wareh..., PartitionFilters: [], PushedFilters: [IsNotNull(value), GreaterThan(value,1), IsNotNull(id)], ReadSchema: struct<id:int,value:int>

15:23:20.814 [ScalaTest-run-running-SortMergeJoinSpec] DEBUG org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator -
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage5(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=5
/* 006 */ final class GeneratedIteratorForCodegenStage5 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private scala.collection.Iterator smj_streamedInput_0;
/* 010 */   private scala.collection.Iterator smj_bufferedInput_0;
/* 011 */   private InternalRow smj_streamedRow_0;
/* 012 */   private InternalRow smj_bufferedRow_0;
/* 013 */   private int smj_value_2;
/* 014 */   private org.apache.spark.sql.execution.ExternalAppendOnlyUnsafeRowArray smj_matches_0;
/* 015 */   private int smj_value_3;
/* 016 */   private boolean wholestagecodegen_initJoin_0;
/* 017 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] smj_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[2];
/* 018 */
/* 019 */   public GeneratedIteratorForCodegenStage5(Object[] references) {
/* 020 */     this.references = references;
/* 021 */   }
/* 022 */
/* 023 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 024 */     partitionIndex = index;
/* 025 */     this.inputs = inputs;
/* 026 */     smj_streamedInput_0 = inputs[0];
/* 027 */     smj_bufferedInput_0 = inputs[1];
/* 028 */
/* 029 */     smj_matches_0 = new org.apache.spark.sql.execution.ExternalAppendOnlyUnsafeRowArray(2147483632, 2147483647);
/* 030 */     smj_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(4, 32);
/* 031 */     smj_mutableStateArray_0[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(3, 96);
/* 032 */
/* 033 */   }
/* 034 */
/* 035 */   private boolean smj_findNextJoinRows_0(
/* 036 */     scala.collection.Iterator streamedIter,
/* 037 */     scala.collection.Iterator bufferedIter) {
/* 038 */     smj_streamedRow_0 = null;
/* 039 */     int comp = 0;
/* 040 */     while (smj_streamedRow_0 == null) {
/* 041 */       if (!streamedIter.hasNext()) return false;
/* 042 */       smj_streamedRow_0 = (InternalRow) streamedIter.next();
/* 043 */       boolean smj_isNull_0 = smj_streamedRow_0.isNullAt(0);
/* 044 */       int smj_value_0 = smj_isNull_0 ?
/* 045 */       -1 : (smj_streamedRow_0.getInt(0));
/* 046 */       if (smj_isNull_0) {
/* 047 */         smj_streamedRow_0 = null;
/* 048 */         continue;
/* 049 */
/* 050 */       }
/* 051 */       if (!smj_matches_0.isEmpty()) {
/* 052 */         comp = 0;
/* 053 */         if (comp == 0) {
/* 054 */           comp = (smj_value_0 > smj_value_3 ? 1 : smj_value_0 < smj_value_3 ? -1 : 0);
/* 055 */         }
/* 056 */
/* 057 */         if (comp == 0) {
/* 058 */           return true;
/* 059 */         }
/* 060 */         smj_matches_0.clear();
/* 061 */       }
/* 062 */
/* 063 */       do {
/* 064 */         if (smj_bufferedRow_0 == null) {
/* 065 */           if (!bufferedIter.hasNext()) {
/* 066 */             smj_value_3 = smj_value_0;
/* 067 */             return !smj_matches_0.isEmpty();
/* 068 */           }
/* 069 */           smj_bufferedRow_0 = (InternalRow) bufferedIter.next();
/* 070 */           boolean smj_isNull_1 = smj_bufferedRow_0.isNullAt(0);
/* 071 */           int smj_value_1 = smj_isNull_1 ?
/* 072 */           -1 : (smj_bufferedRow_0.getInt(0));
/* 073 */           if (smj_isNull_1) {
/* 074 */             smj_bufferedRow_0 = null;
/* 075 */             continue;
/* 076 */           }
/* 077 */           smj_value_2 = smj_value_1;
/* 078 */         }
/* 079 */
/* 080 */         comp = 0;
/* 081 */         if (comp == 0) {
/* 082 */           comp = (smj_value_0 > smj_value_2 ? 1 : smj_value_0 < smj_value_2 ? -1 : 0);
/* 083 */         }
/* 084 */
/* 085 */         if (comp > 0) {
/* 086 */           smj_bufferedRow_0 = null;
/* 087 */         } else if (comp < 0) {
/* 088 */           if (!smj_matches_0.isEmpty()) {
/* 089 */             smj_value_3 = smj_value_0;
/* 090 */             return true;
/* 091 */           } else {
/* 092 */             smj_streamedRow_0 = null;
/* 093 */           }
/* 094 */         } else {
/* 095 */           smj_matches_0.add((UnsafeRow) smj_bufferedRow_0);
/* 096 */           smj_bufferedRow_0 = null;
/* 097 */         }
/* 098 */       } while (smj_streamedRow_0 != null);
/* 099 */     }
/* 100 */     return false; // unreachable
/* 101 */   }
/* 102 */
/* 103 */   protected void processNext() throws java.io.IOException {
/* 104 */     if (!wholestagecodegen_initJoin_0) {
/* 105 */       wholestagecodegen_initJoin_0 = true;
/* 106 */
/* 107 */       ((org.apache.spark.sql.execution.joins.SortMergeJoinExec) references[1] /* plan */).getTaskContext().addTaskCompletionListener(
/* 108 */         new org.apache.spark.util.TaskCompletionListener() {
/* 109 */           @Override
/* 110 */           public void onTaskCompletion(org.apache.spark.TaskContext context) {
/* 111 */             ((org.apache.spark.sql.execution.metric.SQLMetric) references[2] /* spillSize */).add(smj_matches_0.spillSize());
/* 112 */           }
/* 113 */         });
/* 114 */
/* 115 */     }
/* 116 */
/* 117 */     while (smj_findNextJoinRows_0(smj_streamedInput_0, smj_bufferedInput_0)) {
/* 118 */       boolean smj_isNull_2 = false;
/* 119 */       int smj_value_4 = -1;
/* 120 */
/* 121 */       boolean smj_isNull_3 = false;
/* 122 */       UTF8String smj_value_5 = null;
/* 123 */
/* 124 */       smj_isNull_2 = smj_streamedRow_0.isNullAt(0);
/* 125 */       smj_value_4 = smj_isNull_2 ? -1 : (smj_streamedRow_0.getInt(0));
/* 126 */       smj_isNull_3 = smj_streamedRow_0.isNullAt(1);
/* 127 */       smj_value_5 = smj_isNull_3 ? null : (smj_streamedRow_0.getUTF8String(1));
/* 128 */       scala.collection.Iterator<UnsafeRow> smj_iterator_0 = smj_matches_0.generateIterator();
/* 129 */
/* 130 */       while (smj_iterator_0.hasNext()) {
/* 131 */         InternalRow smj_bufferedRow_1 = (InternalRow) smj_iterator_0.next();
/* 132 */
/* 133 */         ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);
/* 134 */
/* 135 */         // common sub-expressions
/* 136 */
/* 137 */         boolean project_isNull_0 = smj_isNull_2;
/* 138 */         UTF8String project_value_0 = null;
/* 139 */         if (!smj_isNull_2) {
/* 140 */           project_value_0 = UTF8String.fromString(String.valueOf(smj_value_4));
/* 141 */         }
/* 142 */         boolean smj_isNull_5 = smj_bufferedRow_1.isNullAt(1);
/* 143 */         int smj_value_7 = smj_isNull_5 ?
/* 144 */         -1 : (smj_bufferedRow_1.getInt(1));
/* 145 */         boolean project_isNull_3 = smj_isNull_5;
/* 146 */         UTF8String project_value_3 = null;
/* 147 */         if (!smj_isNull_5) {
/* 148 */           project_value_3 = UTF8String.fromString(String.valueOf(smj_value_7));
/* 149 */         }
/* 150 */         smj_mutableStateArray_0[1].reset();
/* 151 */
/* 152 */         smj_mutableStateArray_0[1].zeroOutNullBytes();
/* 153 */
/* 154 */         if (project_isNull_0) {
/* 155 */           smj_mutableStateArray_0[1].setNullAt(0);
/* 156 */         } else {
/* 157 */           smj_mutableStateArray_0[1].write(0, project_value_0);
/* 158 */         }
/* 159 */
/* 160 */         if (smj_isNull_3) {
/* 161 */           smj_mutableStateArray_0[1].setNullAt(1);
/* 162 */         } else {
/* 163 */           smj_mutableStateArray_0[1].write(1, smj_value_5);
/* 164 */         }
/* 165 */
/* 166 */         if (project_isNull_3) {
/* 167 */           smj_mutableStateArray_0[1].setNullAt(2);
/* 168 */         } else {
/* 169 */           smj_mutableStateArray_0[1].write(2, project_value_3);
/* 170 */         }
/* 171 */         append((smj_mutableStateArray_0[1].getRow()).copy());
/* 172 */
/* 173 */       }
/* 174 */       if (shouldStop()) return;
/* 175 */     }
/* 176 */     ((org.apache.spark.sql.execution.joins.SortMergeJoinExec) references[1] /* plan */).cleanupResources();
/* 177 */   }
/* 178 */
/* 179 */ }

15:23:20.847 [ScalaTest-run-running-SortMergeJoinSpec] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 35.320612 ms
15:23:20.856 [ScalaTest-run-running-SortMergeJoinSpec] DEBUG org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator -
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage3(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=3
/* 006 */ final class GeneratedIteratorForCodegenStage3 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private boolean sort_needToSort_0;
/* 010 */   private org.apache.spark.sql.execution.UnsafeExternalRowSorter sort_sorter_0;
/* 011 */   private org.apache.spark.executor.TaskMetrics sort_metrics_0;
/* 012 */   private scala.collection.Iterator<UnsafeRow> sort_sortedIter_0;
/* 013 */   private scala.collection.Iterator inputadapter_input_0;
/* 014 */
/* 015 */   public GeneratedIteratorForCodegenStage3(Object[] references) {
/* 016 */     this.references = references;
/* 017 */   }
/* 018 */
/* 019 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 020 */     partitionIndex = index;
/* 021 */     this.inputs = inputs;
/* 022 */     sort_needToSort_0 = true;
/* 023 */     sort_sorter_0 = ((org.apache.spark.sql.execution.SortExec) references[0] /* plan */).createSorter();
/* 024 */     sort_metrics_0 = org.apache.spark.TaskContext.get().taskMetrics();
/* 025 */
/* 026 */     inputadapter_input_0 = inputs[0];
/* 027 */
/* 028 */   }
/* 029 */
/* 030 */   private void sort_addToSorter_0() throws java.io.IOException {
/* 031 */     while ( inputadapter_input_0.hasNext()) {
/* 032 */       InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next();
/* 033 */
/* 034 */       sort_sorter_0.insertRow((UnsafeRow)inputadapter_row_0);
/* 035 */       // shouldStop check is eliminated
/* 036 */     }
/* 037 */
/* 038 */   }
/* 039 */
/* 040 */   protected void processNext() throws java.io.IOException {
/* 041 */     if (sort_needToSort_0) {
/* 042 */       long sort_spillSizeBefore_0 = sort_metrics_0.memoryBytesSpilled();
/* 043 */       sort_addToSorter_0();
/* 044 */       sort_sortedIter_0 = sort_sorter_0.sort();
/* 045 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[3] /* sortTime */).add(sort_sorter_0.getSortTimeNanos() / 1000000);
/* 046 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* peakMemory */).add(sort_sorter_0.getPeakMemoryUsage());
/* 047 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[2] /* spillSize */).add(sort_metrics_0.memoryBytesSpilled() - sort_spillSizeBefore_0);
/* 048 */       sort_metrics_0.incPeakExecutionMemory(sort_sorter_0.getPeakMemoryUsage());
/* 049 */       sort_needToSort_0 = false;
/* 050 */     }
/* 051 */
/* 052 */     while ( sort_sortedIter_0.hasNext()) {
/* 053 */       UnsafeRow sort_outputRow_0 = (UnsafeRow)sort_sortedIter_0.next();
/* 054 */
/* 055 */       append(sort_outputRow_0);
/* 056 */
/* 057 */       if (shouldStop()) return;
/* 058 */     }
/* 059 */   }
/* 060 */
/* 061 */ }

15:23:20.869 [ScalaTest-run-running-SortMergeJoinSpec] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 15.210287 ms
15:23:20.873 [ScalaTest-run-running-SortMergeJoinSpec] DEBUG org.apache.spark.util.ClosureCleaner - Cleaning indylambda closure: $anonfun$doExecute$4$adapted
15:23:20.874 [ScalaTest-run-running-SortMergeJoinSpec] DEBUG org.apache.spark.util.ClosureCleaner -  +++ indylambda closure ($anonfun$doExecute$4$adapted) is now cleaned +++
15:23:20.946 [ScalaTest-run-running-SortMergeJoinSpec] DEBUG org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator -
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage4(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=4
/* 006 */ final class GeneratedIteratorForCodegenStage4 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private boolean sort_needToSort_0;
/* 010 */   private org.apache.spark.sql.execution.UnsafeExternalRowSorter sort_sorter_0;
/* 011 */   private org.apache.spark.executor.TaskMetrics sort_metrics_0;
/* 012 */   private scala.collection.Iterator<UnsafeRow> sort_sortedIter_0;
/* 013 */   private scala.collection.Iterator inputadapter_input_0;
/* 014 */
/* 015 */   public GeneratedIteratorForCodegenStage4(Object[] references) {
/* 016 */     this.references = references;
/* 017 */   }
/* 018 */
/* 019 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 020 */     partitionIndex = index;
/* 021 */     this.inputs = inputs;
/* 022 */     sort_needToSort_0 = true;
/* 023 */     sort_sorter_0 = ((org.apache.spark.sql.execution.SortExec) references[0] /* plan */).createSorter();
/* 024 */     sort_metrics_0 = org.apache.spark.TaskContext.get().taskMetrics();
/* 025 */
/* 026 */     inputadapter_input_0 = inputs[0];
/* 027 */
/* 028 */   }
/* 029 */
/* 030 */   private void sort_addToSorter_0() throws java.io.IOException {
/* 031 */     while ( inputadapter_input_0.hasNext()) {
/* 032 */       InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next();
/* 033 */
/* 034 */       sort_sorter_0.insertRow((UnsafeRow)inputadapter_row_0);
/* 035 */       // shouldStop check is eliminated
/* 036 */     }
/* 037 */
/* 038 */   }
/* 039 */
/* 040 */   protected void processNext() throws java.io.IOException {
/* 041 */     if (sort_needToSort_0) {
/* 042 */       long sort_spillSizeBefore_0 = sort_metrics_0.memoryBytesSpilled();
/* 043 */       sort_addToSorter_0();
/* 044 */       sort_sortedIter_0 = sort_sorter_0.sort();
/* 045 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[3] /* sortTime */).add(sort_sorter_0.getSortTimeNanos() / 1000000);
/* 046 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* peakMemory */).add(sort_sorter_0.getPeakMemoryUsage());
/* 047 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[2] /* spillSize */).add(sort_metrics_0.memoryBytesSpilled() - sort_spillSizeBefore_0);
/* 048 */       sort_metrics_0.incPeakExecutionMemory(sort_sorter_0.getPeakMemoryUsage());
/* 049 */       sort_needToSort_0 = false;
/* 050 */     }
/* 051 */
/* 052 */     while ( sort_sortedIter_0.hasNext()) {
/* 053 */       UnsafeRow sort_outputRow_0 = (UnsafeRow)sort_sortedIter_0.next();
/* 054 */
/* 055 */       append(sort_outputRow_0);
/* 056 */
/* 057 */       if (shouldStop()) return;
/* 058 */     }
/* 059 */   }
/* 060 */
/* 061 */ }

15:23:20.957 [ScalaTest-run-running-SortMergeJoinSpec] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 11.24573 ms
15:23:20.958 [ScalaTest-run-running-SortMergeJoinSpec] DEBUG org.apache.spark.util.ClosureCleaner - Cleaning indylambda closure: $anonfun$doExecute$4$adapted
15:23:20.959 [ScalaTest-run-running-SortMergeJoinSpec] DEBUG org.apache.spark.util.ClosureCleaner -  +++ indylambda closure ($anonfun$doExecute$4$adapted) is now cleaned +++
15:23:20.968 [ScalaTest-run-running-SortMergeJoinSpec] DEBUG org.apache.spark.util.ClosureCleaner - Cleaning indylambda closure: $anonfun$doExecute$5
15:23:20.969 [ScalaTest-run-running-SortMergeJoinSpec] DEBUG org.apache.spark.util.ClosureCleaner -  +++ indylambda closure ($anonfun$doExecute$5) is now cleaned +++
15:23:20.971 [ScalaTest-run-running-SortMergeJoinSpec] DEBUG org.apache.spark.util.ClosureCleaner - Cleaning indylambda closure: $anonfun$doExecute$6$adapted
15:23:20.971 [ScalaTest-run-running-SortMergeJoinSpec] DEBUG org.apache.spark.util.ClosureCleaner -  +++ indylambda closure ($anonfun$doExecute$6$adapted) is now cleaned +++
15:23:20.989 [ScalaTest-run-running-SortMergeJoinSpec] DEBUG org.apache.spark.util.ClosureCleaner - Cleaning indylambda closure: $anonfun$executeTake$2
15:23:20.990 [ScalaTest-run-running-SortMergeJoinSpec] DEBUG org.apache.spark.util.ClosureCleaner -  +++ indylambda closure ($anonfun$executeTake$2) is now cleaned +++
15:23:20.992 [ScalaTest-run-running-SortMergeJoinSpec] DEBUG org.apache.spark.util.ClosureCleaner - Cleaning indylambda closure: $anonfun$runJob$5
15:23:20.995 [ScalaTest-run-running-SortMergeJoinSpec] DEBUG org.apache.spark.util.ClosureCleaner -  +++ indylambda closure ($anonfun$runJob$5) is now cleaned +++
15:23:20.996 [ScalaTest-run-running-SortMergeJoinSpec] INFO  org.apache.spark.SparkContext - Starting job: show at SortMergeJoinSpec.scala:132
15:23:20.997 [ScalaTest-run-running-SortMergeJoinSpec] DEBUG org.apache.spark.scheduler.DAGScheduler - eagerlyComputePartitionsForRddAndAncestors for RDD 14 took 0.000154 seconds
15:23:20.997 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - Merging stage rdd profiles: Set()
15:23:20.998 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - Merging stage rdd profiles: Set()
15:23:20.998 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - Merging stage rdd profiles: Set()
15:23:20.999 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 2 (show at SortMergeJoinSpec.scala:132) with 1 output partitions
15:23:20.999 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 4 (show at SortMergeJoinSpec.scala:132)
15:23:21.000 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 2, ShuffleMapStage 3)
15:23:21.000 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
15:23:21.001 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - submitStage(ResultStage 4 (name=show at SortMergeJoinSpec.scala:132;jobs=2))
15:23:21.002 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - missing: List()
15:23:21.002 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 4 (MapPartitionsRDD[14] at show at SortMergeJoinSpec.scala:132), which has no missing parents
15:23:21.002 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - submitMissingTasks(ResultStage 4)
15:23:21.016 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_4 stored as values in memory (estimated size 43.3 KiB, free 4.1 GiB)
15:23:21.016 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager - Put block broadcast_4 locally took 0 ms
15:23:21.016 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager - Putting block broadcast_4 without replication took 0 ms
15:23:21.017 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_4_piece0 stored as bytes in memory (estimated size 19.9 KiB, free 4.1 GiB)
15:23:21.018 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_4_piece0 for BlockManagerId(driver, 192.168.31.150, 55538, None)
15:23:21.018 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_4_piece0 in memory on 192.168.31.150:55538 (size: 19.9 KiB, free: 4.1 GiB)
15:23:21.018 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_4_piece0
15:23:21.018 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager - Told master about block broadcast_4_piece0
15:23:21.018 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager - Put block broadcast_4_piece0 locally took 1 ms
15:23:21.018 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager - Putting block broadcast_4_piece0 without replication took 1 ms
15:23:21.018 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 4 from broadcast at DAGScheduler.scala:1513
15:23:21.020 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[14] at show at SortMergeJoinSpec.scala:132) (first 15 tasks are for partitions Vector(0))
15:23:21.020 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 4.0 with 1 tasks resource profile 0
15:23:21.020 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.TaskSetManager - Epoch for TaskSet 4.0: 2
15:23:21.021 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.TaskSetManager - Adding pending tasks took 1 ms
15:23:21.022 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.TaskSetManager - Valid locality levels for TaskSet 4.0: NODE_LOCAL, ANY
15:23:21.022 [dispatcher-event-loop-1] DEBUG org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_4.0, runningTasks: 0
15:23:21.025 [dispatcher-event-loop-1] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 4.0 (TID 2) (192.168.31.150, executor driver, partition 0, NODE_LOCAL, 4735 bytes) taskResourceAssignments Map()
15:23:21.026 [Executor task launch worker for task 0.0 in stage 4.0 (TID 2)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 4.0 (TID 2)
15:23:21.027 [Executor task launch worker for task 0.0 in stage 4.0 (TID 2)] DEBUG org.apache.spark.executor.ExecutorMetricsPoller - stageTCMP: (4, 0) -> 1
15:23:21.027 [Executor task launch worker for task 0.0 in stage 4.0 (TID 2)] DEBUG org.apache.spark.storage.BlockManager - Getting local block broadcast_4
15:23:21.028 [Executor task launch worker for task 0.0 in stage 4.0 (TID 2)] DEBUG org.apache.spark.storage.BlockManager - Level for block broadcast_4 is StorageLevel(disk, memory, deserialized, 1 replicas)
15:23:21.098 [Executor task launch worker for task 0.0 in stage 4.0 (TID 2)] DEBUG org.apache.spark.MapOutputTrackerMaster - Fetching outputs for shuffle 0
15:23:21.101 [Executor task launch worker for task 0.0 in stage 4.0 (TID 2)] DEBUG org.apache.spark.MapOutputTrackerMaster - Convert map statuses for shuffle 0, mappers 0-1, partitions 0-200
15:23:21.127 [Executor task launch worker for task 0.0 in stage 4.0 (TID 2)] DEBUG org.apache.spark.storage.ShuffleBlockFetcherIterator - maxBytesInFlight: 50331648, targetRemoteRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
15:23:21.136 [Executor task launch worker for task 0.0 in stage 4.0 (TID 2)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 (1346.0 B) non-empty blocks including 1 (1346.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
15:23:21.138 [Executor task launch worker for task 0.0 in stage 4.0 (TID 2)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 16 ms
15:23:21.139 [Executor task launch worker for task 0.0 in stage 4.0 (TID 2)] DEBUG org.apache.spark.storage.ShuffleBlockFetcherIterator - Start fetching local blocks: (shuffle_0_0_24_193,0)
15:23:21.139 [Executor task launch worker for task 0.0 in stage 4.0 (TID 2)] DEBUG org.apache.spark.storage.BlockManager - Getting local shuffle block shuffle_0_0_24_193
15:23:21.147 [Executor task launch worker for task 0.0 in stage 4.0 (TID 2)] DEBUG org.apache.spark.storage.ShuffleBlockFetcherIterator - Got local blocks in 24 ms
15:23:21.158 [Executor task launch worker for task 0.0 in stage 4.0 (TID 2)] DEBUG org.apache.spark.sql.catalyst.expressions.codegen.GenerateOrdering - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     boolean isNull_0 = a.isNullAt(0);
/* 018 */     int value_0 = isNull_0 ?
/* 019 */     -1 : (a.getInt(0));
/* 020 */     boolean isNull_1 = b.isNullAt(0);
/* 021 */     int value_1 = isNull_1 ?
/* 022 */     -1 : (b.getInt(0));
/* 023 */     if (isNull_0 && isNull_1) {
/* 024 */       // Nothing
/* 025 */     } else if (isNull_0) {
/* 026 */       return -1;
/* 027 */     } else if (isNull_1) {
/* 028 */       return 1;
/* 029 */     } else {
/* 030 */       int comp = (value_0 > value_1 ? 1 : value_0 < value_1 ? -1 : 0);
/* 031 */       if (comp != 0) {
/* 032 */         return comp;
/* 033 */       }
/* 034 */     }
/* 035 */
/* 036 */     return 0;
/* 037 */   }
/* 038 */
/* 039 */
/* 040 */ }

15:23:21.159 [Executor task launch worker for task 0.0 in stage 4.0 (TID 2)] DEBUG org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator -
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     boolean isNull_0 = a.isNullAt(0);
/* 018 */     int value_0 = isNull_0 ?
/* 019 */     -1 : (a.getInt(0));
/* 020 */     boolean isNull_1 = b.isNullAt(0);
/* 021 */     int value_1 = isNull_1 ?
/* 022 */     -1 : (b.getInt(0));
/* 023 */     if (isNull_0 && isNull_1) {
/* 024 */       // Nothing
/* 025 */     } else if (isNull_0) {
/* 026 */       return -1;
/* 027 */     } else if (isNull_1) {
/* 028 */       return 1;
/* 029 */     } else {
/* 030 */       int comp = (value_0 > value_1 ? 1 : value_0 < value_1 ? -1 : 0);
/* 031 */       if (comp != 0) {
/* 032 */         return comp;
/* 033 */       }
/* 034 */     }
/* 035 */
/* 036 */     return 0;
/* 037 */   }
/* 038 */
/* 039 */
/* 040 */ }

15:23:21.177 [Executor task launch worker for task 0.0 in stage 4.0 (TID 2)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 18.699999 ms
15:23:21.186 [Executor task launch worker for task 0.0 in stage 4.0 (TID 2)] DEBUG org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeProjection - code for sortprefix(input[0, int, true] ASC NULLS FIRST):
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_1 = i.isNullAt(0);
/* 032 */     int value_1 = isNull_1 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     long value_0 = 0L;
/* 035 */     boolean isNull_0 = isNull_1;
/* 036 */     if (!isNull_1) {
/* 037 */       value_0 = (long) value_1;
/* 038 */     }
/* 039 */     if (isNull_0) {
/* 040 */       mutableStateArray_0[0].setNullAt(0);
/* 041 */     } else {
/* 042 */       mutableStateArray_0[0].write(0, value_0);
/* 043 */     }
/* 044 */     return (mutableStateArray_0[0].getRow());
/* 045 */   }
/* 046 */
/* 047 */
/* 048 */ }

15:23:21.187 [Executor task launch worker for task 0.0 in stage 4.0 (TID 2)] DEBUG org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator -
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_1 = i.isNullAt(0);
/* 032 */     int value_1 = isNull_1 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     long value_0 = 0L;
/* 035 */     boolean isNull_0 = isNull_1;
/* 036 */     if (!isNull_1) {
/* 037 */       value_0 = (long) value_1;
/* 038 */     }
/* 039 */     if (isNull_0) {
/* 040 */       mutableStateArray_0[0].setNullAt(0);
/* 041 */     } else {
/* 042 */       mutableStateArray_0[0].write(0, value_0);
/* 043 */     }
/* 044 */     return (mutableStateArray_0[0].getRow());
/* 045 */   }
/* 046 */
/* 047 */
/* 048 */ }

15:23:21.194 [Executor task launch worker for task 0.0 in stage 4.0 (TID 2)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 7.244902 ms
15:23:21.200 [Executor task launch worker for task 0.0 in stage 4.0 (TID 2)] DEBUG org.apache.spark.memory.TaskMemoryManager - Task 2 acquired 64.0 KiB for org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@5034005d
15:23:21.203 [Executor task launch worker for task 0.0 in stage 4.0 (TID 2)] DEBUG org.apache.spark.MapOutputTrackerMaster - Fetching outputs for shuffle 1
15:23:21.203 [Executor task launch worker for task 0.0 in stage 4.0 (TID 2)] DEBUG org.apache.spark.MapOutputTrackerMaster - Convert map statuses for shuffle 1, mappers 0-1, partitions 0-200
15:23:21.203 [Executor task launch worker for task 0.0 in stage 4.0 (TID 2)] DEBUG org.apache.spark.storage.ShuffleBlockFetcherIterator - maxBytesInFlight: 50331648, targetRemoteRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
15:23:21.203 [Executor task launch worker for task 0.0 in stage 4.0 (TID 2)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 (1150.0 B) non-empty blocks including 1 (1150.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
15:23:21.203 [Executor task launch worker for task 0.0 in stage 4.0 (TID 2)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
15:23:21.204 [Executor task launch worker for task 0.0 in stage 4.0 (TID 2)] DEBUG org.apache.spark.storage.ShuffleBlockFetcherIterator - Start fetching local blocks: (shuffle_1_1_11_199,0)
15:23:21.204 [Executor task launch worker for task 0.0 in stage 4.0 (TID 2)] DEBUG org.apache.spark.storage.BlockManager - Getting local shuffle block shuffle_1_1_11_199
15:23:21.204 [Executor task launch worker for task 0.0 in stage 4.0 (TID 2)] DEBUG org.apache.spark.storage.ShuffleBlockFetcherIterator - Got local blocks in 0 ms
15:23:21.206 [Executor task launch worker for task 0.0 in stage 4.0 (TID 2)] DEBUG org.apache.spark.sql.catalyst.expressions.codegen.GenerateOrdering - Generated Ordering by input[0, int, true] ASC NULLS FIRST:
/* 001 */ public SpecificOrdering generate(Object[] references) {
/* 002 */   return new SpecificOrdering(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.BaseOrdering {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */
/* 009 */
/* 010 */   public SpecificOrdering(Object[] references) {
/* 011 */     this.references = references;
/* 012 */
/* 013 */   }
/* 014 */
/* 015 */   public int compare(InternalRow a, InternalRow b) {
/* 016 */
/* 017 */     boolean isNull_0 = a.isNullAt(0);
/* 018 */     int value_0 = isNull_0 ?
/* 019 */     -1 : (a.getInt(0));
/* 020 */     boolean isNull_1 = b.isNullAt(0);
/* 021 */     int value_1 = isNull_1 ?
/* 022 */     -1 : (b.getInt(0));
/* 023 */     if (isNull_0 && isNull_1) {
/* 024 */       // Nothing
/* 025 */     } else if (isNull_0) {
/* 026 */       return -1;
/* 027 */     } else if (isNull_1) {
/* 028 */       return 1;
/* 029 */     } else {
/* 030 */       int comp = (value_0 > value_1 ? 1 : value_0 < value_1 ? -1 : 0);
/* 031 */       if (comp != 0) {
/* 032 */         return comp;
/* 033 */       }
/* 034 */     }
/* 035 */
/* 036 */     return 0;
/* 037 */   }
/* 038 */
/* 039 */
/* 040 */ }

15:23:21.207 [Executor task launch worker for task 0.0 in stage 4.0 (TID 2)] DEBUG org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeProjection - code for sortprefix(input[0, int, true] ASC NULLS FIRST):
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_1 = i.isNullAt(0);
/* 032 */     int value_1 = isNull_1 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     long value_0 = 0L;
/* 035 */     boolean isNull_0 = isNull_1;
/* 036 */     if (!isNull_1) {
/* 037 */       value_0 = (long) value_1;
/* 038 */     }
/* 039 */     if (isNull_0) {
/* 040 */       mutableStateArray_0[0].setNullAt(0);
/* 041 */     } else {
/* 042 */       mutableStateArray_0[0].write(0, value_0);
/* 043 */     }
/* 044 */     return (mutableStateArray_0[0].getRow());
/* 045 */   }
/* 046 */
/* 047 */
/* 048 */ }

15:23:21.208 [Executor task launch worker for task 0.0 in stage 4.0 (TID 2)] DEBUG org.apache.spark.memory.TaskMemoryManager - Task 2 acquired 64.0 KiB for org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@74671232
15:23:21.220 [Executor task launch worker for task 0.0 in stage 4.0 (TID 2)] DEBUG org.apache.spark.memory.TaskMemoryManager - Task 2 acquired 64.0 MiB for org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@5034005d
15:23:21.231 [Executor task launch worker for task 0.0 in stage 4.0 (TID 2)] DEBUG org.apache.spark.memory.TaskMemoryManager - Task 2 acquired 64.0 MiB for org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@74671232
15:23:21.239 [Executor task launch worker for task 0.0 in stage 4.0 (TID 2)] DEBUG org.apache.spark.memory.TaskMemoryManager - Task 2 release 64.0 MiB from org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@5034005d
15:23:21.239 [Executor task launch worker for task 0.0 in stage 4.0 (TID 2)] DEBUG org.apache.spark.memory.TaskMemoryManager - Task 2 release 64.0 KiB from org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@5034005d
15:23:21.239 [Executor task launch worker for task 0.0 in stage 4.0 (TID 2)] DEBUG org.apache.spark.memory.TaskMemoryManager - Task 2 release 64.0 MiB from org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@74671232
15:23:21.239 [Executor task launch worker for task 0.0 in stage 4.0 (TID 2)] DEBUG org.apache.spark.memory.TaskMemoryManager - Task 2 release 64.0 KiB from org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@74671232
15:23:21.240 [Executor task launch worker for task 0.0 in stage 4.0 (TID 2)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 4.0 (TID 2). 5476 bytes result sent to driver
15:23:21.241 [Executor task launch worker for task 0.0 in stage 4.0 (TID 2)] DEBUG org.apache.spark.executor.ExecutorMetricsPoller - stageTCMP: (4, 0) -> 0
15:23:21.241 [dispatcher-event-loop-1] DEBUG org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_4.0, runningTasks: 0
15:23:21.241 [dispatcher-event-loop-1] DEBUG org.apache.spark.scheduler.TaskSetManager - No tasks for locality level NODE_LOCAL, so moving to locality level ANY
15:23:21.241 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 4.0 (TID 2) in 218 ms on 192.168.31.150 (executor driver) (1/1)
15:23:21.241 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 4.0, whose tasks have all completed, from pool
15:23:21.242 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 4 (show at SortMergeJoinSpec.scala:132) finished in 0.232 s
15:23:21.243 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - After removal of stage 2, remaining stages = 2
15:23:21.243 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - After removal of stage 4, remaining stages = 1
15:23:21.243 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - After removal of stage 3, remaining stages = 0
15:23:21.243 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
15:23:21.244 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 4: Stage finished
15:23:21.244 [ScalaTest-run-running-SortMergeJoinSpec] INFO  org.apache.spark.scheduler.DAGScheduler - Job 2 finished: show at SortMergeJoinSpec.scala:132, took 0.248549 s
15:23:21.248 [ScalaTest-run-running-SortMergeJoinSpec] DEBUG org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec - withFinalPlanUpdate.executeCollect was finished.
15:23:21.249 [ScalaTest-run-running-SortMergeJoinSpec] DEBUG org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec - Final plan: CollectLimit 21
+- *(5) Project [cast(id#0 as string) AS id#14, name#1, cast(value#4 as string) AS value#16]
   +- *(5) SortMergeJoin [id#0], [id#3], Inner
      :- *(3) Sort [id#0 ASC NULLS FIRST], false, 0
      :  +- AQEShuffleRead coalesced
      :     +- ShuffleQueryStage 0
      :        +- Exchange hashpartitioning(id#0, 200), ENSURE_REQUIREMENTS, [plan_id=51]
      :           +- *(1) Project [id#0, name#1]
      :              +- *(1) Filter isnotnull(id#0)
      :                 +- *(1) ColumnarToRow
      :                    +- FileScan parquet default.t1[id#0,name#1,date#2] Batched: true, DataFilters: [isnotnull(id#0)], Format: Parquet, Location: CatalogFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-wareh..., PartitionFilters: [], PushedFilters: [IsNotNull(id)], ReadSchema: struct<id:int,name:string>
      +- *(4) Sort [id#3 ASC NULLS FIRST], false, 0
         +- AQEShuffleRead coalesced
            +- ShuffleQueryStage 1
               +- Exchange hashpartitioning(id#3, 200), ENSURE_REQUIREMENTS, [plan_id=79]
                  +- *(2) Project [id#3, value#4]
                     +- *(2) Filter ((isnotnull(value#4) AND (value#4 > 1)) AND isnotnull(id#3))
                        +- *(2) ColumnarToRow
                           +- FileScan parquet default.t2[id#3,value#4,date#5] Batched: true, DataFilters: [isnotnull(value#4), (value#4 > 1), isnotnull(id#3)], Format: Parquet, Location: CatalogFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-wareh..., PartitionFilters: [], PushedFilters: [IsNotNull(value), GreaterThan(value,1), IsNotNull(id)], ReadSchema: struct<id:int,value:int>

15:23:21.250 [ScalaTest-run-running-SortMergeJoinSpec] DEBUG org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec - withFinalPlanUpdate.executeCollect.finalPlanUpdate was finished.
15:23:21.263 [ScalaTest-run-running-SortMergeJoinSpec] DEBUG org.apache.spark.sql.catalyst.expressions.codegen.GenerateSafeProjection - code for createexternalrow(input[0, string, true].toString, input[1, string, true].toString, input[2, string, true].toString, StructField(id,StringType,true), StructField(name,StringType,true), StructField(value,StringType,true)):
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificSafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private InternalRow mutableRow;
/* 009 */
/* 010 */
/* 011 */   public SpecificSafeProjection(Object[] references) {
/* 012 */     this.references = references;
/* 013 */     mutableRow = (InternalRow) references[references.length - 1];
/* 014 */
/* 015 */   }
/* 016 */
/* 017 */   public void initialize(int partitionIndex) {
/* 018 */
/* 019 */   }
/* 020 */
/* 021 */   public java.lang.Object apply(java.lang.Object _i) {
/* 022 */     InternalRow i = (InternalRow) _i;
/* 023 */     org.apache.spark.sql.Row value_7 = CreateExternalRow_0(i);
/* 024 */     if (false) {
/* 025 */       mutableRow.setNullAt(0);
/* 026 */     } else {
/* 027 */
/* 028 */       mutableRow.update(0, value_7);
/* 029 */     }
/* 030 */
/* 031 */     return mutableRow;
/* 032 */   }
/* 033 */
/* 034 */
/* 035 */   private org.apache.spark.sql.Row CreateExternalRow_0(InternalRow i) {
/* 036 */     Object[] values_0 = new Object[3];
/* 037 */
/* 038 */     boolean isNull_2 = i.isNullAt(0);
/* 039 */     UTF8String value_2 = isNull_2 ?
/* 040 */     null : (i.getUTF8String(0));
/* 041 */     boolean isNull_1 = true;
/* 042 */     java.lang.String value_1 = null;
/* 043 */     if (!isNull_2) {
/* 044 */       isNull_1 = false;
/* 045 */       if (!isNull_1) {
/* 046 */
/* 047 */         Object funcResult_0 = null;
/* 048 */         funcResult_0 = value_2.toString();
/* 049 */         value_1 = (java.lang.String) funcResult_0;
/* 050 */
/* 051 */       }
/* 052 */     }
/* 053 */     if (isNull_1) {
/* 054 */       values_0[0] = null;
/* 055 */     } else {
/* 056 */       values_0[0] = value_1;
/* 057 */     }
/* 058 */
/* 059 */     boolean isNull_4 = i.isNullAt(1);
/* 060 */     UTF8String value_4 = isNull_4 ?
/* 061 */     null : (i.getUTF8String(1));
/* 062 */     boolean isNull_3 = true;
/* 063 */     java.lang.String value_3 = null;
/* 064 */     if (!isNull_4) {
/* 065 */       isNull_3 = false;
/* 066 */       if (!isNull_3) {
/* 067 */
/* 068 */         Object funcResult_1 = null;
/* 069 */         funcResult_1 = value_4.toString();
/* 070 */         value_3 = (java.lang.String) funcResult_1;
/* 071 */
/* 072 */       }
/* 073 */     }
/* 074 */     if (isNull_3) {
/* 075 */       values_0[1] = null;
/* 076 */     } else {
/* 077 */       values_0[1] = value_3;
/* 078 */     }
/* 079 */
/* 080 */     boolean isNull_6 = i.isNullAt(2);
/* 081 */     UTF8String value_6 = isNull_6 ?
/* 082 */     null : (i.getUTF8String(2));
/* 083 */     boolean isNull_5 = true;
/* 084 */     java.lang.String value_5 = null;
/* 085 */     if (!isNull_6) {
/* 086 */       isNull_5 = false;
/* 087 */       if (!isNull_5) {
/* 088 */
/* 089 */         Object funcResult_2 = null;
/* 090 */         funcResult_2 = value_6.toString();
/* 091 */         value_5 = (java.lang.String) funcResult_2;
/* 092 */
/* 093 */       }
/* 094 */     }
/* 095 */     if (isNull_5) {
/* 096 */       values_0[2] = null;
/* 097 */     } else {
/* 098 */       values_0[2] = value_5;
/* 099 */     }
/* 100 */
/* 101 */     final org.apache.spark.sql.Row value_0 = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values_0, ((org.apache.spark.sql.types.StructType) references[0] /* schema */));
/* 102 */
/* 103 */     return value_0;
/* 104 */   }
/* 105 */
/* 106 */ }

15:23:21.264 [ScalaTest-run-running-SortMergeJoinSpec] DEBUG org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator -
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificSafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private InternalRow mutableRow;
/* 009 */
/* 010 */
/* 011 */   public SpecificSafeProjection(Object[] references) {
/* 012 */     this.references = references;
/* 013 */     mutableRow = (InternalRow) references[references.length - 1];
/* 014 */
/* 015 */   }
/* 016 */
/* 017 */   public void initialize(int partitionIndex) {
/* 018 */
/* 019 */   }
/* 020 */
/* 021 */   public java.lang.Object apply(java.lang.Object _i) {
/* 022 */     InternalRow i = (InternalRow) _i;
/* 023 */     org.apache.spark.sql.Row value_7 = CreateExternalRow_0(i);
/* 024 */     if (false) {
/* 025 */       mutableRow.setNullAt(0);
/* 026 */     } else {
/* 027 */
/* 028 */       mutableRow.update(0, value_7);
/* 029 */     }
/* 030 */
/* 031 */     return mutableRow;
/* 032 */   }
/* 033 */
/* 034 */
/* 035 */   private org.apache.spark.sql.Row CreateExternalRow_0(InternalRow i) {
/* 036 */     Object[] values_0 = new Object[3];
/* 037 */
/* 038 */     boolean isNull_2 = i.isNullAt(0);
/* 039 */     UTF8String value_2 = isNull_2 ?
/* 040 */     null : (i.getUTF8String(0));
/* 041 */     boolean isNull_1 = true;
/* 042 */     java.lang.String value_1 = null;
/* 043 */     if (!isNull_2) {
/* 044 */       isNull_1 = false;
/* 045 */       if (!isNull_1) {
/* 046 */
/* 047 */         Object funcResult_0 = null;
/* 048 */         funcResult_0 = value_2.toString();
/* 049 */         value_1 = (java.lang.String) funcResult_0;
/* 050 */
/* 051 */       }
/* 052 */     }
/* 053 */     if (isNull_1) {
/* 054 */       values_0[0] = null;
/* 055 */     } else {
/* 056 */       values_0[0] = value_1;
/* 057 */     }
/* 058 */
/* 059 */     boolean isNull_4 = i.isNullAt(1);
/* 060 */     UTF8String value_4 = isNull_4 ?
/* 061 */     null : (i.getUTF8String(1));
/* 062 */     boolean isNull_3 = true;
/* 063 */     java.lang.String value_3 = null;
/* 064 */     if (!isNull_4) {
/* 065 */       isNull_3 = false;
/* 066 */       if (!isNull_3) {
/* 067 */
/* 068 */         Object funcResult_1 = null;
/* 069 */         funcResult_1 = value_4.toString();
/* 070 */         value_3 = (java.lang.String) funcResult_1;
/* 071 */
/* 072 */       }
/* 073 */     }
/* 074 */     if (isNull_3) {
/* 075 */       values_0[1] = null;
/* 076 */     } else {
/* 077 */       values_0[1] = value_3;
/* 078 */     }
/* 079 */
/* 080 */     boolean isNull_6 = i.isNullAt(2);
/* 081 */     UTF8String value_6 = isNull_6 ?
/* 082 */     null : (i.getUTF8String(2));
/* 083 */     boolean isNull_5 = true;
/* 084 */     java.lang.String value_5 = null;
/* 085 */     if (!isNull_6) {
/* 086 */       isNull_5 = false;
/* 087 */       if (!isNull_5) {
/* 088 */
/* 089 */         Object funcResult_2 = null;
/* 090 */         funcResult_2 = value_6.toString();
/* 091 */         value_5 = (java.lang.String) funcResult_2;
/* 092 */
/* 093 */       }
/* 094 */     }
/* 095 */     if (isNull_5) {
/* 096 */       values_0[2] = null;
/* 097 */     } else {
/* 098 */       values_0[2] = value_5;
/* 099 */     }
/* 100 */
/* 101 */     final org.apache.spark.sql.Row value_0 = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values_0, ((org.apache.spark.sql.types.StructType) references[0] /* schema */));
/* 102 */
/* 103 */     return value_0;
/* 104 */   }
/* 105 */
/* 106 */ }

15:23:21.272 [ScalaTest-run-running-SortMergeJoinSpec] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 9.576573 ms
+---+------+-----+
| id|  name|value|
+---+------+-----+
|  4| name4|    2|
|  6| name6|    3|
|  8| name8|    4|
| 10|name10|    5|
| 12|name12|    6|
| 14|name14|    7|
| 16|name16|    8|
| 18|name18|    9|
| 20|name20|   10|
+---+------+-----+


15:23:21.305 [ScalaTest-run] INFO  org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://192.168.31.150:4040
15:23:21.315 [dispatcher-event-loop-0] INFO  org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
15:23:21.491 [ScalaTest-run] INFO  org.apache.spark.storage.memory.MemoryStore - MemoryStore cleared
15:23:21.492 [ScalaTest-run] INFO  org.apache.spark.storage.BlockManager - BlockManager stopped
15:23:21.499 [ScalaTest-run] INFO  org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
15:23:21.501 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
15:23:21.510 [ScalaTest-run] INFO  org.apache.spark.SparkContext - Successfully stopped SparkContext

15:23:21.516 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager - Shutdown hook called
15:23:21.517 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager - Deleting directory /private/var/folders/8w/3j0ns1bd6xjf_9n5_h2dyjt80000gn/T/spark-56467b45-8fc4-480b-9d1e-f7e95fbd66b0
