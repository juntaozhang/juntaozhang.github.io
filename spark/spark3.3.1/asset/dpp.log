15:54:26.566 [ScalaTest-run-running-PhysicalPlanSpec] INFO  org.apache.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
15:54:26.568 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.internal.SharedState - Applying other initial session options to HadoopConf: spark.app.name -> LogicalPlanTest
15:54:26.568 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.internal.SharedState - Applying other initial session options to HadoopConf: spark.master -> local
15:54:26.569 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.internal.SharedState - Applying static initial session options to SparkConf: spark.sql.catalogImplementation -> hive
15:54:26.590 [ScalaTest-run-running-PhysicalPlanSpec] INFO  org.apache.spark.sql.internal.SharedState - Warehouse path is 'file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-warehouse'.
15:54:27.706 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.execution.SparkSqlParser - Parsing command: set spark.sql.codegen.wholeStage=false
org.apache.spark.sql.catalyst.parser.SqlBaseParser$SingleStatementContext =>  setspark.sql.codegen.wholeStage=false <EOF>
org.apache.spark.sql.catalyst.parser.SqlBaseParser$SingleStatementContext =>  setspark.sql.codegen.wholeStage=false <EOF>
org.apache.spark.sql.catalyst.parser.SqlBaseParser$SetConfigurationContext =>  set spark . sql . codegen . wholeStage = false
Unresolved Logical Plan ====> 
SetCommand (spark.sql.codegen.wholeStage,Some(false))

15:54:28.030 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: spark_grouping_id
org.apache.spark.sql.catalyst.parser.SqlBaseParser$SingleMultipartIdentifierContext =>  spark_grouping_id <EOF>
org.apache.spark.sql.catalyst.parser.SqlBaseParser$MultipartIdentifierContext =>  spark_grouping_id
analyzed ====> 
SetCommand (spark.sql.codegen.wholeStage,Some(false))

analyzed ====> 
SetCommand (spark.sql.codegen.wholeStage,Some(false))

optimizedPlan ====> 
SetCommand (spark.sql.codegen.wholeStage,Some(false))

SparkPlanner(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2@67b3960b) - org.apache.spark.sql.execution.SparkStrategies$SpecialLimits$ -->【
ReturnAnswer
+- SetCommand (spark.sql.codegen.wholeStage,Some(false))
】 --> 【
PlanLater SetCommand (spark.sql.codegen.wholeStage,Some(false))
】

SparkPlanner(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2@67b3960b) - org.apache.spark.sql.execution.SparkStrategies$BasicOperators$ -->【
SetCommand (spark.sql.codegen.wholeStage,Some(false))
】 --> 【
Execute SetCommand
   +- SetCommand (spark.sql.codegen.wholeStage,Some(false))
】

sparkPlan ====> 
Execute SetCommand
   +- SetCommand (spark.sql.codegen.wholeStage,Some(false))

org.apache.spark.sql.execution.QueryExecution$ prepareForExecution final result ==>【
Execute SetCommand
   +- SetCommand (spark.sql.codegen.wholeStage,Some(false))
】 ==> 【
Execute SetCommand
   +- SetCommand (spark.sql.codegen.wholeStage,Some(false))
】

executedPlan ====> 
Execute SetCommand
   +- SetCommand (spark.sql.codegen.wholeStage,Some(false))

15:54:29.893 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.execution.SparkSqlParser - Parsing command: set spark.sql.optimizer.dynamicPartitionPruning.reuseBroadcastOnly=false
org.apache.spark.sql.catalyst.parser.SqlBaseParser$SingleStatementContext =>  setspark.sql.optimizer.dynamicPartitionPruning.reuseBroadcastOnly=false <EOF>
org.apache.spark.sql.catalyst.parser.SqlBaseParser$SingleStatementContext =>  setspark.sql.optimizer.dynamicPartitionPruning.reuseBroadcastOnly=false <EOF>
org.apache.spark.sql.catalyst.parser.SqlBaseParser$SetConfigurationContext =>  set spark . sql . optimizer . dynamicPartitionPruning . reuseBroadcastOnly = false
Unresolved Logical Plan ====> 
SetCommand (spark.sql.optimizer.dynamicPartitionPruning.reuseBroadcastOnly,Some(false))

analyzed ====> 
SetCommand (spark.sql.optimizer.dynamicPartitionPruning.reuseBroadcastOnly,Some(false))

analyzed ====> 
SetCommand (spark.sql.optimizer.dynamicPartitionPruning.reuseBroadcastOnly,Some(false))

optimizedPlan ====> 
SetCommand (spark.sql.optimizer.dynamicPartitionPruning.reuseBroadcastOnly,Some(false))

SparkPlanner(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2@67b3960b) - org.apache.spark.sql.execution.SparkStrategies$SpecialLimits$ -->【
ReturnAnswer
+- SetCommand (spark.sql.optimizer.dynamicPartitionPruning.reuseBroadcastOnly,Some(false))
】 --> 【
PlanLater SetCommand (spark.sql.optimizer.dynamicPartitionPruning.reuseBroadcastOnly,Some(false))
】

SparkPlanner(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2@67b3960b) - org.apache.spark.sql.execution.SparkStrategies$BasicOperators$ -->【
SetCommand (spark.sql.optimizer.dynamicPartitionPruning.reuseBroadcastOnly,Some(false))
】 --> 【
Execute SetCommand
   +- SetCommand (spark.sql.optimizer.dynamicPartitionPruning.reuseBroadcastOnly,Some(false))
】

sparkPlan ====> 
Execute SetCommand
   +- SetCommand (spark.sql.optimizer.dynamicPartitionPruning.reuseBroadcastOnly,Some(false))

org.apache.spark.sql.execution.QueryExecution$ prepareForExecution final result ==>【
Execute SetCommand
   +- SetCommand (spark.sql.optimizer.dynamicPartitionPruning.reuseBroadcastOnly,Some(false))
】 ==> 【
Execute SetCommand
   +- SetCommand (spark.sql.optimizer.dynamicPartitionPruning.reuseBroadcastOnly,Some(false))
】

executedPlan ====> 
Execute SetCommand
   +- SetCommand (spark.sql.optimizer.dynamicPartitionPruning.reuseBroadcastOnly,Some(false))

15:54:29.906 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.execution.SparkSqlParser - Parsing command: set spark.sql.exchange.reuse=false
org.apache.spark.sql.catalyst.parser.SqlBaseParser$SingleStatementContext =>  setspark.sql.exchange.reuse=false <EOF>
org.apache.spark.sql.catalyst.parser.SqlBaseParser$SingleStatementContext =>  setspark.sql.exchange.reuse=false <EOF>
org.apache.spark.sql.catalyst.parser.SqlBaseParser$SetConfigurationContext =>  set spark . sql . exchange . reuse = false
Unresolved Logical Plan ====> 
SetCommand (spark.sql.exchange.reuse,Some(false))

analyzed ====> 
SetCommand (spark.sql.exchange.reuse,Some(false))

analyzed ====> 
SetCommand (spark.sql.exchange.reuse,Some(false))

optimizedPlan ====> 
SetCommand (spark.sql.exchange.reuse,Some(false))

SparkPlanner(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2@67b3960b) - org.apache.spark.sql.execution.SparkStrategies$SpecialLimits$ -->【
ReturnAnswer
+- SetCommand (spark.sql.exchange.reuse,Some(false))
】 --> 【
PlanLater SetCommand (spark.sql.exchange.reuse,Some(false))
】

SparkPlanner(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2@67b3960b) - org.apache.spark.sql.execution.SparkStrategies$BasicOperators$ -->【
SetCommand (spark.sql.exchange.reuse,Some(false))
】 --> 【
Execute SetCommand
   +- SetCommand (spark.sql.exchange.reuse,Some(false))
】

sparkPlan ====> 
Execute SetCommand
   +- SetCommand (spark.sql.exchange.reuse,Some(false))

org.apache.spark.sql.execution.QueryExecution$ prepareForExecution final result ==>【
Execute SetCommand
   +- SetCommand (spark.sql.exchange.reuse,Some(false))
】 ==> 【
Execute SetCommand
   +- SetCommand (spark.sql.exchange.reuse,Some(false))
】

executedPlan ====> 
Execute SetCommand
   +- SetCommand (spark.sql.exchange.reuse,Some(false))

15:54:29.916 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.execution.SparkSqlParser - Parsing command: 
      SELECT f.id, f.value, d.event
      FROM fact_table f
      JOIN dim_table d
      ON f.date = d.date
      WHERE d.event = 'New Year'
    
org.apache.spark.sql.catalyst.parser.SqlBaseParser$SingleStatementContext =>  SELECTf.id,f.value,d.eventFROMfact_tablefJOINdim_tabledONf.date=d.dateWHEREd.event='New Year' <EOF>
org.apache.spark.sql.catalyst.parser.SqlBaseParser$SingleStatementContext =>  SELECTf.id,f.value,d.eventFROMfact_tablefJOINdim_tabledONf.date=d.dateWHEREd.event='New Year' <EOF>
org.apache.spark.sql.catalyst.parser.SqlBaseParser$QueryContext =>  SELECTf.id,f.value,d.eventFROMfact_tablefJOINdim_tabledONf.date=d.dateWHEREd.event='New Year' 
org.apache.spark.sql.catalyst.parser.SqlBaseParser$RegularQuerySpecificationContext =>  SELECTf.id,f.value,d.event FROMfact_tablefJOINdim_tabledONf.date=d.date WHEREd.event='New Year'
org.apache.spark.sql.catalyst.parser.SqlBaseParser$FromClauseContext =>  FROM fact_tablefJOINdim_tabledONf.date=d.date
org.apache.spark.sql.catalyst.parser.SqlBaseParser$TableNameContext =>  fact_table f
org.apache.spark.sql.catalyst.parser.SqlBaseParser$MultipartIdentifierContext =>  fact_table
org.apache.spark.sql.catalyst.parser.SqlBaseParser$JoinRelationContext =>   JOIN dim_tabled ONf.date=d.date
org.apache.spark.sql.catalyst.parser.SqlBaseParser$PredicatedContext =>  f.date=d.date
org.apache.spark.sql.catalyst.parser.SqlBaseParser$ComparisonContext =>  f.date = d.date
org.apache.spark.sql.catalyst.parser.SqlBaseParser$DereferenceContext =>  f . date
org.apache.spark.sql.catalyst.parser.SqlBaseParser$ColumnReferenceContext =>  f
org.apache.spark.sql.catalyst.parser.SqlBaseParser$DereferenceContext =>  d . date
org.apache.spark.sql.catalyst.parser.SqlBaseParser$ColumnReferenceContext =>  d
org.apache.spark.sql.catalyst.parser.SqlBaseParser$TableNameContext =>  dim_table d
org.apache.spark.sql.catalyst.parser.SqlBaseParser$MultipartIdentifierContext =>  dim_table
org.apache.spark.sql.catalyst.parser.SqlBaseParser$RegularQuerySpecificationContext =>  SELECTf.id,f.value,d.event FROMfact_tablefJOINdim_tabledONf.date=d.date WHEREd.event='New Year'
org.apache.spark.sql.catalyst.parser.SqlBaseParser$NamedExpressionContext =>  f.id
org.apache.spark.sql.catalyst.parser.SqlBaseParser$PredicatedContext =>  f.id
org.apache.spark.sql.catalyst.parser.SqlBaseParser$DereferenceContext =>  f . id
org.apache.spark.sql.catalyst.parser.SqlBaseParser$ColumnReferenceContext =>  f
org.apache.spark.sql.catalyst.parser.SqlBaseParser$NamedExpressionContext =>  f.value
org.apache.spark.sql.catalyst.parser.SqlBaseParser$PredicatedContext =>  f.value
org.apache.spark.sql.catalyst.parser.SqlBaseParser$DereferenceContext =>  f . value
org.apache.spark.sql.catalyst.parser.SqlBaseParser$ColumnReferenceContext =>  f
org.apache.spark.sql.catalyst.parser.SqlBaseParser$NamedExpressionContext =>  d.event
org.apache.spark.sql.catalyst.parser.SqlBaseParser$PredicatedContext =>  d.event
org.apache.spark.sql.catalyst.parser.SqlBaseParser$DereferenceContext =>  d . event
org.apache.spark.sql.catalyst.parser.SqlBaseParser$ColumnReferenceContext =>  d
org.apache.spark.sql.catalyst.parser.SqlBaseParser$PredicatedContext =>  d.event='New Year'
org.apache.spark.sql.catalyst.parser.SqlBaseParser$ComparisonContext =>  d.event = 'New Year'
org.apache.spark.sql.catalyst.parser.SqlBaseParser$DereferenceContext =>  d . event
org.apache.spark.sql.catalyst.parser.SqlBaseParser$ColumnReferenceContext =>  d
org.apache.spark.sql.catalyst.parser.SqlBaseParser$StringLiteralContext =>  'New Year'
org.apache.spark.sql.catalyst.parser.SqlBaseParser$QueryOrganizationContext => 
Unresolved Logical Plan ====> 
'Project ['f.id, 'f.value, 'd.event]
+- 'Filter ('d.event = New Year)
   +- 'Join Inner, ('f.date = 'd.date)
      :- 'SubqueryAlias f
      :  +- 'UnresolvedRelation [fact_table], [], false
      +- 'SubqueryAlias d
         +- 'UnresolvedRelation [dim_table], [], false

15:54:30.061 [ScalaTest-run-running-PhysicalPlanSpec] INFO  org.apache.spark.sql.hive.HiveUtils - Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.
15:54:30.519 [ScalaTest-run-running-PhysicalPlanSpec] INFO  org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 2.3.9) is file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-warehouse
15:54:30.679 [ScalaTest-run-running-PhysicalPlanSpec] WARN  org.apache.hadoop.hive.conf.HiveConf - HiveConf of name hive.stats.jdbc.timeout does not exist
15:54:30.680 [ScalaTest-run-running-PhysicalPlanSpec] WARN  org.apache.hadoop.hive.conf.HiveConf - HiveConf of name hive.stats.retries.wait does not exist
15:54:31.267 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG com.jolbox.bonecp.BoneCPDataSource - JDBC URL = jdbc:derby:;databaseName=metastore_db;create=true, Username = APP, partitions = 1, max (per partition) = 10, min (per partition) = 0, idle max age = 60 min, idle test period = 240 min, strategy = DEFAULT
15:54:32.708 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG com.jolbox.bonecp.BoneCPDataSource - JDBC URL = jdbc:derby:;databaseName=metastore_db;create=true, Username = APP, partitions = 1, max (per partition) = 10, min (per partition) = 0, idle max age = 60 min, idle test period = 240 min, strategy = DEFAULT
15:54:33.089 [ScalaTest-run-running-PhysicalPlanSpec] WARN  org.apache.hadoop.hive.metastore.ObjectStore - Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 2.3.0
15:54:33.089 [ScalaTest-run-running-PhysicalPlanSpec] WARN  org.apache.hadoop.hive.metastore.ObjectStore - setMetaStoreSchemaVersion called but recording version is disabled: version = 2.3.0, comment = Set by MetaStore juntao@192.168.31.69
15:54:33.522 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: int
org.apache.spark.sql.catalyst.parser.SqlBaseParser$SingleDataTypeContext =>  int <EOF>
org.apache.spark.sql.catalyst.parser.SqlBaseParser$PrimitiveDataTypeContext =>  int
15:54:33.525 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: int
org.apache.spark.sql.catalyst.parser.SqlBaseParser$SingleDataTypeContext =>  int <EOF>
org.apache.spark.sql.catalyst.parser.SqlBaseParser$PrimitiveDataTypeContext =>  int
15:54:33.526 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
org.apache.spark.sql.catalyst.parser.SqlBaseParser$SingleDataTypeContext =>  string <EOF>
org.apache.spark.sql.catalyst.parser.SqlBaseParser$PrimitiveDataTypeContext =>  string
15:54:33.544 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: array<string>
org.apache.spark.sql.catalyst.parser.SqlBaseParser$SingleDataTypeContext =>  array<string> <EOF>
org.apache.spark.sql.catalyst.parser.SqlBaseParser$ComplexDataTypeContext =>  array < string >
org.apache.spark.sql.catalyst.parser.SqlBaseParser$PrimitiveDataTypeContext =>  string
15:54:33.633 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
org.apache.spark.sql.catalyst.parser.SqlBaseParser$SingleDataTypeContext =>  string <EOF>
org.apache.spark.sql.catalyst.parser.SqlBaseParser$PrimitiveDataTypeContext =>  string
15:54:33.634 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
org.apache.spark.sql.catalyst.parser.SqlBaseParser$SingleDataTypeContext =>  string <EOF>
org.apache.spark.sql.catalyst.parser.SqlBaseParser$PrimitiveDataTypeContext =>  string
Analyzer(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$1@2527765a) org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations --> old【
'Project ['f.id, 'f.value, 'd.event]
+- 'Filter ('d.event = New Year)
   +- 'Join Inner, ('f.date = 'd.date)
      :- 'SubqueryAlias f
      :  +- 'UnresolvedRelation [fact_table], [], false
      +- 'SubqueryAlias d
         +- 'UnresolvedRelation [dim_table], [], false
】--> new【
'Project ['f.id, 'f.value, 'd.event]
+- 'Filter ('d.event = New Year)
   +- 'Join Inner, ('f.date = 'd.date)
      :- 'SubqueryAlias f
      :  +- 'SubqueryAlias spark_catalog.default.fact_table
      :     +- 'UnresolvedCatalogRelation `default`.`fact_table`, org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, [], false
      +- 'SubqueryAlias d
         +- 'SubqueryAlias spark_catalog.default.dim_table
            +- 'UnresolvedCatalogRelation `default`.`dim_table`, org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, [], false
】


15:54:33.909 [ScalaTest-run-running-PhysicalPlanSpec] INFO  org.apache.spark.sql.execution.datasources.InMemoryFileIndex - It took 45 ms to list leaf files for 1 paths.
Analyzer(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$1@2527765a) org.apache.spark.sql.execution.datasources.FindDataSourceTable --> old【
'Project ['f.id, 'f.value, 'd.event]
+- 'Filter ('d.event = New Year)
   +- 'Join Inner, ('f.date = 'd.date)
      :- 'SubqueryAlias f
      :  +- 'SubqueryAlias spark_catalog.default.fact_table
      :     +- 'UnresolvedCatalogRelation `default`.`fact_table`, org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, [], false
      +- 'SubqueryAlias d
         +- 'SubqueryAlias spark_catalog.default.dim_table
            +- 'UnresolvedCatalogRelation `default`.`dim_table`, org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, [], false
】--> new【
'Project ['f.id, 'f.value, 'd.event]
+- 'Filter ('d.event = New Year)
   +- 'Join Inner, ('f.date = 'd.date)
      :- SubqueryAlias f
      :  +- SubqueryAlias spark_catalog.default.fact_table
      :     +- Relation default.fact_table[id#30,value#31,date#32] parquet
      +- SubqueryAlias d
         +- SubqueryAlias spark_catalog.default.dim_table
            +- Relation default.dim_table[date#33,event#34] parquet
】


Analyzer(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$1@2527765a) org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences --> old【
'Project ['f.id, 'f.value, 'd.event]
+- 'Filter ('d.event = New Year)
   +- 'Join Inner, ('f.date = 'd.date)
      :- SubqueryAlias f
      :  +- SubqueryAlias spark_catalog.default.fact_table
      :     +- Relation default.fact_table[id#30,value#31,date#32] parquet
      +- SubqueryAlias d
         +- SubqueryAlias spark_catalog.default.dim_table
            +- Relation default.dim_table[date#33,event#34] parquet
】--> new【
Project [id#30, value#31, event#34]
+- Filter (event#34 = New Year)
   +- Join Inner, (date#32 = date#33)
      :- SubqueryAlias f
      :  +- SubqueryAlias spark_catalog.default.fact_table
      :     +- Relation default.fact_table[id#30,value#31,date#32] parquet
      +- SubqueryAlias d
         +- SubqueryAlias spark_catalog.default.dim_table
            +- Relation default.dim_table[date#33,event#34] parquet
】


RuleExecutor.execute[Analyzer(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$1@2527765a)] final result ==> old【
'Project ['f.id, 'f.value, 'd.event]
+- 'Filter ('d.event = New Year)
   +- 'Join Inner, ('f.date = 'd.date)
      :- 'SubqueryAlias f
      :  +- 'UnresolvedRelation [fact_table], [], false
      +- 'SubqueryAlias d
         +- 'UnresolvedRelation [dim_table], [], false
】==> new【
Project [id#30, value#31, event#34]
+- Filter (event#34 = New Year)
   +- Join Inner, (date#32 = date#33)
      :- SubqueryAlias f
      :  +- SubqueryAlias spark_catalog.default.fact_table
      :     +- Relation default.fact_table[id#30,value#31,date#32] parquet
      +- SubqueryAlias d
         +- SubqueryAlias spark_catalog.default.dim_table
            +- Relation default.dim_table[date#33,event#34] parquet
】


analyzed ====> 
Project [id#30, value#31, event#34]
+- Filter (event#34 = New Year)
   +- Join Inner, (date#32 = date#33)
      :- SubqueryAlias f
      :  +- SubqueryAlias spark_catalog.default.fact_table
      :     +- Relation default.fact_table[id#30,value#31,date#32] parquet
      +- SubqueryAlias d
         +- SubqueryAlias spark_catalog.default.dim_table
            +- Relation default.dim_table[date#33,event#34] parquet

Analyzer(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$1@2527765a) org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveAliases --> old【
'Project [unresolvedalias(cast(id#30 as string), None), unresolvedalias(cast(value#31 as string), None), unresolvedalias(cast(event#34 as string), None)]
+- Project [id#30, value#31, event#34]
   +- Filter (event#34 = New Year)
      +- Join Inner, (date#32 = date#33)
         :- SubqueryAlias f
         :  +- SubqueryAlias spark_catalog.default.fact_table
         :     +- Relation default.fact_table[id#30,value#31,date#32] parquet
         +- SubqueryAlias d
            +- SubqueryAlias spark_catalog.default.dim_table
               +- Relation default.dim_table[date#33,event#34] parquet
】--> new【
Project [cast(id#30 as string) AS id#43, cast(value#31 as string) AS value#44, cast(event#34 as string) AS event#45]
+- Project [id#30, value#31, event#34]
   +- Filter (event#34 = New Year)
      +- Join Inner, (date#32 = date#33)
         :- SubqueryAlias f
         :  +- SubqueryAlias spark_catalog.default.fact_table
         :     +- Relation default.fact_table[id#30,value#31,date#32] parquet
         +- SubqueryAlias d
            +- SubqueryAlias spark_catalog.default.dim_table
               +- Relation default.dim_table[date#33,event#34] parquet
】


Analyzer(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$1@2527765a) org.apache.spark.sql.catalyst.analysis.ResolveTimeZone --> old【
Project [cast(id#30 as string) AS id#43, cast(value#31 as string) AS value#44, cast(event#34 as string) AS event#45]
+- Project [id#30, value#31, event#34]
   +- Filter (event#34 = New Year)
      +- Join Inner, (date#32 = date#33)
         :- SubqueryAlias f
         :  +- SubqueryAlias spark_catalog.default.fact_table
         :     +- Relation default.fact_table[id#30,value#31,date#32] parquet
         +- SubqueryAlias d
            +- SubqueryAlias spark_catalog.default.dim_table
               +- Relation default.dim_table[date#33,event#34] parquet
】--> new【
Project [cast(id#30 as string) AS id#43, cast(value#31 as string) AS value#44, cast(event#34 as string) AS event#45]
+- Project [id#30, value#31, event#34]
   +- Filter (event#34 = New Year)
      +- Join Inner, (date#32 = date#33)
         :- SubqueryAlias f
         :  +- SubqueryAlias spark_catalog.default.fact_table
         :     +- Relation default.fact_table[id#30,value#31,date#32] parquet
         +- SubqueryAlias d
            +- SubqueryAlias spark_catalog.default.dim_table
               +- Relation default.dim_table[date#33,event#34] parquet
】


RuleExecutor.execute[Analyzer(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$1@2527765a)] final result ==> old【
'Project [unresolvedalias(cast(id#30 as string), None), unresolvedalias(cast(value#31 as string), None), unresolvedalias(cast(event#34 as string), None)]
+- Project [id#30, value#31, event#34]
   +- Filter (event#34 = New Year)
      +- Join Inner, (date#32 = date#33)
         :- SubqueryAlias f
         :  +- SubqueryAlias spark_catalog.default.fact_table
         :     +- Relation default.fact_table[id#30,value#31,date#32] parquet
         +- SubqueryAlias d
            +- SubqueryAlias spark_catalog.default.dim_table
               +- Relation default.dim_table[date#33,event#34] parquet
】==> new【
Project [cast(id#30 as string) AS id#43, cast(value#31 as string) AS value#44, cast(event#34 as string) AS event#45]
+- Project [id#30, value#31, event#34]
   +- Filter (event#34 = New Year)
      +- Join Inner, (date#32 = date#33)
         :- SubqueryAlias f
         :  +- SubqueryAlias spark_catalog.default.fact_table
         :     +- Relation default.fact_table[id#30,value#31,date#32] parquet
         +- SubqueryAlias d
            +- SubqueryAlias spark_catalog.default.dim_table
               +- Relation default.dim_table[date#33,event#34] parquet
】


analyzed ====> 
Project [cast(id#30 as string) AS id#43, cast(value#31 as string) AS value#44, cast(event#34 as string) AS event#45]
+- Project [id#30, value#31, event#34]
   +- Filter (event#34 = New Year)
      +- Join Inner, (date#32 = date#33)
         :- SubqueryAlias f
         :  +- SubqueryAlias spark_catalog.default.fact_table
         :     +- Relation default.fact_table[id#30,value#31,date#32] parquet
         +- SubqueryAlias d
            +- SubqueryAlias spark_catalog.default.dim_table
               +- Relation default.dim_table[date#33,event#34] parquet

analyzed ====> 
GlobalLimit 21
+- LocalLimit 21
   +- Project [cast(id#30 as string) AS id#43, cast(value#31 as string) AS value#44, cast(event#34 as string) AS event#45]
      +- Project [id#30, value#31, event#34]
         +- Filter (event#34 = New Year)
            +- Join Inner, (date#32 = date#33)
               :- SubqueryAlias f
               :  +- SubqueryAlias spark_catalog.default.fact_table
               :     +- Relation default.fact_table[id#30,value#31,date#32] parquet
               +- SubqueryAlias d
                  +- SubqueryAlias spark_catalog.default.dim_table
                     +- Relation default.dim_table[date#33,event#34] parquet

SparkOptimizer(org.apache.spark.sql.internal.BaseSessionStateBuilder$$anon$2@2fb171f5) org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis --> old【
GlobalLimit 21
+- LocalLimit 21
   +- Project [cast(id#30 as string) AS id#43, cast(value#31 as string) AS value#44, cast(event#34 as string) AS event#45]
      +- Project [id#30, value#31, event#34]
         +- Filter (event#34 = New Year)
            +- Join Inner, (date#32 = date#33)
               :- SubqueryAlias f
               :  +- SubqueryAlias spark_catalog.default.fact_table
               :     +- Relation default.fact_table[id#30,value#31,date#32] parquet
               +- SubqueryAlias d
                  +- SubqueryAlias spark_catalog.default.dim_table
                     +- Relation default.dim_table[date#33,event#34] parquet
】--> new【
GlobalLimit 21
+- LocalLimit 21
   +- Project [cast(id#30 as string) AS id#43, cast(value#31 as string) AS value#44, cast(event#34 as string) AS event#45]
      +- Project [id#30, value#31, event#34]
         +- Filter (event#34 = New Year)
            +- Join Inner, (date#32 = date#33)
               :- Relation default.fact_table[id#30,value#31,date#32] parquet
               +- Relation default.dim_table[date#33,event#34] parquet
】


SparkOptimizer(org.apache.spark.sql.internal.BaseSessionStateBuilder$$anon$2@2fb171f5) org.apache.spark.sql.catalyst.optimizer.PushDownPredicates --> old【
GlobalLimit 21
+- LocalLimit 21
   +- Project [cast(id#30 as string) AS id#43, cast(value#31 as string) AS value#44, cast(event#34 as string) AS event#45]
      +- Project [id#30, value#31, event#34]
         +- Filter (event#34 = New Year)
            +- Join Inner, (date#32 = date#33)
               :- Relation default.fact_table[id#30,value#31,date#32] parquet
               +- Relation default.dim_table[date#33,event#34] parquet
】--> new【
GlobalLimit 21
+- LocalLimit 21
   +- Project [cast(id#30 as string) AS id#43, cast(value#31 as string) AS value#44, cast(event#34 as string) AS event#45]
      +- Project [id#30, value#31, event#34]
         +- Join Inner, (date#32 = date#33)
            :- Relation default.fact_table[id#30,value#31,date#32] parquet
            +- Filter (event#34 = New Year)
               +- Relation default.dim_table[date#33,event#34] parquet
】


SparkOptimizer(org.apache.spark.sql.internal.BaseSessionStateBuilder$$anon$2@2fb171f5) org.apache.spark.sql.catalyst.optimizer.CollapseProject --> old【
GlobalLimit 21
+- LocalLimit 21
   +- Project [cast(id#30 as string) AS id#43, cast(value#31 as string) AS value#44, cast(event#34 as string) AS event#45]
      +- Project [id#30, value#31, event#34]
         +- Join Inner, (date#32 = date#33)
            :- Relation default.fact_table[id#30,value#31,date#32] parquet
            +- Filter (event#34 = New Year)
               +- Relation default.dim_table[date#33,event#34] parquet
】--> new【
GlobalLimit 21
+- LocalLimit 21
   +- Project [cast(id#30 as string) AS id#43, cast(value#31 as string) AS value#44, cast(event#34 as string) AS event#45]
      +- Join Inner, (date#32 = date#33)
         :- Relation default.fact_table[id#30,value#31,date#32] parquet
         +- Filter (event#34 = New Year)
            +- Relation default.dim_table[date#33,event#34] parquet
】


SparkOptimizer(org.apache.spark.sql.internal.BaseSessionStateBuilder$$anon$2@2fb171f5) org.apache.spark.sql.catalyst.optimizer.SimplifyCasts --> old【
GlobalLimit 21
+- LocalLimit 21
   +- Project [cast(id#30 as string) AS id#43, cast(value#31 as string) AS value#44, cast(event#34 as string) AS event#45]
      +- Join Inner, (date#32 = date#33)
         :- Relation default.fact_table[id#30,value#31,date#32] parquet
         +- Filter (event#34 = New Year)
            +- Relation default.dim_table[date#33,event#34] parquet
】--> new【
GlobalLimit 21
+- LocalLimit 21
   +- Project [cast(id#30 as string) AS id#43, cast(value#31 as string) AS value#44, event#34 AS event#45]
      +- Join Inner, (date#32 = date#33)
         :- Relation default.fact_table[id#30,value#31,date#32] parquet
         +- Filter (event#34 = New Year)
            +- Relation default.dim_table[date#33,event#34] parquet
】


SparkOptimizer(org.apache.spark.sql.internal.BaseSessionStateBuilder$$anon$2@2fb171f5) org.apache.spark.sql.catalyst.optimizer.RemoveRedundantAliases --> old【
GlobalLimit 21
+- LocalLimit 21
   +- Project [cast(id#30 as string) AS id#43, cast(value#31 as string) AS value#44, event#34 AS event#45]
      +- Join Inner, (date#32 = date#33)
         :- Relation default.fact_table[id#30,value#31,date#32] parquet
         +- Filter (event#34 = New Year)
            +- Relation default.dim_table[date#33,event#34] parquet
】--> new【
GlobalLimit 21
+- LocalLimit 21
   +- Project [cast(id#30 as string) AS id#43, cast(value#31 as string) AS value#44, event#34]
      +- Join Inner, (date#32 = date#33)
         :- Relation default.fact_table[id#30,value#31,date#32] parquet
         +- Filter (event#34 = New Year)
            +- Relation default.dim_table[date#33,event#34] parquet
】


SparkOptimizer(org.apache.spark.sql.internal.BaseSessionStateBuilder$$anon$2@2fb171f5) org.apache.spark.sql.catalyst.optimizer.InferFiltersFromConstraints --> old【
GlobalLimit 21
+- LocalLimit 21
   +- Project [cast(id#30 as string) AS id#43, cast(value#31 as string) AS value#44, event#34]
      +- Join Inner, (date#32 = date#33)
         :- Relation default.fact_table[id#30,value#31,date#32] parquet
         +- Filter (event#34 = New Year)
            +- Relation default.dim_table[date#33,event#34] parquet
】--> new【
GlobalLimit 21
+- LocalLimit 21
   +- Project [cast(id#30 as string) AS id#43, cast(value#31 as string) AS value#44, event#34]
      +- Join Inner, (date#32 = date#33)
         :- Filter isnotnull(date#32)
         :  +- Relation default.fact_table[id#30,value#31,date#32] parquet
         +- Filter isnotnull(date#33)
            +- Filter (isnotnull(event#34) AND (event#34 = New Year))
               +- Relation default.dim_table[date#33,event#34] parquet
】


SparkOptimizer(org.apache.spark.sql.internal.BaseSessionStateBuilder$$anon$2@2fb171f5) org.apache.spark.sql.catalyst.optimizer.PushDownPredicates --> old【
GlobalLimit 21
+- LocalLimit 21
   +- Project [cast(id#30 as string) AS id#43, cast(value#31 as string) AS value#44, event#34]
      +- Join Inner, (date#32 = date#33)
         :- Filter isnotnull(date#32)
         :  +- Relation default.fact_table[id#30,value#31,date#32] parquet
         +- Filter isnotnull(date#33)
            +- Filter (isnotnull(event#34) AND (event#34 = New Year))
               +- Relation default.dim_table[date#33,event#34] parquet
】--> new【
GlobalLimit 21
+- LocalLimit 21
   +- Project [cast(id#30 as string) AS id#43, cast(value#31 as string) AS value#44, event#34]
      +- Join Inner, (date#32 = date#33)
         :- Filter isnotnull(date#32)
         :  +- Relation default.fact_table[id#30,value#31,date#32] parquet
         +- Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
            +- Relation default.dim_table[date#33,event#34] parquet
】


15:54:34.286 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: int
org.apache.spark.sql.catalyst.parser.SqlBaseParser$SingleDataTypeContext =>  int <EOF>
org.apache.spark.sql.catalyst.parser.SqlBaseParser$PrimitiveDataTypeContext =>  int
15:54:34.287 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: int
org.apache.spark.sql.catalyst.parser.SqlBaseParser$SingleDataTypeContext =>  int <EOF>
org.apache.spark.sql.catalyst.parser.SqlBaseParser$PrimitiveDataTypeContext =>  int
15:54:34.287 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
org.apache.spark.sql.catalyst.parser.SqlBaseParser$SingleDataTypeContext =>  string <EOF>
org.apache.spark.sql.catalyst.parser.SqlBaseParser$PrimitiveDataTypeContext =>  string
15:54:34.428 [ScalaTest-run-running-PhysicalPlanSpec] INFO  org.apache.spark.sql.execution.datasources.InMemoryFileIndex - It took 6 ms to list leaf files for 3 paths.
SparkOptimizer(org.apache.spark.sql.internal.BaseSessionStateBuilder$$anon$2@2fb171f5) org.apache.spark.sql.execution.datasources.PruneFileSourcePartitions --> old【
GlobalLimit 21
+- LocalLimit 21
   +- Project [cast(id#30 as string) AS id#43, cast(value#31 as string) AS value#44, event#34]
      +- Join Inner, (date#32 = date#33)
         :- Filter isnotnull(date#32)
         :  +- Relation default.fact_table[id#30,value#31,date#32] parquet
         +- Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
            +- Relation default.dim_table[date#33,event#34] parquet
】--> new【
GlobalLimit 21
+- LocalLimit 21
   +- Project [cast(id#30 as string) AS id#43, cast(value#31 as string) AS value#44, event#34]
      +- Join Inner, (date#32 = date#33)
         :- Project [id#30, value#31, date#32]
         :  +- Filter isnotnull(date#32)
         :     +- Relation default.fact_table[id#30,value#31,date#32] parquet
         +- Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
            +- Relation default.dim_table[date#33,event#34] parquet
】


SparkOptimizer(org.apache.spark.sql.internal.BaseSessionStateBuilder$$anon$2@2fb171f5) org.apache.spark.sql.catalyst.optimizer.RemoveNoopOperators --> old【
GlobalLimit 21
+- LocalLimit 21
   +- Project [cast(id#30 as string) AS id#43, cast(value#31 as string) AS value#44, event#34]
      +- Join Inner, (date#32 = date#33)
         :- Project [id#30, value#31, date#32]
         :  +- Filter isnotnull(date#32)
         :     +- Relation default.fact_table[id#30,value#31,date#32] parquet
         +- Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
            +- Relation default.dim_table[date#33,event#34] parquet
】--> new【
GlobalLimit 21
+- LocalLimit 21
   +- Project [cast(id#30 as string) AS id#43, cast(value#31 as string) AS value#44, event#34]
      +- Join Inner, (date#32 = date#33)
         :- Filter isnotnull(date#32)
         :  +- Relation default.fact_table[id#30,value#31,date#32] parquet
         +- Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
            +- Relation default.dim_table[date#33,event#34] parquet
】


15:54:34.451 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - Considering join on: Some((date#32 = date#33))
15:54:34.453 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - leftKeys:List(date#32) | rightKeys:List(date#33)
15:54:34.453 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - Considering join on: Some((date#32 = date#33))
15:54:34.454 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - leftKeys:List(date#32) | rightKeys:List(date#33)
SparkOptimizer(org.apache.spark.sql.internal.BaseSessionStateBuilder$$anon$2@2fb171f5) org.apache.spark.sql.execution.dynamicpruning.PartitionPruning --> old【
GlobalLimit 21
+- LocalLimit 21
   +- Project [cast(id#30 as string) AS id#43, cast(value#31 as string) AS value#44, event#34]
      +- Join Inner, (date#32 = date#33)
         :- Filter isnotnull(date#32)
         :  +- Relation default.fact_table[id#30,value#31,date#32] parquet
         +- Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
            +- Relation default.dim_table[date#33,event#34] parquet
】--> new【
GlobalLimit 21
+- LocalLimit 21
   +- Project [cast(id#30 as string) AS id#43, cast(value#31 as string) AS value#44, event#34]
      +- Join Inner, (date#32 = date#33)
         :- Filter dynamicpruning#49 [date#32]
         :  :  +- Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
         :  :     +- Relation default.dim_table[date#33,event#34] parquet
         :  +- Filter isnotnull(date#32)
         :     +- Relation default.fact_table[id#30,value#31,date#32] parquet
         +- Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
            +- Relation default.dim_table[date#33,event#34] parquet
】


SparkOptimizer(org.apache.spark.sql.internal.BaseSessionStateBuilder$$anon$2@2fb171f5) org.apache.spark.sql.catalyst.optimizer.PushDownPredicates --> old【
GlobalLimit 21
+- LocalLimit 21
   +- Project [cast(id#30 as string) AS id#43, cast(value#31 as string) AS value#44, event#34]
      +- Join Inner, (date#32 = date#33)
         :- Filter dynamicpruning#49 [date#32]
         :  :  +- Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
         :  :     +- Relation default.dim_table[date#33,event#34] parquet
         :  +- Filter isnotnull(date#32)
         :     +- Relation default.fact_table[id#30,value#31,date#32] parquet
         +- Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
            +- Relation default.dim_table[date#33,event#34] parquet
】--> new【
GlobalLimit 21
+- LocalLimit 21
   +- Project [cast(id#30 as string) AS id#43, cast(value#31 as string) AS value#44, event#34]
      +- Join Inner, (date#32 = date#33)
         :- Filter (isnotnull(date#32) AND dynamicpruning#49 [date#32])
         :  :  +- Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
         :  :     +- Relation default.dim_table[date#33,event#34] parquet
         :  +- Relation default.fact_table[id#30,value#31,date#32] parquet
         +- Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
            +- Relation default.dim_table[date#33,event#34] parquet
】


RuleExecutor.execute[SparkOptimizer(org.apache.spark.sql.internal.BaseSessionStateBuilder$$anon$2@2fb171f5)] final result ==> old【
GlobalLimit 21
+- LocalLimit 21
   +- Project [cast(id#30 as string) AS id#43, cast(value#31 as string) AS value#44, cast(event#34 as string) AS event#45]
      +- Project [id#30, value#31, event#34]
         +- Filter (event#34 = New Year)
            +- Join Inner, (date#32 = date#33)
               :- SubqueryAlias f
               :  +- SubqueryAlias spark_catalog.default.fact_table
               :     +- Relation default.fact_table[id#30,value#31,date#32] parquet
               +- SubqueryAlias d
                  +- SubqueryAlias spark_catalog.default.dim_table
                     +- Relation default.dim_table[date#33,event#34] parquet
】==> new【
GlobalLimit 21
+- LocalLimit 21
   +- Project [cast(id#30 as string) AS id#43, cast(value#31 as string) AS value#44, event#34]
      +- Join Inner, (date#32 = date#33)
         :- Filter (isnotnull(date#32) AND dynamicpruning#49 [date#32])
         :  :  +- Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
         :  :     +- Relation default.dim_table[date#33,event#34] parquet
         :  +- Relation default.fact_table[id#30,value#31,date#32] parquet
         +- Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
            +- Relation default.dim_table[date#33,event#34] parquet
】


optimizedPlan ====> 
GlobalLimit 21
+- LocalLimit 21
   +- Project [cast(id#30 as string) AS id#43, cast(value#31 as string) AS value#44, event#34]
      +- Join Inner, (date#32 = date#33)
         :- Filter (isnotnull(date#32) AND dynamicpruning#49 [date#32])
         :  :  +- Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
         :  :     +- Relation default.dim_table[date#33,event#34] parquet
         :  +- Relation default.fact_table[id#30,value#31,date#32] parquet
         +- Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
            +- Relation default.dim_table[date#33,event#34] parquet

SparkPlanner(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2@67b3960b) - org.apache.spark.sql.execution.SparkStrategies$SpecialLimits$ -->【
ReturnAnswer
+- GlobalLimit 21
   +- LocalLimit 21
      +- Project [cast(id#30 as string) AS id#43, cast(value#31 as string) AS value#44, event#34]
         +- Join Inner, (date#32 = date#33)
            :- Filter (isnotnull(date#32) AND dynamicpruning#49 [date#32])
            :  :  +- Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
            :  :     +- Relation default.dim_table[date#33,event#34] parquet
            :  +- Relation default.fact_table[id#30,value#31,date#32] parquet
            +- Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
               +- Relation default.dim_table[date#33,event#34] parquet
】 --> 【
CollectLimit 21
+- PlanLater Project [cast(id#30 as string) AS id#43, cast(value#31 as string) AS value#44, event#34]
】

SparkPlanner(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2@67b3960b) - org.apache.spark.sql.execution.SparkStrategies$BasicOperators$ -->【
Project [cast(id#30 as string) AS id#43, cast(value#31 as string) AS value#44, event#34]
+- Join Inner, (date#32 = date#33)
   :- Filter (isnotnull(date#32) AND dynamicpruning#49 [date#32])
   :  :  +- Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
   :  :     +- Relation default.dim_table[date#33,event#34] parquet
   :  +- Relation default.fact_table[id#30,value#31,date#32] parquet
   +- Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
      +- Relation default.dim_table[date#33,event#34] parquet
】 --> 【
Project [cast(id#30 as string) AS id#43, cast(value#31 as string) AS value#44, event#34]
+- PlanLater Join Inner, (date#32 = date#33)
】

15:54:34.514 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - Considering join on: Some((date#32 = date#33))
15:54:34.514 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - leftKeys:List(date#32) | rightKeys:List(date#33)
15:54:34.516 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - Considering join on: Some((date#32 = date#33))
15:54:34.517 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - leftKeys:List(date#32) | rightKeys:List(date#33)
SparkPlanner(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2@67b3960b) - org.apache.spark.sql.execution.SparkStrategies$JoinSelection$ -->【
Join Inner, (date#32 = date#33)
:- Filter (isnotnull(date#32) AND dynamicpruning#49 [date#32])
:  :  +- Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
:  :     +- Relation default.dim_table[date#33,event#34] parquet
:  +- Relation default.fact_table[id#30,value#31,date#32] parquet
+- Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
   +- Relation default.dim_table[date#33,event#34] parquet
】 --> 【
BroadcastHashJoin [date#32], [date#33], Inner, BuildRight, false
:- PlanLater Filter (isnotnull(date#32) AND dynamicpruning#49 [date#32])
+- PlanLater Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
】

15:54:34.531 [ScalaTest-run-running-PhysicalPlanSpec] INFO  org.apache.spark.sql.execution.datasources.DataSourceStrategy - Pruning directories with: isnotnull(date#32),dynamicpruning#49 [date#32]
15:54:34.533 [ScalaTest-run-running-PhysicalPlanSpec] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: 
15:54:34.534 [ScalaTest-run-running-PhysicalPlanSpec] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: 
15:54:34.535 [ScalaTest-run-running-PhysicalPlanSpec] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<id: int, value: int>
SparkPlanner(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2@67b3960b) - org.apache.spark.sql.execution.datasources.FileSourceStrategy$ -->【
Filter (isnotnull(date#32) AND dynamicpruning#49 [date#32])
:  +- Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
:     +- Relation default.dim_table[date#33,event#34] parquet
+- Relation default.fact_table[id#30,value#31,date#32] parquet
】 --> 【
FileScan parquet default.fact_table[id#30,value#31,date#32] Batched: false, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(3 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [isnotnull(date#32), dynamicpruning#49 [date#32]], PushedFilters: [], ReadSchema: struct<id:int,value:int>
   +- Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
      +- Relation default.dim_table[date#33,event#34] parquet
】

15:54:34.560 [ScalaTest-run-running-PhysicalPlanSpec] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: IsNotNull(event),EqualTo(event,New Year),IsNotNull(date)
15:54:34.560 [ScalaTest-run-running-PhysicalPlanSpec] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(event#34),(event#34 = New Year),isnotnull(date#33)
15:54:34.560 [ScalaTest-run-running-PhysicalPlanSpec] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<date: string, event: string>
SparkPlanner(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2@67b3960b) - org.apache.spark.sql.execution.datasources.FileSourceStrategy$ -->【
Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
+- Relation default.dim_table[date#33,event#34] parquet
】 --> 【
Project [date#33, event#34]
+- Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
   +- FileScan parquet default.dim_table[date#33,event#34] Batched: false, DataFilters: [isnotnull(event#34), (event#34 = New Year), isnotnull(date#33)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(event), EqualTo(event,New Year), IsNotNull(date)], ReadSchema: struct<date:string,event:string>
】

sparkPlan ====> 
CollectLimit 21
+- Project [cast(id#30 as string) AS id#43, cast(value#31 as string) AS value#44, event#34]
   +- BroadcastHashJoin [date#32], [date#33], Inner, BuildRight, false
      :- FileScan parquet default.fact_table[id#30,value#31,date#32] Batched: false, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(3 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [isnotnull(date#32), dynamicpruning#49 [date#32]], PushedFilters: [], ReadSchema: struct<id:int,value:int>
      :     +- Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
      :        +- Relation default.dim_table[date#33,event#34] parquet
      +- Project [date#33, event#34]
         +- Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
            +- FileScan parquet default.dim_table[date#33,event#34] Batched: false, DataFilters: [isnotnull(event#34), (event#34 = New Year), isnotnull(date#33)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(event), EqualTo(event,New Year), IsNotNull(date)], ReadSchema: struct<date:string,event:string>

SparkPlanner(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2@67b3960b) - org.apache.spark.sql.execution.SparkStrategies$SpecialLimits$ -->【
ReturnAnswer
+- Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
   +- Relation default.dim_table[date#33,event#34] parquet
】 --> 【
PlanLater Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
】

15:54:35.775 [ScalaTest-run-running-PhysicalPlanSpec] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: IsNotNull(event),EqualTo(event,New Year),IsNotNull(date)
15:54:35.775 [ScalaTest-run-running-PhysicalPlanSpec] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(event#34),(event#34 = New Year),isnotnull(date#33)
15:54:35.775 [ScalaTest-run-running-PhysicalPlanSpec] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<date: string, event: string>
SparkPlanner(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2@67b3960b) - org.apache.spark.sql.execution.datasources.FileSourceStrategy$ -->【
Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
+- Relation default.dim_table[date#33,event#34] parquet
】 --> 【
Project [date#33, event#34]
+- Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
   +- FileScan parquet default.dim_table[date#33,event#34] Batched: false, DataFilters: [isnotnull(event#34), (event#34 = New Year), isnotnull(date#33)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(event), EqualTo(event,New Year), IsNotNull(date)], ReadSchema: struct<date:string,event:string>
】

15:54:35.785 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.execution.adaptive.InsertAdaptiveSparkPlan - Adaptive execution enabled for plan: 
Project [date#33, event#34]
+- Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
   +- FileScan parquet default.dim_table[date#33,event#34] Batched: false, DataFilters: [isnotnull(event#34), (event#34 = New Year), isnotnull(date#33)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(event), EqualTo(event,New Year), IsNotNull(date)], ReadSchema: struct<date:string,event:string>
 ==>
Project [date#33, event#34]
+- Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
   +- FileScan parquet default.dim_table[date#33,event#34] Batched: false, DataFilters: [isnotnull(event#34), (event#34 = New Year), isnotnull(date#33)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(event), EqualTo(event,New Year), IsNotNull(date)], ReadSchema: struct<date:string,event:string>

org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec$ - applyPhysicalRules - org.apache.spark.sql.execution.RemoveRedundantProjects$ -->【
Project [date#33, event#34]
+- Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
   +- FileScan parquet default.dim_table[date#33,event#34] Batched: false, DataFilters: [isnotnull(event#34), (event#34 = New Year), isnotnull(date#33)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(event), EqualTo(event,New Year), IsNotNull(date)], ReadSchema: struct<date:string,event:string>
】AQE --> 【
Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
+- FileScan parquet default.dim_table[date#33,event#34] Batched: false, DataFilters: [isnotnull(event#34), (event#34 = New Year), isnotnull(date#33)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(event), EqualTo(event,New Year), IsNotNull(date)], ReadSchema: struct<date:string,event:string>
】

org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec$ final result ==>【
Project [date#33, event#34]
+- Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
   +- FileScan parquet default.dim_table[date#33,event#34] Batched: false, DataFilters: [isnotnull(event#34), (event#34 = New Year), isnotnull(date#33)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(event), EqualTo(event,New Year), IsNotNull(date)], ReadSchema: struct<date:string,event:string>
】AQE ==> 【
Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
+- FileScan parquet default.dim_table[date#33,event#34] Batched: false, DataFilters: [isnotnull(event#34), (event#34 = New Year), isnotnull(date#33)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(event), EqualTo(event,New Year), IsNotNull(date)], ReadSchema: struct<date:string,event:string>
】

15:54:35.839 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.execution.adaptive.InsertAdaptiveSparkPlan - Adaptive execution enabled for plan: 
CollectLimit 21
+- Project [cast(id#30 as string) AS id#43, cast(value#31 as string) AS value#44, event#34]
   +- BroadcastHashJoin [date#32], [date#33], Inner, BuildRight, false
      :- FileScan parquet default.fact_table[id#30,value#31,date#32] Batched: false, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(3 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [isnotnull(date#32), dynamicpruning#49 [date#32]], PushedFilters: [], ReadSchema: struct<id:int,value:int>
      :     +- Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
      :        +- Relation default.dim_table[date#33,event#34] parquet
      +- Project [date#33, event#34]
         +- Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
            +- FileScan parquet default.dim_table[date#33,event#34] Batched: false, DataFilters: [isnotnull(event#34), (event#34 = New Year), isnotnull(date#33)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(event), EqualTo(event,New Year), IsNotNull(date)], ReadSchema: struct<date:string,event:string>
 ==>
CollectLimit 21
+- Project [cast(id#30 as string) AS id#43, cast(value#31 as string) AS value#44, event#34]
   +- BroadcastHashJoin [date#32], [date#33], Inner, BuildRight, false
      :- FileScan parquet default.fact_table[id#30,value#31,date#32] Batched: false, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(3 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [isnotnull(date#32), dynamicpruningexpression(date#32 IN dynamicpruning#49)], PushedFilters: [], ReadSchema: struct<id:int,value:int>
      :     +- SubqueryAdaptiveBroadcast dynamicpruning#49, 0, false, Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33)), [date#33]
      :        +- AdaptiveSparkPlan isFinalPlan=false
      :           +- Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
      :              +- FileScan parquet default.dim_table[date#33,event#34] Batched: false, DataFilters: [isnotnull(event#34), (event#34 = New Year), isnotnull(date#33)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(event), EqualTo(event,New Year), IsNotNull(date)], ReadSchema: struct<date:string,event:string>
      +- Project [date#33, event#34]
         +- Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
            +- FileScan parquet default.dim_table[date#33,event#34] Batched: false, DataFilters: [isnotnull(event#34), (event#34 = New Year), isnotnull(date#33)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(event), EqualTo(event,New Year), IsNotNull(date)], ReadSchema: struct<date:string,event:string>

org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec$ - applyPhysicalRules - org.apache.spark.sql.execution.RemoveRedundantProjects$ -->【
CollectLimit 21
+- Project [cast(id#30 as string) AS id#43, cast(value#31 as string) AS value#44, event#34]
   +- BroadcastHashJoin [date#32], [date#33], Inner, BuildRight, false
      :- FileScan parquet default.fact_table[id#30,value#31,date#32] Batched: false, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(3 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [isnotnull(date#32), dynamicpruningexpression(date#32 IN dynamicpruning#49)], PushedFilters: [], ReadSchema: struct<id:int,value:int>
      :     +- SubqueryAdaptiveBroadcast dynamicpruning#49, 0, false, Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33)), [date#33]
      :        +- AdaptiveSparkPlan isFinalPlan=false
      :           +- Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
      :              +- FileScan parquet default.dim_table[date#33,event#34] Batched: false, DataFilters: [isnotnull(event#34), (event#34 = New Year), isnotnull(date#33)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(event), EqualTo(event,New Year), IsNotNull(date)], ReadSchema: struct<date:string,event:string>
      +- Project [date#33, event#34]
         +- Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
            +- FileScan parquet default.dim_table[date#33,event#34] Batched: false, DataFilters: [isnotnull(event#34), (event#34 = New Year), isnotnull(date#33)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(event), EqualTo(event,New Year), IsNotNull(date)], ReadSchema: struct<date:string,event:string>
】AQE --> 【
CollectLimit 21
+- Project [cast(id#30 as string) AS id#43, cast(value#31 as string) AS value#44, event#34]
   +- BroadcastHashJoin [date#32], [date#33], Inner, BuildRight, false
      :- FileScan parquet default.fact_table[id#30,value#31,date#32] Batched: false, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(3 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [isnotnull(date#32), dynamicpruningexpression(date#32 IN dynamicpruning#49)], PushedFilters: [], ReadSchema: struct<id:int,value:int>
      :     +- SubqueryAdaptiveBroadcast dynamicpruning#49, 0, false, Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33)), [date#33]
      :        +- AdaptiveSparkPlan isFinalPlan=false
      :           +- Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
      :              +- FileScan parquet default.dim_table[date#33,event#34] Batched: false, DataFilters: [isnotnull(event#34), (event#34 = New Year), isnotnull(date#33)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(event), EqualTo(event,New Year), IsNotNull(date)], ReadSchema: struct<date:string,event:string>
      +- Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
         +- FileScan parquet default.dim_table[date#33,event#34] Batched: false, DataFilters: [isnotnull(event#34), (event#34 = New Year), isnotnull(date#33)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(event), EqualTo(event,New Year), IsNotNull(date)], ReadSchema: struct<date:string,event:string>
】

org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec$ - applyPhysicalRules - org.apache.spark.sql.execution.exchange.EnsureRequirements -->【
CollectLimit 21
+- Project [cast(id#30 as string) AS id#43, cast(value#31 as string) AS value#44, event#34]
   +- BroadcastHashJoin [date#32], [date#33], Inner, BuildRight, false
      :- FileScan parquet default.fact_table[id#30,value#31,date#32] Batched: false, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(3 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [isnotnull(date#32), dynamicpruningexpression(date#32 IN dynamicpruning#49)], PushedFilters: [], ReadSchema: struct<id:int,value:int>
      :     +- SubqueryAdaptiveBroadcast dynamicpruning#49, 0, false, Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33)), [date#33]
      :        +- AdaptiveSparkPlan isFinalPlan=false
      :           +- Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
      :              +- FileScan parquet default.dim_table[date#33,event#34] Batched: false, DataFilters: [isnotnull(event#34), (event#34 = New Year), isnotnull(date#33)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(event), EqualTo(event,New Year), IsNotNull(date)], ReadSchema: struct<date:string,event:string>
      +- Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
         +- FileScan parquet default.dim_table[date#33,event#34] Batched: false, DataFilters: [isnotnull(event#34), (event#34 = New Year), isnotnull(date#33)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(event), EqualTo(event,New Year), IsNotNull(date)], ReadSchema: struct<date:string,event:string>
】AQE --> 【
CollectLimit 21
+- Project [cast(id#30 as string) AS id#43, cast(value#31 as string) AS value#44, event#34]
   +- BroadcastHashJoin [date#32], [date#33], Inner, BuildRight, false
      :- FileScan parquet default.fact_table[id#30,value#31,date#32] Batched: false, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(3 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [isnotnull(date#32), dynamicpruningexpression(date#32 IN dynamicpruning#49)], PushedFilters: [], ReadSchema: struct<id:int,value:int>
      :     +- SubqueryAdaptiveBroadcast dynamicpruning#49, 0, false, Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33)), [date#33]
      :        +- AdaptiveSparkPlan isFinalPlan=false
      :           +- Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
      :              +- FileScan parquet default.dim_table[date#33,event#34] Batched: false, DataFilters: [isnotnull(event#34), (event#34 = New Year), isnotnull(date#33)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(event), EqualTo(event,New Year), IsNotNull(date)], ReadSchema: struct<date:string,event:string>
      +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, false]),false), [plan_id=52]
         +- Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
            +- FileScan parquet default.dim_table[date#33,event#34] Batched: false, DataFilters: [isnotnull(event#34), (event#34 = New Year), isnotnull(date#33)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(event), EqualTo(event,New Year), IsNotNull(date)], ReadSchema: struct<date:string,event:string>
】

org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec$ final result ==>【
CollectLimit 21
+- Project [cast(id#30 as string) AS id#43, cast(value#31 as string) AS value#44, event#34]
   +- BroadcastHashJoin [date#32], [date#33], Inner, BuildRight, false
      :- FileScan parquet default.fact_table[id#30,value#31,date#32] Batched: false, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(3 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [isnotnull(date#32), dynamicpruningexpression(date#32 IN dynamicpruning#49)], PushedFilters: [], ReadSchema: struct<id:int,value:int>
      :     +- SubqueryAdaptiveBroadcast dynamicpruning#49, 0, false, Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33)), [date#33]
      :        +- AdaptiveSparkPlan isFinalPlan=false
      :           +- Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
      :              +- FileScan parquet default.dim_table[date#33,event#34] Batched: false, DataFilters: [isnotnull(event#34), (event#34 = New Year), isnotnull(date#33)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(event), EqualTo(event,New Year), IsNotNull(date)], ReadSchema: struct<date:string,event:string>
      +- Project [date#33, event#34]
         +- Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
            +- FileScan parquet default.dim_table[date#33,event#34] Batched: false, DataFilters: [isnotnull(event#34), (event#34 = New Year), isnotnull(date#33)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(event), EqualTo(event,New Year), IsNotNull(date)], ReadSchema: struct<date:string,event:string>
】AQE ==> 【
CollectLimit 21
+- Project [cast(id#30 as string) AS id#43, cast(value#31 as string) AS value#44, event#34]
   +- BroadcastHashJoin [date#32], [date#33], Inner, BuildRight, false
      :- FileScan parquet default.fact_table[id#30,value#31,date#32] Batched: false, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(3 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [isnotnull(date#32), dynamicpruningexpression(date#32 IN dynamicpruning#49)], PushedFilters: [], ReadSchema: struct<id:int,value:int>
      :     +- SubqueryAdaptiveBroadcast dynamicpruning#49, 0, false, Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33)), [date#33]
      :        +- AdaptiveSparkPlan isFinalPlan=false
      :           +- Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
      :              +- FileScan parquet default.dim_table[date#33,event#34] Batched: false, DataFilters: [isnotnull(event#34), (event#34 = New Year), isnotnull(date#33)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(event), EqualTo(event,New Year), IsNotNull(date)], ReadSchema: struct<date:string,event:string>
      +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, false]),false), [plan_id=52]
         +- Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
            +- FileScan parquet default.dim_table[date#33,event#34] Batched: false, DataFilters: [isnotnull(event#34), (event#34 = New Year), isnotnull(date#33)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(event), EqualTo(event,New Year), IsNotNull(date)], ReadSchema: struct<date:string,event:string>
】

org.apache.spark.sql.execution.QueryExecution$ - org.apache.spark.sql.execution.adaptive.InsertAdaptiveSparkPlan -->【
CollectLimit 21
+- Project [cast(id#30 as string) AS id#43, cast(value#31 as string) AS value#44, event#34]
   +- BroadcastHashJoin [date#32], [date#33], Inner, BuildRight, false
      :- FileScan parquet default.fact_table[id#30,value#31,date#32] Batched: false, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(3 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [isnotnull(date#32), dynamicpruning#49 [date#32]], PushedFilters: [], ReadSchema: struct<id:int,value:int>
      :     +- Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
      :        +- Relation default.dim_table[date#33,event#34] parquet
      +- Project [date#33, event#34]
         +- Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
            +- FileScan parquet default.dim_table[date#33,event#34] Batched: false, DataFilters: [isnotnull(event#34), (event#34 = New Year), isnotnull(date#33)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(event), EqualTo(event,New Year), IsNotNull(date)], ReadSchema: struct<date:string,event:string>
】 --> 【
AdaptiveSparkPlan isFinalPlan=false
+- CollectLimit 21
   +- Project [cast(id#30 as string) AS id#43, cast(value#31 as string) AS value#44, event#34]
      +- BroadcastHashJoin [date#32], [date#33], Inner, BuildRight, false
         :- FileScan parquet default.fact_table[id#30,value#31,date#32] Batched: false, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(3 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [isnotnull(date#32), dynamicpruningexpression(date#32 IN dynamicpruning#49)], PushedFilters: [], ReadSchema: struct<id:int,value:int>
         :     +- SubqueryAdaptiveBroadcast dynamicpruning#49, 0, false, Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33)), [date#33]
         :        +- AdaptiveSparkPlan isFinalPlan=false
         :           +- Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
         :              +- FileScan parquet default.dim_table[date#33,event#34] Batched: false, DataFilters: [isnotnull(event#34), (event#34 = New Year), isnotnull(date#33)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(event), EqualTo(event,New Year), IsNotNull(date)], ReadSchema: struct<date:string,event:string>
         +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, false]),false), [plan_id=52]
            +- Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
               +- FileScan parquet default.dim_table[date#33,event#34] Batched: false, DataFilters: [isnotnull(event#34), (event#34 = New Year), isnotnull(date#33)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(event), EqualTo(event,New Year), IsNotNull(date)], ReadSchema: struct<date:string,event:string>
】

org.apache.spark.sql.execution.QueryExecution$ prepareForExecution final result ==>【
CollectLimit 21
+- Project [cast(id#30 as string) AS id#43, cast(value#31 as string) AS value#44, event#34]
   +- BroadcastHashJoin [date#32], [date#33], Inner, BuildRight, false
      :- FileScan parquet default.fact_table[id#30,value#31,date#32] Batched: false, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(3 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [isnotnull(date#32), dynamicpruning#49 [date#32]], PushedFilters: [], ReadSchema: struct<id:int,value:int>
      :     +- Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
      :        +- Relation default.dim_table[date#33,event#34] parquet
      +- Project [date#33, event#34]
         +- Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
            +- FileScan parquet default.dim_table[date#33,event#34] Batched: false, DataFilters: [isnotnull(event#34), (event#34 = New Year), isnotnull(date#33)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(event), EqualTo(event,New Year), IsNotNull(date)], ReadSchema: struct<date:string,event:string>
】 ==> 【
AdaptiveSparkPlan isFinalPlan=false
+- CollectLimit 21
   +- Project [cast(id#30 as string) AS id#43, cast(value#31 as string) AS value#44, event#34]
      +- BroadcastHashJoin [date#32], [date#33], Inner, BuildRight, false
         :- FileScan parquet default.fact_table[id#30,value#31,date#32] Batched: false, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(3 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [isnotnull(date#32), dynamicpruningexpression(date#32 IN dynamicpruning#49)], PushedFilters: [], ReadSchema: struct<id:int,value:int>
         :     +- SubqueryAdaptiveBroadcast dynamicpruning#49, 0, false, Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33)), [date#33]
         :        +- AdaptiveSparkPlan isFinalPlan=false
         :           +- Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
         :              +- FileScan parquet default.dim_table[date#33,event#34] Batched: false, DataFilters: [isnotnull(event#34), (event#34 = New Year), isnotnull(date#33)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(event), EqualTo(event,New Year), IsNotNull(date)], ReadSchema: struct<date:string,event:string>
         +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, false]),false), [plan_id=52]
            +- Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
               +- FileScan parquet default.dim_table[date#33,event#34] Batched: false, DataFilters: [isnotnull(event#34), (event#34 = New Year), isnotnull(date#33)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(event), EqualTo(event,New Year), IsNotNull(date)], ReadSchema: struct<date:string,event:string>
】

executedPlan ====> 
AdaptiveSparkPlan isFinalPlan=false
+- CollectLimit 21
   +- Project [cast(id#30 as string) AS id#43, cast(value#31 as string) AS value#44, event#34]
      +- BroadcastHashJoin [date#32], [date#33], Inner, BuildRight, false
         :- FileScan parquet default.fact_table[id#30,value#31,date#32] Batched: false, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(3 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [isnotnull(date#32), dynamicpruningexpression(date#32 IN dynamicpruning#49)], PushedFilters: [], ReadSchema: struct<id:int,value:int>
         :     +- SubqueryAdaptiveBroadcast dynamicpruning#49, 0, false, Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33)), [date#33]
         :        +- AdaptiveSparkPlan isFinalPlan=false
         :           +- Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
         :              +- FileScan parquet default.dim_table[date#33,event#34] Batched: false, DataFilters: [isnotnull(event#34), (event#34 = New Year), isnotnull(date#33)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(event), EqualTo(event,New Year), IsNotNull(date)], ReadSchema: struct<date:string,event:string>
         +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, false]),false), [plan_id=52]
            +- Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
               +- FileScan parquet default.dim_table[date#33,event#34] Batched: false, DataFilters: [isnotnull(event#34), (event#34 = New Year), isnotnull(date#33)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(event), EqualTo(event,New Year), IsNotNull(date)], ReadSchema: struct<date:string,event:string>

SparkPlanner(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2@67b3960b) - org.apache.spark.sql.execution.SparkStrategies$SpecialLimits$ -->【
ReturnAnswer
+- Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
   +- Relation default.dim_table[date#33,event#34] parquet
】 --> 【
PlanLater Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
】

15:54:50.520 [ScalaTest-run-running-PhysicalPlanSpec] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: IsNotNull(event),EqualTo(event,New Year),IsNotNull(date)
15:54:50.522 [ScalaTest-run-running-PhysicalPlanSpec] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(event#34),(event#34 = New Year),isnotnull(date#33)
15:54:50.525 [ScalaTest-run-running-PhysicalPlanSpec] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<date: string, event: string>
SparkPlanner(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2@67b3960b) - org.apache.spark.sql.execution.datasources.FileSourceStrategy$ -->【
Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
+- Relation default.dim_table[date#33,event#34] parquet
】 --> 【
Project [date#33, event#34]
+- Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
   +- FileScan parquet default.dim_table[date#33,event#34] Batched: false, DataFilters: [isnotnull(event#34), (event#34 = New Year), isnotnull(date#33)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(event), EqualTo(event,New Year), IsNotNull(date)], ReadSchema: struct<date:string,event:string>
】

15:54:55.521 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.execution.adaptive.InsertAdaptiveSparkPlan - Adaptive execution enabled for plan: 
Project [date#33, event#34]
+- Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
   +- FileScan parquet default.dim_table[date#33,event#34] Batched: false, DataFilters: [isnotnull(event#34), (event#34 = New Year), isnotnull(date#33)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(event), EqualTo(event,New Year), IsNotNull(date)], ReadSchema: struct<date:string,event:string>
 ==>
Project [date#33, event#34]
+- Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
   +- FileScan parquet default.dim_table[date#33,event#34] Batched: false, DataFilters: [isnotnull(event#34), (event#34 = New Year), isnotnull(date#33)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(event), EqualTo(event,New Year), IsNotNull(date)], ReadSchema: struct<date:string,event:string>

org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec$ - applyPhysicalRules - org.apache.spark.sql.execution.RemoveRedundantProjects$ -->【
Project [date#33, event#34]
+- Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
   +- FileScan parquet default.dim_table[date#33,event#34] Batched: false, DataFilters: [isnotnull(event#34), (event#34 = New Year), isnotnull(date#33)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(event), EqualTo(event,New Year), IsNotNull(date)], ReadSchema: struct<date:string,event:string>
】AQE --> 【
Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
+- FileScan parquet default.dim_table[date#33,event#34] Batched: false, DataFilters: [isnotnull(event#34), (event#34 = New Year), isnotnull(date#33)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(event), EqualTo(event,New Year), IsNotNull(date)], ReadSchema: struct<date:string,event:string>
】

org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec$ final result ==>【
Project [date#33, event#34]
+- Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
   +- FileScan parquet default.dim_table[date#33,event#34] Batched: false, DataFilters: [isnotnull(event#34), (event#34 = New Year), isnotnull(date#33)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(event), EqualTo(event,New Year), IsNotNull(date)], ReadSchema: struct<date:string,event:string>
】AQE ==> 【
Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
+- FileScan parquet default.dim_table[date#33,event#34] Batched: false, DataFilters: [isnotnull(event#34), (event#34 = New Year), isnotnull(date#33)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(event), EqualTo(event,New Year), IsNotNull(date)], ReadSchema: struct<date:string,event:string>
】

15:55:03.848 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.execution.adaptive.InsertAdaptiveSparkPlan - Adaptive execution enabled for plan: 
CollectLimit 21
+- Project [cast(id#30 as string) AS id#43, cast(value#31 as string) AS value#44, event#34]
   +- BroadcastHashJoin [date#32], [date#33], Inner, BuildRight, false
      :- FileScan parquet default.fact_table[id#30,value#31,date#32] Batched: false, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(3 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [isnotnull(date#32), dynamicpruning#49 [date#32]], PushedFilters: [], ReadSchema: struct<id:int,value:int>
      :     +- Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
      :        +- Relation default.dim_table[date#33,event#34] parquet
      +- Project [date#33, event#34]
         +- Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
            +- FileScan parquet default.dim_table[date#33,event#34] Batched: false, DataFilters: [isnotnull(event#34), (event#34 = New Year), isnotnull(date#33)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(event), EqualTo(event,New Year), IsNotNull(date)], ReadSchema: struct<date:string,event:string>
 ==>
CollectLimit 21
+- Project [cast(id#30 as string) AS id#43, cast(value#31 as string) AS value#44, event#34]
   +- BroadcastHashJoin [date#32], [date#33], Inner, BuildRight, false
      :- FileScan parquet default.fact_table[id#30,value#31,date#32] Batched: false, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(3 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [isnotnull(date#32), dynamicpruningexpression(date#32 IN dynamicpruning#49)], PushedFilters: [], ReadSchema: struct<id:int,value:int>
      :     +- SubqueryAdaptiveBroadcast dynamicpruning#49, 0, false, Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33)), [date#33]
      :        +- AdaptiveSparkPlan isFinalPlan=false
      :           +- Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
      :              +- FileScan parquet default.dim_table[date#33,event#34] Batched: false, DataFilters: [isnotnull(event#34), (event#34 = New Year), isnotnull(date#33)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(event), EqualTo(event,New Year), IsNotNull(date)], ReadSchema: struct<date:string,event:string>
      +- Project [date#33, event#34]
         +- Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
            +- FileScan parquet default.dim_table[date#33,event#34] Batched: false, DataFilters: [isnotnull(event#34), (event#34 = New Year), isnotnull(date#33)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(event), EqualTo(event,New Year), IsNotNull(date)], ReadSchema: struct<date:string,event:string>

org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec$ - applyPhysicalRules - org.apache.spark.sql.execution.RemoveRedundantProjects$ -->【
CollectLimit 21
+- Project [cast(id#30 as string) AS id#43, cast(value#31 as string) AS value#44, event#34]
   +- BroadcastHashJoin [date#32], [date#33], Inner, BuildRight, false
      :- FileScan parquet default.fact_table[id#30,value#31,date#32] Batched: false, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(3 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [isnotnull(date#32), dynamicpruningexpression(date#32 IN dynamicpruning#49)], PushedFilters: [], ReadSchema: struct<id:int,value:int>
      :     +- SubqueryAdaptiveBroadcast dynamicpruning#49, 0, false, Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33)), [date#33]
      :        +- AdaptiveSparkPlan isFinalPlan=false
      :           +- Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
      :              +- FileScan parquet default.dim_table[date#33,event#34] Batched: false, DataFilters: [isnotnull(event#34), (event#34 = New Year), isnotnull(date#33)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(event), EqualTo(event,New Year), IsNotNull(date)], ReadSchema: struct<date:string,event:string>
      +- Project [date#33, event#34]
         +- Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
            +- FileScan parquet default.dim_table[date#33,event#34] Batched: false, DataFilters: [isnotnull(event#34), (event#34 = New Year), isnotnull(date#33)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(event), EqualTo(event,New Year), IsNotNull(date)], ReadSchema: struct<date:string,event:string>
】AQE --> 【
CollectLimit 21
+- Project [cast(id#30 as string) AS id#43, cast(value#31 as string) AS value#44, event#34]
   +- BroadcastHashJoin [date#32], [date#33], Inner, BuildRight, false
      :- FileScan parquet default.fact_table[id#30,value#31,date#32] Batched: false, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(3 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [isnotnull(date#32), dynamicpruningexpression(date#32 IN dynamicpruning#49)], PushedFilters: [], ReadSchema: struct<id:int,value:int>
      :     +- SubqueryAdaptiveBroadcast dynamicpruning#49, 0, false, Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33)), [date#33]
      :        +- AdaptiveSparkPlan isFinalPlan=false
      :           +- Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
      :              +- FileScan parquet default.dim_table[date#33,event#34] Batched: false, DataFilters: [isnotnull(event#34), (event#34 = New Year), isnotnull(date#33)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(event), EqualTo(event,New Year), IsNotNull(date)], ReadSchema: struct<date:string,event:string>
      +- Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
         +- FileScan parquet default.dim_table[date#33,event#34] Batched: false, DataFilters: [isnotnull(event#34), (event#34 = New Year), isnotnull(date#33)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(event), EqualTo(event,New Year), IsNotNull(date)], ReadSchema: struct<date:string,event:string>
】

org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec$ - applyPhysicalRules - org.apache.spark.sql.execution.exchange.EnsureRequirements -->【
CollectLimit 21
+- Project [cast(id#30 as string) AS id#43, cast(value#31 as string) AS value#44, event#34]
   +- BroadcastHashJoin [date#32], [date#33], Inner, BuildRight, false
      :- FileScan parquet default.fact_table[id#30,value#31,date#32] Batched: false, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(3 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [isnotnull(date#32), dynamicpruningexpression(date#32 IN dynamicpruning#49)], PushedFilters: [], ReadSchema: struct<id:int,value:int>
      :     +- SubqueryAdaptiveBroadcast dynamicpruning#49, 0, false, Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33)), [date#33]
      :        +- AdaptiveSparkPlan isFinalPlan=false
      :           +- Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
      :              +- FileScan parquet default.dim_table[date#33,event#34] Batched: false, DataFilters: [isnotnull(event#34), (event#34 = New Year), isnotnull(date#33)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(event), EqualTo(event,New Year), IsNotNull(date)], ReadSchema: struct<date:string,event:string>
      +- Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
         +- FileScan parquet default.dim_table[date#33,event#34] Batched: false, DataFilters: [isnotnull(event#34), (event#34 = New Year), isnotnull(date#33)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(event), EqualTo(event,New Year), IsNotNull(date)], ReadSchema: struct<date:string,event:string>
】AQE --> 【
CollectLimit 21
+- Project [cast(id#30 as string) AS id#43, cast(value#31 as string) AS value#44, event#34]
   +- BroadcastHashJoin [date#32], [date#33], Inner, BuildRight, false
      :- FileScan parquet default.fact_table[id#30,value#31,date#32] Batched: false, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(3 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [isnotnull(date#32), dynamicpruningexpression(date#32 IN dynamicpruning#49)], PushedFilters: [], ReadSchema: struct<id:int,value:int>
      :     +- SubqueryAdaptiveBroadcast dynamicpruning#49, 0, false, Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33)), [date#33]
      :        +- AdaptiveSparkPlan isFinalPlan=false
      :           +- Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
      :              +- FileScan parquet default.dim_table[date#33,event#34] Batched: false, DataFilters: [isnotnull(event#34), (event#34 = New Year), isnotnull(date#33)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(event), EqualTo(event,New Year), IsNotNull(date)], ReadSchema: struct<date:string,event:string>
      +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, false]),false), [plan_id=70]
         +- Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
            +- FileScan parquet default.dim_table[date#33,event#34] Batched: false, DataFilters: [isnotnull(event#34), (event#34 = New Year), isnotnull(date#33)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(event), EqualTo(event,New Year), IsNotNull(date)], ReadSchema: struct<date:string,event:string>
】

org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec$ final result ==>【
CollectLimit 21
+- Project [cast(id#30 as string) AS id#43, cast(value#31 as string) AS value#44, event#34]
   +- BroadcastHashJoin [date#32], [date#33], Inner, BuildRight, false
      :- FileScan parquet default.fact_table[id#30,value#31,date#32] Batched: false, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(3 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [isnotnull(date#32), dynamicpruningexpression(date#32 IN dynamicpruning#49)], PushedFilters: [], ReadSchema: struct<id:int,value:int>
      :     +- SubqueryAdaptiveBroadcast dynamicpruning#49, 0, false, Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33)), [date#33]
      :        +- AdaptiveSparkPlan isFinalPlan=false
      :           +- Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
      :              +- FileScan parquet default.dim_table[date#33,event#34] Batched: false, DataFilters: [isnotnull(event#34), (event#34 = New Year), isnotnull(date#33)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(event), EqualTo(event,New Year), IsNotNull(date)], ReadSchema: struct<date:string,event:string>
      +- Project [date#33, event#34]
         +- Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
            +- FileScan parquet default.dim_table[date#33,event#34] Batched: false, DataFilters: [isnotnull(event#34), (event#34 = New Year), isnotnull(date#33)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(event), EqualTo(event,New Year), IsNotNull(date)], ReadSchema: struct<date:string,event:string>
】AQE ==> 【
CollectLimit 21
+- Project [cast(id#30 as string) AS id#43, cast(value#31 as string) AS value#44, event#34]
   +- BroadcastHashJoin [date#32], [date#33], Inner, BuildRight, false
      :- FileScan parquet default.fact_table[id#30,value#31,date#32] Batched: false, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(3 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [isnotnull(date#32), dynamicpruningexpression(date#32 IN dynamicpruning#49)], PushedFilters: [], ReadSchema: struct<id:int,value:int>
      :     +- SubqueryAdaptiveBroadcast dynamicpruning#49, 0, false, Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33)), [date#33]
      :        +- AdaptiveSparkPlan isFinalPlan=false
      :           +- Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
      :              +- FileScan parquet default.dim_table[date#33,event#34] Batched: false, DataFilters: [isnotnull(event#34), (event#34 = New Year), isnotnull(date#33)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(event), EqualTo(event,New Year), IsNotNull(date)], ReadSchema: struct<date:string,event:string>
      +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, false]),false), [plan_id=70]
         +- Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
            +- FileScan parquet default.dim_table[date#33,event#34] Batched: false, DataFilters: [isnotnull(event#34), (event#34 = New Year), isnotnull(date#33)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(event), EqualTo(event,New Year), IsNotNull(date)], ReadSchema: struct<date:string,event:string>
】

org.apache.spark.sql.execution.QueryExecution$ - org.apache.spark.sql.execution.adaptive.InsertAdaptiveSparkPlan -->【
CollectLimit 21
+- Project [cast(id#30 as string) AS id#43, cast(value#31 as string) AS value#44, event#34]
   +- BroadcastHashJoin [date#32], [date#33], Inner, BuildRight, false
      :- FileScan parquet default.fact_table[id#30,value#31,date#32] Batched: false, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(3 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [isnotnull(date#32), dynamicpruning#49 [date#32]], PushedFilters: [], ReadSchema: struct<id:int,value:int>
      :     +- Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
      :        +- Relation default.dim_table[date#33,event#34] parquet
      +- Project [date#33, event#34]
         +- Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
            +- FileScan parquet default.dim_table[date#33,event#34] Batched: false, DataFilters: [isnotnull(event#34), (event#34 = New Year), isnotnull(date#33)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(event), EqualTo(event,New Year), IsNotNull(date)], ReadSchema: struct<date:string,event:string>
】 --> 【
AdaptiveSparkPlan isFinalPlan=false
+- CollectLimit 21
   +- Project [cast(id#30 as string) AS id#43, cast(value#31 as string) AS value#44, event#34]
      +- BroadcastHashJoin [date#32], [date#33], Inner, BuildRight, false
         :- FileScan parquet default.fact_table[id#30,value#31,date#32] Batched: false, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(3 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [isnotnull(date#32), dynamicpruningexpression(date#32 IN dynamicpruning#49)], PushedFilters: [], ReadSchema: struct<id:int,value:int>
         :     +- SubqueryAdaptiveBroadcast dynamicpruning#49, 0, false, Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33)), [date#33]
         :        +- AdaptiveSparkPlan isFinalPlan=false
         :           +- Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
         :              +- FileScan parquet default.dim_table[date#33,event#34] Batched: false, DataFilters: [isnotnull(event#34), (event#34 = New Year), isnotnull(date#33)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(event), EqualTo(event,New Year), IsNotNull(date)], ReadSchema: struct<date:string,event:string>
         +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, false]),false), [plan_id=70]
            +- Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
               +- FileScan parquet default.dim_table[date#33,event#34] Batched: false, DataFilters: [isnotnull(event#34), (event#34 = New Year), isnotnull(date#33)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(event), EqualTo(event,New Year), IsNotNull(date)], ReadSchema: struct<date:string,event:string>
】

org.apache.spark.sql.execution.QueryExecution$ prepareForExecution final result ==>【
CollectLimit 21
+- Project [cast(id#30 as string) AS id#43, cast(value#31 as string) AS value#44, event#34]
   +- BroadcastHashJoin [date#32], [date#33], Inner, BuildRight, false
      :- FileScan parquet default.fact_table[id#30,value#31,date#32] Batched: false, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(3 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [isnotnull(date#32), dynamicpruning#49 [date#32]], PushedFilters: [], ReadSchema: struct<id:int,value:int>
      :     +- Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
      :        +- Relation default.dim_table[date#33,event#34] parquet
      +- Project [date#33, event#34]
         +- Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
            +- FileScan parquet default.dim_table[date#33,event#34] Batched: false, DataFilters: [isnotnull(event#34), (event#34 = New Year), isnotnull(date#33)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(event), EqualTo(event,New Year), IsNotNull(date)], ReadSchema: struct<date:string,event:string>
】 ==> 【
AdaptiveSparkPlan isFinalPlan=false
+- CollectLimit 21
   +- Project [cast(id#30 as string) AS id#43, cast(value#31 as string) AS value#44, event#34]
      +- BroadcastHashJoin [date#32], [date#33], Inner, BuildRight, false
         :- FileScan parquet default.fact_table[id#30,value#31,date#32] Batched: false, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(3 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [isnotnull(date#32), dynamicpruningexpression(date#32 IN dynamicpruning#49)], PushedFilters: [], ReadSchema: struct<id:int,value:int>
         :     +- SubqueryAdaptiveBroadcast dynamicpruning#49, 0, false, Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33)), [date#33]
         :        +- AdaptiveSparkPlan isFinalPlan=false
         :           +- Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
         :              +- FileScan parquet default.dim_table[date#33,event#34] Batched: false, DataFilters: [isnotnull(event#34), (event#34 = New Year), isnotnull(date#33)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(event), EqualTo(event,New Year), IsNotNull(date)], ReadSchema: struct<date:string,event:string>
         +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, false]),false), [plan_id=70]
            +- Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
               +- FileScan parquet default.dim_table[date#33,event#34] Batched: false, DataFilters: [isnotnull(event#34), (event#34 = New Year), isnotnull(date#33)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(event), EqualTo(event,New Year), IsNotNull(date)], ReadSchema: struct<date:string,event:string>
】

executedPlan ====> 
AdaptiveSparkPlan isFinalPlan=false
+- CollectLimit 21
   +- Project [cast(id#30 as string) AS id#43, cast(value#31 as string) AS value#44, event#34]
      +- BroadcastHashJoin [date#32], [date#33], Inner, BuildRight, false
         :- FileScan parquet default.fact_table[id#30,value#31,date#32] Batched: false, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(3 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [isnotnull(date#32), dynamicpruningexpression(date#32 IN dynamicpruning#49)], PushedFilters: [], ReadSchema: struct<id:int,value:int>
         :     +- SubqueryAdaptiveBroadcast dynamicpruning#49, 0, false, Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33)), [date#33]
         :        +- AdaptiveSparkPlan isFinalPlan=false
         :           +- Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
         :              +- FileScan parquet default.dim_table[date#33,event#34] Batched: false, DataFilters: [isnotnull(event#34), (event#34 = New Year), isnotnull(date#33)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(event), EqualTo(event,New Year), IsNotNull(date)], ReadSchema: struct<date:string,event:string>
         +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, false]),false), [plan_id=70]
            +- Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
               +- FileScan parquet default.dim_table[date#33,event#34] Batched: false, DataFilters: [isnotnull(event#34), (event#34 = New Year), isnotnull(date#33)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(event), EqualTo(event,New Year), IsNotNull(date)], ReadSchema: struct<date:string,event:string>

Analyzer(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$1@2527765a) org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveDeserializer --> old【
'DeserializeToObject unresolveddeserializer(createexternalrow(getcolumnbyordinal(0, StructField(id,StringType,true), StructField(value,StringType,true), StructField(event,StringType,true)).toString, getcolumnbyordinal(1, StructField(id,StringType,true), StructField(value,StringType,true), StructField(event,StringType,true)).toString, getcolumnbyordinal(2, StructField(id,StringType,true), StructField(value,StringType,true), StructField(event,StringType,true)).toString, StructField(id,StringType,true), StructField(value,StringType,true), StructField(event,StringType,true))), obj#50: org.apache.spark.sql.Row
+- LocalRelation <empty>, [id#43, value#44, event#45]
】--> new【
DeserializeToObject createexternalrow(id#43.toString, value#44.toString, event#45.toString, StructField(id,StringType,true), StructField(value,StringType,true), StructField(event,StringType,true)), obj#50: org.apache.spark.sql.Row
+- LocalRelation <empty>, [id#43, value#44, event#45]
】


RuleExecutor.execute[Analyzer(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$1@2527765a)] final result ==> old【
'DeserializeToObject unresolveddeserializer(createexternalrow(getcolumnbyordinal(0, StructField(id,StringType,true), StructField(value,StringType,true), StructField(event,StringType,true)).toString, getcolumnbyordinal(1, StructField(id,StringType,true), StructField(value,StringType,true), StructField(event,StringType,true)).toString, getcolumnbyordinal(2, StructField(id,StringType,true), StructField(value,StringType,true), StructField(event,StringType,true)).toString, StructField(id,StringType,true), StructField(value,StringType,true), StructField(event,StringType,true))), obj#50: org.apache.spark.sql.Row
+- LocalRelation <empty>, [id#43, value#44, event#45]
】==> new【
DeserializeToObject createexternalrow(id#43.toString, value#44.toString, event#45.toString, StructField(id,StringType,true), StructField(value,StringType,true), StructField(event,StringType,true)), obj#50: org.apache.spark.sql.Row
+- LocalRelation <empty>, [id#43, value#44, event#45]
】


====== collectFromPlan ====== 
AdaptiveSparkPlan isFinalPlan=false
+- CollectLimit 21
   +- Project [cast(id#30 as string) AS id#43, cast(value#31 as string) AS value#44, event#34]
      +- BroadcastHashJoin [date#32], [date#33], Inner, BuildRight, false
         :- FileScan parquet default.fact_table[id#30,value#31,date#32] Batched: false, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(3 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [isnotnull(date#32), dynamicpruningexpression(date#32 IN dynamicpruning#49)], PushedFilters: [], ReadSchema: struct<id:int,value:int>
         :     +- SubqueryAdaptiveBroadcast dynamicpruning#49, 0, false, Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33)), [date#33]
         :        +- AdaptiveSparkPlan isFinalPlan=false
         :           +- Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
         :              +- FileScan parquet default.dim_table[date#33,event#34] Batched: false, DataFilters: [isnotnull(event#34), (event#34 = New Year), isnotnull(date#33)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(event), EqualTo(event,New Year), IsNotNull(date)], ReadSchema: struct<date:string,event:string>
         +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, false]),false), [plan_id=70]
            +- Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
               +- FileScan parquet default.dim_table[date#33,event#34] Batched: false, DataFilters: [isnotnull(event#34), (event#34 = New Year), isnotnull(date#33)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(event), EqualTo(event,New Year), IsNotNull(date)], ReadSchema: struct<date:string,event:string>

15:55:03.904 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec - withFinalPlanUpdate.executeCollect ==> before get final plan: 
CollectLimit 21
+- Project [cast(id#30 as string) AS id#43, cast(value#31 as string) AS value#44, event#34]
   +- BroadcastHashJoin [date#32], [date#33], Inner, BuildRight, false
      :- FileScan parquet default.fact_table[id#30,value#31,date#32] Batched: false, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(3 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [isnotnull(date#32), dynamicpruningexpression(date#32 IN dynamicpruning#49)], PushedFilters: [], ReadSchema: struct<id:int,value:int>
      :     +- SubqueryAdaptiveBroadcast dynamicpruning#49, 0, false, Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33)), [date#33]
      :        +- AdaptiveSparkPlan isFinalPlan=false
      :           +- Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
      :              +- FileScan parquet default.dim_table[date#33,event#34] Batched: false, DataFilters: [isnotnull(event#34), (event#34 = New Year), isnotnull(date#33)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(event), EqualTo(event,New Year), IsNotNull(date)], ReadSchema: struct<date:string,event:string>
      +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, false]),false), [plan_id=70]
         +- Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
            +- FileScan parquet default.dim_table[date#33,event#34] Batched: false, DataFilters: [isnotnull(event#34), (event#34 = New Year), isnotnull(date#33)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(event), EqualTo(event,New Year), IsNotNull(date)], ReadSchema: struct<date:string,event:string>

15:55:03.931 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.execution.adaptive.BroadcastQueryStageExec - Materialize query stage BroadcastQueryStageExec: 0, query stage plan:
BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, false]),false), [plan_id=70]
+- Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
   +- FileScan parquet default.dim_table[date#33,event#34] Batched: false, DataFilters: [isnotnull(event#34), (event#34 = New Year), isnotnull(date#33)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(event), EqualTo(event,New Year), IsNotNull(date)], ReadSchema: struct<date:string,event:string>

15:55:04.298 [broadcast-exchange-0] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 359.3 KiB, free 4.1 GiB)
15:55:04.300 [broadcast-exchange-0] DEBUG org.apache.spark.storage.BlockManager - Put block broadcast_0 locally took 39 ms
15:55:04.302 [broadcast-exchange-0] DEBUG org.apache.spark.storage.BlockManager - Putting block broadcast_0 without replication took 40 ms
15:55:04.626 [broadcast-exchange-0] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 4.1 GiB)
15:55:04.628 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_0_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
15:55:04.629 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on 192.168.31.69:58410 (size: 34.5 KiB, free: 4.1 GiB)
15:55:04.632 [broadcast-exchange-0] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_0_piece0
15:55:04.632 [broadcast-exchange-0] DEBUG org.apache.spark.storage.BlockManager - Told master about block broadcast_0_piece0
15:55:04.632 [broadcast-exchange-0] DEBUG org.apache.spark.storage.BlockManager - Put block broadcast_0_piece0 locally took 7 ms
15:55:04.632 [broadcast-exchange-0] DEBUG org.apache.spark.storage.BlockManager - Putting block broadcast_0_piece0 without replication took 7 ms
15:55:04.633 [broadcast-exchange-0] INFO  org.apache.spark.SparkContext - Created broadcast 0 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266
15:55:04.651 [broadcast-exchange-0] INFO  org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 4195093 bytes, open cost is considered as scanning 4194304 bytes.
15:55:04.716 [broadcast-exchange-0] DEBUG org.apache.spark.util.ClosureCleaner - Cleaning indylambda closure: $anonfun$collect$2
15:55:04.734 [broadcast-exchange-0] DEBUG org.apache.spark.util.ClosureCleaner -  +++ indylambda closure ($anonfun$collect$2) is now cleaned +++
15:55:04.887 [broadcast-exchange-0] DEBUG org.apache.spark.util.ClosureCleaner - Cleaning indylambda closure: $anonfun$runJob$5
15:55:04.890 [broadcast-exchange-0] DEBUG org.apache.spark.util.ClosureCleaner -  +++ indylambda closure ($anonfun$runJob$5) is now cleaned +++
15:55:04.893 [broadcast-exchange-0] INFO  org.apache.spark.SparkContext - Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266
15:55:04.896 [broadcast-exchange-0] DEBUG org.apache.spark.scheduler.DAGScheduler - eagerlyComputePartitionsForRddAndAncestors for RDD 3 took 0.001111 seconds
15:55:04.901 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - Merging stage rdd profiles: Set()
15:55:04.914 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 0 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) with 1 output partitions
15:55:04.915 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266)
15:55:04.915 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
15:55:04.916 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
15:55:04.919 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - submitStage(ResultStage 0 (name=$anonfun$withThreadLocalCaptured$1 at FutureTask.java:266;jobs=0))
15:55:04.919 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - missing: List()
15:55:04.921 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (MapPartitionsRDD[3] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266), which has no missing parents
15:55:04.922 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - submitMissingTasks(ResultStage 0)
15:55:04.946 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 18.7 KiB, free 4.1 GiB)
15:55:04.946 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager - Put block broadcast_1 locally took 1 ms
15:55:04.946 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager - Putting block broadcast_1 without replication took 1 ms
15:55:04.947 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 9.1 KiB, free 4.1 GiB)
15:55:04.947 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_1_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
15:55:04.948 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on 192.168.31.69:58410 (size: 9.1 KiB, free: 4.1 GiB)
15:55:04.948 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_1_piece0
15:55:04.948 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager - Told master about block broadcast_1_piece0
15:55:04.948 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager - Put block broadcast_1_piece0 locally took 1 ms
15:55:04.948 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager - Putting block broadcast_1_piece0 without replication took 1 ms
15:55:04.948 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1513
15:55:04.962 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) (first 15 tasks are for partitions Vector(0))
15:55:04.963 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks resource profile 0
15:55:04.985 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.TaskSetManager - Epoch for TaskSet 0.0: 0
15:55:04.988 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.TaskSetManager - Adding pending tasks took 2 ms
15:55:04.989 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.TaskSetManager - Valid locality levels for TaskSet 0.0: NO_PREF, ANY
15:55:05.000 [dispatcher-event-loop-0] DEBUG org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0.0, runningTasks: 0
15:55:05.000 [dispatcher-event-loop-0] DEBUG org.apache.spark.scheduler.TaskSetManager - Valid locality levels for TaskSet 0.0: NO_PREF, ANY
15:55:05.013 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0) (192.168.31.69, executor driver, partition 0, PROCESS_LOCAL, 5026 bytes) taskResourceAssignments Map()
15:55:05.028 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
15:55:05.034 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] DEBUG org.apache.spark.executor.ExecutorMetricsPoller - stageTCMP: (0, 0) -> 1
15:55:05.057 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] DEBUG org.apache.spark.storage.BlockManager - Getting local block broadcast_1
15:55:05.058 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] DEBUG org.apache.spark.storage.BlockManager - Level for block broadcast_1 is StorageLevel(disk, memory, deserialized, 1 replicas)
15:55:05.576 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 187.97192 ms
15:55:05.595 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 9.152046 ms
15:55:05.597 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-warehouse/dim_table/part-00000-392e1f87-219f-4557-a467-6e64b17b45b2-c000.snappy.parquet, range: 0-789, partition values: [empty row]
15:55:05.598 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] DEBUG org.apache.spark.storage.BlockManager - Getting local block broadcast_0
15:55:05.598 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] DEBUG org.apache.spark.storage.BlockManager - Level for block broadcast_0 is StorageLevel(disk, memory, deserialized, 1 replicas)
15:55:06.077 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] DEBUG org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport - Going to read the following fields from the Parquet file with the following schema:
Parquet file schema:
message spark_schema {
  optional binary date (STRING);
  optional binary event (STRING);
}

Parquet clipped schema:
message spark_schema {
  optional binary date (STRING);
  optional binary event (STRING);
}

Parquet requested schema:
message spark_schema {
  optional binary date (STRING);
  optional binary event (STRING);
}

Catalyst requested schema:
root
-- date: string (nullable = true)
-- event: string (nullable = true)

       
15:55:06.139 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] DEBUG org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat - Appending StructType() [empty row]
15:55:06.596 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 1791 bytes result sent to driver
15:55:06.598 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] DEBUG org.apache.spark.executor.ExecutorMetricsPoller - stageTCMP: (0, 0) -> 0
15:55:06.599 [dispatcher-event-loop-0] DEBUG org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0.0, runningTasks: 0
15:55:06.600 [dispatcher-event-loop-0] DEBUG org.apache.spark.scheduler.TaskSetManager - No tasks for locality level NO_PREF, so moving to locality level ANY
15:55:06.603 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 1599 ms on 192.168.31.69 (executor driver) (1/1)
15:55:06.605 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
15:55:06.611 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 0 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) finished in 1.669 s
15:55:06.613 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - After removal of stage 0, remaining stages = 0
15:55:06.614 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
15:55:06.614 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 0: Stage finished
15:55:06.616 [broadcast-exchange-0] INFO  org.apache.spark.scheduler.DAGScheduler - Job 0 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266, took 1.722966 s
15:55:06.633 [broadcast-exchange-0] DEBUG org.apache.spark.memory.TaskMemoryManager - Task 0 acquired 1024.0 B for org.apache.spark.unsafe.map.BytesToBytesMap@59e4734d
15:55:06.642 [broadcast-exchange-0] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 6.962997 ms
15:55:06.643 [broadcast-exchange-0] DEBUG org.apache.spark.memory.TaskMemoryManager - Task 0 acquired 64.0 MiB for org.apache.spark.unsafe.map.BytesToBytesMap@59e4734d
15:55:06.652 [broadcast-exchange-0] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 64.0 MiB, free 4.0 GiB)
15:55:06.652 [broadcast-exchange-0] DEBUG org.apache.spark.storage.BlockManager - Put block broadcast_2 locally took 0 ms
15:55:06.652 [broadcast-exchange-0] DEBUG org.apache.spark.storage.BlockManager - Putting block broadcast_2 without replication took 0 ms
15:55:06.655 [broadcast-exchange-0] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 206.0 B, free 4.0 GiB)
15:55:06.656 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_2_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
15:55:06.656 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on 192.168.31.69:58410 (size: 206.0 B, free: 4.1 GiB)
15:55:06.656 [broadcast-exchange-0] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_2_piece0
15:55:06.656 [broadcast-exchange-0] DEBUG org.apache.spark.storage.BlockManager - Told master about block broadcast_2_piece0
15:55:06.656 [broadcast-exchange-0] DEBUG org.apache.spark.storage.BlockManager - Put block broadcast_2_piece0 locally took 1 ms
15:55:06.656 [broadcast-exchange-0] DEBUG org.apache.spark.storage.BlockManager - Putting block broadcast_2_piece0 without replication took 1 ms
15:55:06.656 [broadcast-exchange-0] INFO  org.apache.spark.SparkContext - Created broadcast 2 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266
15:55:06.668 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - Considering join on: Some((date#32 = date#33))
15:55:06.668 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - leftKeys:List(date#32) | rightKeys:List(date#33)
SparkPlanner(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2@67b3960b) - org.apache.spark.sql.execution.SparkStrategies$SpecialLimits$ -->【
ReturnAnswer
+- GlobalLimit 21
   +- LocalLimit 21
      +- Project [cast(id#30 as string) AS id#43, cast(value#31 as string) AS value#44, event#34]
         +- Join Inner, (date#32 = date#33)
            :- Filter (isnotnull(date#32) AND dynamicpruning#49 [date#32])
            :  :  +- Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
            :  :     +- Relation default.dim_table[date#33,event#34] parquet
            :  +- Relation default.fact_table[id#30,value#31,date#32] parquet
            +- LogicalQueryStage Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33)), BroadcastQueryStage 0
】 --> 【
CollectLimit 21
+- PlanLater Project [cast(id#30 as string) AS id#43, cast(value#31 as string) AS value#44, event#34]
】

SparkPlanner(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2@67b3960b) - org.apache.spark.sql.execution.SparkStrategies$BasicOperators$ -->【
Project [cast(id#30 as string) AS id#43, cast(value#31 as string) AS value#44, event#34]
+- Join Inner, (date#32 = date#33)
   :- Filter (isnotnull(date#32) AND dynamicpruning#49 [date#32])
   :  :  +- Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
   :  :     +- Relation default.dim_table[date#33,event#34] parquet
   :  +- Relation default.fact_table[id#30,value#31,date#32] parquet
   +- LogicalQueryStage Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33)), BroadcastQueryStage 0
】 --> 【
Project [cast(id#30 as string) AS id#43, cast(value#31 as string) AS value#44, event#34]
+- PlanLater Join Inner, (date#32 = date#33)
】

15:55:06.671 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - Considering join on: Some((date#32 = date#33))
15:55:06.671 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - leftKeys:List(date#32) | rightKeys:List(date#33)
SparkPlanner(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2@67b3960b) - org.apache.spark.sql.execution.adaptive.LogicalQueryStageStrategy$ -->【
Join Inner, (date#32 = date#33)
:- Filter (isnotnull(date#32) AND dynamicpruning#49 [date#32])
:  :  +- Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
:  :     +- Relation default.dim_table[date#33,event#34] parquet
:  +- Relation default.fact_table[id#30,value#31,date#32] parquet
+- LogicalQueryStage Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33)), BroadcastQueryStage 0
】 --> 【
BroadcastHashJoin [date#32], [date#33], Inner, BuildRight, false
:- PlanLater Filter (isnotnull(date#32) AND dynamicpruning#49 [date#32])
+- PlanLater LogicalQueryStage Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33)), BroadcastQueryStage 0
】

15:55:06.673 [ScalaTest-run-running-PhysicalPlanSpec] INFO  org.apache.spark.sql.execution.datasources.DataSourceStrategy - Pruning directories with: isnotnull(date#32),dynamicpruning#49 [date#32]
15:55:06.673 [ScalaTest-run-running-PhysicalPlanSpec] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: 
15:55:06.673 [ScalaTest-run-running-PhysicalPlanSpec] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: 
15:55:06.673 [ScalaTest-run-running-PhysicalPlanSpec] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<id: int, value: int>
SparkPlanner(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2@67b3960b) - org.apache.spark.sql.execution.datasources.FileSourceStrategy$ -->【
Filter (isnotnull(date#32) AND dynamicpruning#49 [date#32])
:  +- Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
:     +- Relation default.dim_table[date#33,event#34] parquet
+- Relation default.fact_table[id#30,value#31,date#32] parquet
】 --> 【
FileScan parquet default.fact_table[id#30,value#31,date#32] Batched: false, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(3 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [isnotnull(date#32), dynamicpruning#49 [date#32]], PushedFilters: [], ReadSchema: struct<id:int,value:int>
   +- Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
      +- Relation default.dim_table[date#33,event#34] parquet
】

SparkPlanner(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2@67b3960b) - org.apache.spark.sql.execution.adaptive.LogicalQueryStageStrategy$ -->【
LogicalQueryStage Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33)), BroadcastQueryStage 0
】 --> 【
BroadcastQueryStage 0
+- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, false]),false), [plan_id=70]
   +- Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
      +- FileScan parquet default.dim_table[date#33,event#34] Batched: false, DataFilters: [isnotnull(event#34), (event#34 = New Year), isnotnull(date#33)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(event), EqualTo(event,New Year), IsNotNull(date)], ReadSchema: struct<date:string,event:string>
】

org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec$ - applyPhysicalRules - org.apache.spark.sql.execution.adaptive.PlanAdaptiveSubqueries -->【
CollectLimit 21
+- Project [cast(id#30 as string) AS id#43, cast(value#31 as string) AS value#44, event#34]
   +- BroadcastHashJoin [date#32], [date#33], Inner, BuildRight, false
      :- FileScan parquet default.fact_table[id#30,value#31,date#32] Batched: false, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(3 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [isnotnull(date#32), dynamicpruning#49 [date#32]], PushedFilters: [], ReadSchema: struct<id:int,value:int>
      :     +- Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
      :        +- Relation default.dim_table[date#33,event#34] parquet
      +- BroadcastQueryStage 0
         +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, false]),false), [plan_id=70]
            +- Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
               +- FileScan parquet default.dim_table[date#33,event#34] Batched: false, DataFilters: [isnotnull(event#34), (event#34 = New Year), isnotnull(date#33)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(event), EqualTo(event,New Year), IsNotNull(date)], ReadSchema: struct<date:string,event:string>
】AQE --> 【
CollectLimit 21
+- Project [cast(id#30 as string) AS id#43, cast(value#31 as string) AS value#44, event#34]
   +- BroadcastHashJoin [date#32], [date#33], Inner, BuildRight, false
      :- FileScan parquet default.fact_table[id#30,value#31,date#32] Batched: false, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(3 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [isnotnull(date#32), dynamicpruningexpression(date#32 IN dynamicpruning#49)], PushedFilters: [], ReadSchema: struct<id:int,value:int>
      :     +- SubqueryAdaptiveBroadcast dynamicpruning#49, 0, false, Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33)), [date#33]
      :        +- AdaptiveSparkPlan isFinalPlan=false
      :           +- Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
      :              +- FileScan parquet default.dim_table[date#33,event#34] Batched: false, DataFilters: [isnotnull(event#34), (event#34 = New Year), isnotnull(date#33)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(event), EqualTo(event,New Year), IsNotNull(date)], ReadSchema: struct<date:string,event:string>
      +- BroadcastQueryStage 0
         +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, false]),false), [plan_id=70]
            +- Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
               +- FileScan parquet default.dim_table[date#33,event#34] Batched: false, DataFilters: [isnotnull(event#34), (event#34 = New Year), isnotnull(date#33)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(event), EqualTo(event,New Year), IsNotNull(date)], ReadSchema: struct<date:string,event:string>
】

org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec$ final result ==>【
CollectLimit 21
+- Project [cast(id#30 as string) AS id#43, cast(value#31 as string) AS value#44, event#34]
   +- BroadcastHashJoin [date#32], [date#33], Inner, BuildRight, false
      :- FileScan parquet default.fact_table[id#30,value#31,date#32] Batched: false, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(3 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [isnotnull(date#32), dynamicpruning#49 [date#32]], PushedFilters: [], ReadSchema: struct<id:int,value:int>
      :     +- Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
      :        +- Relation default.dim_table[date#33,event#34] parquet
      +- BroadcastQueryStage 0
         +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, false]),false), [plan_id=70]
            +- Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
               +- FileScan parquet default.dim_table[date#33,event#34] Batched: false, DataFilters: [isnotnull(event#34), (event#34 = New Year), isnotnull(date#33)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(event), EqualTo(event,New Year), IsNotNull(date)], ReadSchema: struct<date:string,event:string>
】AQE ==> 【
CollectLimit 21
+- Project [cast(id#30 as string) AS id#43, cast(value#31 as string) AS value#44, event#34]
   +- BroadcastHashJoin [date#32], [date#33], Inner, BuildRight, false
      :- FileScan parquet default.fact_table[id#30,value#31,date#32] Batched: false, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(3 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [isnotnull(date#32), dynamicpruningexpression(date#32 IN dynamicpruning#49)], PushedFilters: [], ReadSchema: struct<id:int,value:int>
      :     +- SubqueryAdaptiveBroadcast dynamicpruning#49, 0, false, Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33)), [date#33]
      :        +- AdaptiveSparkPlan isFinalPlan=false
      :           +- Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
      :              +- FileScan parquet default.dim_table[date#33,event#34] Batched: false, DataFilters: [isnotnull(event#34), (event#34 = New Year), isnotnull(date#33)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(event), EqualTo(event,New Year), IsNotNull(date)], ReadSchema: struct<date:string,event:string>
      +- BroadcastQueryStage 0
         +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, false]),false), [plan_id=70]
            +- Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
               +- FileScan parquet default.dim_table[date#33,event#34] Batched: false, DataFilters: [isnotnull(event#34), (event#34 = New Year), isnotnull(date#33)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(event), EqualTo(event,New Year), IsNotNull(date)], ReadSchema: struct<date:string,event:string>
】

SparkPlanner(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2@67b3960b) - org.apache.spark.sql.execution.SparkStrategies$SpecialLimits$ -->【
ReturnAnswer
+- Aggregate [date#33 AS date#33#54], [date#33 AS date#33#54]
   +- Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
      +- Relation default.dim_table[date#33,event#34] parquet
】 --> 【
PlanLater Aggregate [date#33 AS date#33#54], [date#33 AS date#33#54]
】

SparkPlanner(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2@67b3960b) - org.apache.spark.sql.execution.SparkStrategies$Aggregation$ -->【
Aggregate [date#33 AS date#33#54], [date#33 AS date#33#54]
+- Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
   +- Relation default.dim_table[date#33,event#34] parquet
】 --> 【
HashAggregate(keys=[date#33#54], functions=[], output=[date#33#54])
+- HashAggregate(keys=[date#33 AS date#33#54], functions=[], output=[date#33#54])
   +- PlanLater Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
】

15:55:06.734 [ScalaTest-run-running-PhysicalPlanSpec] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: IsNotNull(event),EqualTo(event,New Year),IsNotNull(date)
15:55:06.734 [ScalaTest-run-running-PhysicalPlanSpec] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(event#34),(event#34 = New Year),isnotnull(date#33)
15:55:06.734 [ScalaTest-run-running-PhysicalPlanSpec] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<date: string, event: string>
SparkPlanner(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2@67b3960b) - org.apache.spark.sql.execution.datasources.FileSourceStrategy$ -->【
Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
+- Relation default.dim_table[date#33,event#34] parquet
】 --> 【
Project [date#33, event#34]
+- Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
   +- FileScan parquet default.dim_table[date#33,event#34] Batched: false, DataFilters: [isnotnull(event#34), (event#34 = New Year), isnotnull(date#33)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(event), EqualTo(event,New Year), IsNotNull(date)], ReadSchema: struct<date:string,event:string>
】

16:03:49.280 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.execution.adaptive.InsertAdaptiveSparkPlan - Adaptive execution enabled for plan: 
HashAggregate(keys=[date#33#54], functions=[], output=[date#33#54])
+- HashAggregate(keys=[date#33 AS date#33#54], functions=[], output=[date#33#54])
   +- Project [date#33, event#34]
      +- Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
         +- FileScan parquet default.dim_table[date#33,event#34] Batched: false, DataFilters: [isnotnull(event#34), (event#34 = New Year), isnotnull(date#33)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(event), EqualTo(event,New Year), IsNotNull(date)], ReadSchema: struct<date:string,event:string>
 ==>
HashAggregate(keys=[date#33#54], functions=[], output=[date#33#54])
+- HashAggregate(keys=[date#33 AS date#33#54], functions=[], output=[date#33#54])
   +- Project [date#33, event#34]
      +- Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
         +- FileScan parquet default.dim_table[date#33,event#34] Batched: false, DataFilters: [isnotnull(event#34), (event#34 = New Year), isnotnull(date#33)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(event), EqualTo(event,New Year), IsNotNull(date)], ReadSchema: struct<date:string,event:string>

16:03:49.281 [dispatcher-event-loop-1] WARN  org.apache.spark.HeartbeatReceiver - Removing executor driver with no recent heartbeats: 525429 ms exceeds timeout 120000 ms
16:03:49.282 [executor-heartbeater] DEBUG org.apache.spark.executor.ExecutorMetricsPoller - removing (0, 0) from stageTCMP
org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec$ - applyPhysicalRules - org.apache.spark.sql.execution.RemoveRedundantProjects$ -->【
HashAggregate(keys=[date#33#54], functions=[], output=[date#33#54])
+- HashAggregate(keys=[date#33 AS date#33#54], functions=[], output=[date#33#54])
   +- Project [date#33, event#34]
      +- Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
         +- FileScan parquet default.dim_table[date#33,event#34] Batched: false, DataFilters: [isnotnull(event#34), (event#34 = New Year), isnotnull(date#33)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(event), EqualTo(event,New Year), IsNotNull(date)], ReadSchema: struct<date:string,event:string>
】AQE --> 【
HashAggregate(keys=[date#33#54], functions=[], output=[date#33#54])
+- HashAggregate(keys=[date#33 AS date#33#54], functions=[], output=[date#33#54])
   +- Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
      +- FileScan parquet default.dim_table[date#33,event#34] Batched: false, DataFilters: [isnotnull(event#34), (event#34 = New Year), isnotnull(date#33)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(event), EqualTo(event,New Year), IsNotNull(date)], ReadSchema: struct<date:string,event:string>
】

16:03:49.288 [dispatcher-event-loop-1] DEBUG org.apache.spark.HeartbeatReceiver - Received heartbeat from unknown executor driver
org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec$ - applyPhysicalRules - org.apache.spark.sql.execution.exchange.EnsureRequirements -->【
HashAggregate(keys=[date#33#54], functions=[], output=[date#33#54])
+- HashAggregate(keys=[date#33 AS date#33#54], functions=[], output=[date#33#54])
   +- Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
      +- FileScan parquet default.dim_table[date#33,event#34] Batched: false, DataFilters: [isnotnull(event#34), (event#34 = New Year), isnotnull(date#33)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(event), EqualTo(event,New Year), IsNotNull(date)], ReadSchema: struct<date:string,event:string>
】AQE --> 【
HashAggregate(keys=[date#33#54], functions=[], output=[date#33#54])
+- Exchange hashpartitioning(date#33#54, 200), ENSURE_REQUIREMENTS, [plan_id=116]
   +- HashAggregate(keys=[date#33 AS date#33#54], functions=[], output=[date#33#54])
      +- Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
         +- FileScan parquet default.dim_table[date#33,event#34] Batched: false, DataFilters: [isnotnull(event#34), (event#34 = New Year), isnotnull(date#33)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(event), EqualTo(event,New Year), IsNotNull(date)], ReadSchema: struct<date:string,event:string>
】

16:03:49.289 [executor-heartbeater] INFO  org.apache.spark.executor.Executor - Told to re-register on heartbeat
16:03:49.289 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManager - BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None) re-registering with master
16:03:49.289 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.289 [kill-executor-thread] WARN  org.apache.spark.SparkContext - Killing executors is not supported by current scheduler.
16:03:49.289 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.DefaultTopologyMapper - Got a request for 192.168.31.69
16:03:49.292 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.292 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManager - Reporting 6 blocks to the master.
org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec$ final result ==>【
HashAggregate(keys=[date#33#54], functions=[], output=[date#33#54])
+- HashAggregate(keys=[date#33 AS date#33#54], functions=[], output=[date#33#54])
   +- Project [date#33, event#34]
      +- Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
         +- FileScan parquet default.dim_table[date#33,event#34] Batched: false, DataFilters: [isnotnull(event#34), (event#34 = New Year), isnotnull(date#33)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(event), EqualTo(event,New Year), IsNotNull(date)], ReadSchema: struct<date:string,event:string>
】AQE ==> 【
HashAggregate(keys=[date#33#54], functions=[], output=[date#33#54])
+- Exchange hashpartitioning(date#33#54, 200), ENSURE_REQUIREMENTS, [plan_id=116]
   +- HashAggregate(keys=[date#33 AS date#33#54], functions=[], output=[date#33#54])
      +- Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
         +- FileScan parquet default.dim_table[date#33,event#34] Batched: false, DataFilters: [isnotnull(event#34), (event#34 = New Year), isnotnull(date#33)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(event), EqualTo(event,New Year), IsNotNull(date)], ReadSchema: struct<date:string,event:string>
】

org.apache.spark.sql.execution.QueryExecution$ - org.apache.spark.sql.execution.adaptive.InsertAdaptiveSparkPlan -->【
HashAggregate(keys=[date#33#54], functions=[], output=[date#33#54])
+- HashAggregate(keys=[date#33 AS date#33#54], functions=[], output=[date#33#54])
   +- Project [date#33, event#34]
      +- Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
         +- FileScan parquet default.dim_table[date#33,event#34] Batched: false, DataFilters: [isnotnull(event#34), (event#34 = New Year), isnotnull(date#33)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(event), EqualTo(event,New Year), IsNotNull(date)], ReadSchema: struct<date:string,event:string>
】 --> 【
AdaptiveSparkPlan isFinalPlan=false
+- HashAggregate(keys=[date#33#54], functions=[], output=[date#33#54])
   +- Exchange hashpartitioning(date#33#54, 200), ENSURE_REQUIREMENTS, [plan_id=116]
      +- HashAggregate(keys=[date#33 AS date#33#54], functions=[], output=[date#33#54])
         +- Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
            +- FileScan parquet default.dim_table[date#33,event#34] Batched: false, DataFilters: [isnotnull(event#34), (event#34 = New Year), isnotnull(date#33)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(event), EqualTo(event,New Year), IsNotNull(date)], ReadSchema: struct<date:string,event:string>
】

16:03:49.294 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_0_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
org.apache.spark.sql.execution.QueryExecution$ prepareForExecution final result ==>【
HashAggregate(keys=[date#33#54], functions=[], output=[date#33#54])
+- HashAggregate(keys=[date#33 AS date#33#54], functions=[], output=[date#33#54])
   +- Project [date#33, event#34]
      +- Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
         +- FileScan parquet default.dim_table[date#33,event#34] Batched: false, DataFilters: [isnotnull(event#34), (event#34 = New Year), isnotnull(date#33)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(event), EqualTo(event,New Year), IsNotNull(date)], ReadSchema: struct<date:string,event:string>
】 ==> 【
AdaptiveSparkPlan isFinalPlan=false
+- HashAggregate(keys=[date#33#54], functions=[], output=[date#33#54])
   +- Exchange hashpartitioning(date#33#54, 200), ENSURE_REQUIREMENTS, [plan_id=116]
      +- HashAggregate(keys=[date#33 AS date#33#54], functions=[], output=[date#33#54])
         +- Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
            +- FileScan parquet default.dim_table[date#33,event#34] Batched: false, DataFilters: [isnotnull(event#34), (event#34 = New Year), isnotnull(date#33)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(event), EqualTo(event,New Year), IsNotNull(date)], ReadSchema: struct<date:string,event:string>
】

16:03:49.295 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_0_piece0 in memory on 192.168.31.69:58410 (current size: 34.5 KiB, original size: 34.5 KiB, free: 4.1 GiB)
16:03:49.295 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_0_piece0
16:03:49.296 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_2_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.296 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_2_piece0 in memory on 192.168.31.69:58410 (current size: 206.0 B, original size: 206.0 B, free: 4.1 GiB)
16:03:49.296 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_2_piece0
16:03:49.297 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_1_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.298 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_1_piece0 in memory on 192.168.31.69:58410 (current size: 9.1 KiB, original size: 9.1 KiB, free: 4.1 GiB)
16:03:49.298 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_1_piece0
16:03:49.299 [dispatcher-event-loop-0] DEBUG org.apache.spark.HeartbeatReceiver - Received heartbeat from unknown executor driver
16:03:49.299 [executor-heartbeater] INFO  org.apache.spark.executor.Executor - Told to re-register on heartbeat
16:03:49.299 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManager - BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None) re-registering with master
16:03:49.299 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.299 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.DefaultTopologyMapper - Got a request for 192.168.31.69
16:03:49.299 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.299 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManager - Reporting 6 blocks to the master.
16:03:49.299 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_0_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
optimizeQueryStage - org.apache.spark.sql.execution.adaptive.PlanAdaptiveDynamicPruningFilters -> old【
CollectLimit 21
+- Project [cast(id#30 as string) AS id#43, cast(value#31 as string) AS value#44, event#34]
   +- BroadcastHashJoin [date#32], [date#33], Inner, BuildRight, false
      :- FileScan parquet default.fact_table[id#30,value#31,date#32] Batched: false, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(3 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [isnotnull(date#32), dynamicpruningexpression(date#32 IN dynamicpruning#49)], PushedFilters: [], ReadSchema: struct<id:int,value:int>
      :     +- SubqueryAdaptiveBroadcast dynamicpruning#49, 0, false, Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33)), [date#33]
      :        +- AdaptiveSparkPlan isFinalPlan=false
      :           +- Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
      :              +- FileScan parquet default.dim_table[date#33,event#34] Batched: false, DataFilters: [isnotnull(event#34), (event#34 = New Year), isnotnull(date#33)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(event), EqualTo(event,New Year), IsNotNull(date)], ReadSchema: struct<date:string,event:string>
      +- BroadcastQueryStage 0
         +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, false]),false), [plan_id=70]
            +- Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
               +- FileScan parquet default.dim_table[date#33,event#34] Batched: false, DataFilters: [isnotnull(event#34), (event#34 = New Year), isnotnull(date#33)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(event), EqualTo(event,New Year), IsNotNull(date)], ReadSchema: struct<date:string,event:string>
】AQE --> new【
CollectLimit 21
+- Project [cast(id#30 as string) AS id#43, cast(value#31 as string) AS value#44, event#34]
   +- BroadcastHashJoin [date#32], [date#33], Inner, BuildRight, false
      :- FileScan parquet default.fact_table[id#30,value#31,date#32] Batched: false, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(3 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [isnotnull(date#32), dynamicpruningexpression(date#32 IN dynamicpruning#49)], PushedFilters: [], ReadSchema: struct<id:int,value:int>
      :     +- Subquery dynamicpruning#49, [id=#118]
      :        +- AdaptiveSparkPlan isFinalPlan=false
      :           +- HashAggregate(keys=[date#33#54], functions=[], output=[date#33#54])
      :              +- Exchange hashpartitioning(date#33#54, 200), ENSURE_REQUIREMENTS, [plan_id=116]
      :                 +- HashAggregate(keys=[date#33 AS date#33#54], functions=[], output=[date#33#54])
      :                    +- Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
      :                       +- FileScan parquet default.dim_table[date#33,event#34] Batched: false, DataFilters: [isnotnull(event#34), (event#34 = New Year), isnotnull(date#33)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(event), EqualTo(event,New Year), IsNotNull(date)], ReadSchema: struct<date:string,event:string>
      +- BroadcastQueryStage 0
         +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, false]),false), [plan_id=70]
            +- Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
               +- FileScan parquet default.dim_table[date#33,event#34] Batched: false, DataFilters: [isnotnull(event#34), (event#34 = New Year), isnotnull(date#33)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(event), EqualTo(event,New Year), IsNotNull(date)], ReadSchema: struct<date:string,event:string>
】


16:03:49.300 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_0_piece0 in memory on 192.168.31.69:58410 (current size: 34.5 KiB, original size: 34.5 KiB, free: 4.1 GiB)
16:03:49.300 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_0_piece0
16:03:49.300 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_2_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.300 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_2_piece0 in memory on 192.168.31.69:58410 (current size: 206.0 B, original size: 206.0 B, free: 4.1 GiB)
16:03:49.301 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_2_piece0
16:03:49.301 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_1_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.301 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_1_piece0 in memory on 192.168.31.69:58410 (current size: 9.1 KiB, original size: 9.1 KiB, free: 4.1 GiB)
16:03:49.301 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_1_piece0
16:03:49.302 [dispatcher-event-loop-1] DEBUG org.apache.spark.HeartbeatReceiver - Received heartbeat from unknown executor driver
16:03:49.302 [executor-heartbeater] INFO  org.apache.spark.executor.Executor - Told to re-register on heartbeat
16:03:49.302 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManager - BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None) re-registering with master
16:03:49.302 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.302 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.DefaultTopologyMapper - Got a request for 192.168.31.69
16:03:49.303 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.303 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManager - Reporting 6 blocks to the master.
16:03:49.303 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_0_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.303 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_0_piece0 in memory on 192.168.31.69:58410 (current size: 34.5 KiB, original size: 34.5 KiB, free: 4.1 GiB)
16:03:49.303 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_0_piece0
16:03:49.303 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_2_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.304 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_2_piece0 in memory on 192.168.31.69:58410 (current size: 206.0 B, original size: 206.0 B, free: 4.1 GiB)
16:03:49.304 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_2_piece0
16:03:49.304 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_1_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.305 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_1_piece0 in memory on 192.168.31.69:58410 (current size: 9.1 KiB, original size: 9.1 KiB, free: 4.1 GiB)
16:03:49.305 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_1_piece0
16:03:49.305 [dispatcher-event-loop-0] DEBUG org.apache.spark.HeartbeatReceiver - Received heartbeat from unknown executor driver
16:03:49.305 [executor-heartbeater] INFO  org.apache.spark.executor.Executor - Told to re-register on heartbeat
16:03:49.305 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManager - BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None) re-registering with master
16:03:49.305 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.306 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.DefaultTopologyMapper - Got a request for 192.168.31.69
16:03:49.306 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.306 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManager - Reporting 6 blocks to the master.
16:03:49.306 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_0_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.306 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_0_piece0 in memory on 192.168.31.69:58410 (current size: 34.5 KiB, original size: 34.5 KiB, free: 4.1 GiB)
16:03:49.306 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_0_piece0
16:03:49.307 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_2_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.307 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_2_piece0 in memory on 192.168.31.69:58410 (current size: 206.0 B, original size: 206.0 B, free: 4.1 GiB)
16:03:49.307 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_2_piece0
16:03:49.307 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_1_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.308 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_1_piece0 in memory on 192.168.31.69:58410 (current size: 9.1 KiB, original size: 9.1 KiB, free: 4.1 GiB)
16:03:49.308 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_1_piece0
16:03:49.308 [dispatcher-event-loop-1] DEBUG org.apache.spark.HeartbeatReceiver - Received heartbeat from unknown executor driver
16:03:49.308 [executor-heartbeater] INFO  org.apache.spark.executor.Executor - Told to re-register on heartbeat
16:03:49.309 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManager - BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None) re-registering with master
16:03:49.309 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.309 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.DefaultTopologyMapper - Got a request for 192.168.31.69
16:03:49.309 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.309 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManager - Reporting 6 blocks to the master.
16:03:49.309 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_0_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.309 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_0_piece0 in memory on 192.168.31.69:58410 (current size: 34.5 KiB, original size: 34.5 KiB, free: 4.1 GiB)
16:03:49.310 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_0_piece0
16:03:49.310 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_2_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.310 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_2_piece0 in memory on 192.168.31.69:58410 (current size: 206.0 B, original size: 206.0 B, free: 4.1 GiB)
16:03:49.310 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_2_piece0
16:03:49.310 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_1_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.311 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_1_piece0 in memory on 192.168.31.69:58410 (current size: 9.1 KiB, original size: 9.1 KiB, free: 4.1 GiB)
16:03:49.311 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_1_piece0
16:03:49.311 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec - withFinalPlanUpdate.executeCollect ==> after get final plan: 
CollectLimit 21
+- Project [cast(id#30 as string) AS id#43, cast(value#31 as string) AS value#44, event#34]
   +- BroadcastHashJoin [date#32], [date#33], Inner, BuildRight, false
      :- FileScan parquet default.fact_table[id#30,value#31,date#32] Batched: false, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(3 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [isnotnull(date#32), dynamicpruningexpression(date#32 IN dynamicpruning#49)], PushedFilters: [], ReadSchema: struct<id:int,value:int>
      :     +- Subquery dynamicpruning#49, [id=#118]
      :        +- AdaptiveSparkPlan isFinalPlan=false
      :           +- HashAggregate(keys=[date#33#54], functions=[], output=[date#33#54])
      :              +- Exchange hashpartitioning(date#33#54, 200), ENSURE_REQUIREMENTS, [plan_id=116]
      :                 +- HashAggregate(keys=[date#33 AS date#33#54], functions=[], output=[date#33#54])
      :                    +- Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
      :                       +- FileScan parquet default.dim_table[date#33,event#34] Batched: false, DataFilters: [isnotnull(event#34), (event#34 = New Year), isnotnull(date#33)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(event), EqualTo(event,New Year), IsNotNull(date)], ReadSchema: struct<date:string,event:string>
      +- BroadcastQueryStage 0
         +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, false]),false), [plan_id=70]
            +- Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
               +- FileScan parquet default.dim_table[date#33,event#34] Batched: false, DataFilters: [isnotnull(event#34), (event#34 = New Year), isnotnull(date#33)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(event), EqualTo(event,New Year), IsNotNull(date)], ReadSchema: struct<date:string,event:string>

16:03:49.311 [dispatcher-event-loop-0] DEBUG org.apache.spark.HeartbeatReceiver - Received heartbeat from unknown executor driver
16:03:49.311 [executor-heartbeater] INFO  org.apache.spark.executor.Executor - Told to re-register on heartbeat
16:03:49.311 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManager - BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None) re-registering with master
16:03:49.312 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.312 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.DefaultTopologyMapper - Got a request for 192.168.31.69
16:03:49.312 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.312 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManager - Reporting 6 blocks to the master.
16:03:49.312 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_0_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.313 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_0_piece0 in memory on 192.168.31.69:58410 (current size: 34.5 KiB, original size: 34.5 KiB, free: 4.1 GiB)
16:03:49.313 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_0_piece0
16:03:49.313 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_2_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.314 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_2_piece0 in memory on 192.168.31.69:58410 (current size: 206.0 B, original size: 206.0 B, free: 4.1 GiB)
16:03:49.314 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_2_piece0
16:03:49.314 [subquery-0] DEBUG org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec - withFinalPlanUpdate.executeCollect ==> before get final plan: 
HashAggregate(keys=[date#33#54], functions=[], output=[date#33#54])
+- Exchange hashpartitioning(date#33#54, 200), ENSURE_REQUIREMENTS, [plan_id=116]
   +- HashAggregate(keys=[date#33 AS date#33#54], functions=[], output=[date#33#54])
      +- Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
         +- FileScan parquet default.dim_table[date#33,event#34] Batched: false, DataFilters: [isnotnull(event#34), (event#34 = New Year), isnotnull(date#33)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(event), EqualTo(event,New Year), IsNotNull(date)], ReadSchema: struct<date:string,event:string>

16:03:49.314 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_1_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.315 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_1_piece0 in memory on 192.168.31.69:58410 (current size: 9.1 KiB, original size: 9.1 KiB, free: 4.1 GiB)
16:03:49.315 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_1_piece0
16:03:49.315 [dispatcher-event-loop-1] DEBUG org.apache.spark.HeartbeatReceiver - Received heartbeat from unknown executor driver
16:03:49.316 [executor-heartbeater] INFO  org.apache.spark.executor.Executor - Told to re-register on heartbeat
16:03:49.316 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManager - BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None) re-registering with master
16:03:49.316 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.316 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.DefaultTopologyMapper - Got a request for 192.168.31.69
16:03:49.316 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.316 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManager - Reporting 6 blocks to the master.
16:03:49.316 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_0_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.317 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_0_piece0 in memory on 192.168.31.69:58410 (current size: 34.5 KiB, original size: 34.5 KiB, free: 4.1 GiB)
16:03:49.317 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_0_piece0
16:03:49.317 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_2_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.317 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_2_piece0 in memory on 192.168.31.69:58410 (current size: 206.0 B, original size: 206.0 B, free: 4.1 GiB)
16:03:49.317 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_2_piece0
16:03:49.318 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_1_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.318 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_1_piece0 in memory on 192.168.31.69:58410 (current size: 9.1 KiB, original size: 9.1 KiB, free: 4.1 GiB)
16:03:49.318 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_1_piece0
16:03:49.318 [dispatcher-event-loop-0] DEBUG org.apache.spark.HeartbeatReceiver - Received heartbeat from unknown executor driver
16:03:49.319 [executor-heartbeater] INFO  org.apache.spark.executor.Executor - Told to re-register on heartbeat
16:03:49.319 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManager - BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None) re-registering with master
16:03:49.319 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.319 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.DefaultTopologyMapper - Got a request for 192.168.31.69
16:03:49.319 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.319 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManager - Reporting 6 blocks to the master.
16:03:49.319 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_0_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.319 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_0_piece0 in memory on 192.168.31.69:58410 (current size: 34.5 KiB, original size: 34.5 KiB, free: 4.1 GiB)
16:03:49.319 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_0_piece0
16:03:49.320 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_2_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.320 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_2_piece0 in memory on 192.168.31.69:58410 (current size: 206.0 B, original size: 206.0 B, free: 4.1 GiB)
16:03:49.320 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_2_piece0
16:03:49.321 [subquery-0] DEBUG org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec - Materialize query stage ShuffleQueryStageExec: 0, query stage plan:
Exchange hashpartitioning(date#33#54, 200), ENSURE_REQUIREMENTS, [plan_id=116]
+- HashAggregate(keys=[date#33 AS date#33#54], functions=[], output=[date#33#54])
   +- Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
      +- FileScan parquet default.dim_table[date#33,event#34] Batched: false, DataFilters: [isnotnull(event#34), (event#34 = New Year), isnotnull(date#33)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(event), EqualTo(event,New Year), IsNotNull(date)], ReadSchema: struct<date:string,event:string>

16:03:49.321 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_1_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.321 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_1_piece0 in memory on 192.168.31.69:58410 (current size: 9.1 KiB, original size: 9.1 KiB, free: 4.1 GiB)
16:03:49.321 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_1_piece0
16:03:49.322 [dispatcher-event-loop-1] DEBUG org.apache.spark.HeartbeatReceiver - Received heartbeat from unknown executor driver
16:03:49.323 [executor-heartbeater] INFO  org.apache.spark.executor.Executor - Told to re-register on heartbeat
16:03:49.323 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManager - BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None) re-registering with master
16:03:49.323 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.323 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.DefaultTopologyMapper - Got a request for 192.168.31.69
16:03:49.323 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.323 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManager - Reporting 6 blocks to the master.
16:03:49.323 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_0_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.324 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_0_piece0 in memory on 192.168.31.69:58410 (current size: 34.5 KiB, original size: 34.5 KiB, free: 4.1 GiB)
16:03:49.324 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_0_piece0
16:03:49.324 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_2_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.325 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_2_piece0 in memory on 192.168.31.69:58410 (current size: 206.0 B, original size: 206.0 B, free: 4.1 GiB)
16:03:49.325 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_2_piece0
16:03:49.326 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_1_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.326 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_1_piece0 in memory on 192.168.31.69:58410 (current size: 9.1 KiB, original size: 9.1 KiB, free: 4.1 GiB)
16:03:49.326 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_1_piece0
16:03:49.326 [dispatcher-event-loop-0] DEBUG org.apache.spark.HeartbeatReceiver - Received heartbeat from unknown executor driver
16:03:49.327 [executor-heartbeater] INFO  org.apache.spark.executor.Executor - Told to re-register on heartbeat
16:03:49.327 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManager - BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None) re-registering with master
16:03:49.327 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.327 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.DefaultTopologyMapper - Got a request for 192.168.31.69
16:03:49.327 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.327 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManager - Reporting 7 blocks to the master.
16:03:49.327 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_0_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.328 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_0_piece0 in memory on 192.168.31.69:58410 (current size: 34.5 KiB, original size: 34.5 KiB, free: 4.1 GiB)
16:03:49.328 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_0_piece0
16:03:49.328 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_2_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.329 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_2_piece0 in memory on 192.168.31.69:58410 (current size: 206.0 B, original size: 206.0 B, free: 4.1 GiB)
16:03:49.329 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_2_piece0
16:03:49.329 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_1_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.329 [subquery-0] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_3 stored as values in memory (estimated size 359.3 KiB, free 4.0 GiB)
16:03:49.330 [subquery-0] DEBUG org.apache.spark.storage.BlockManager - Put block broadcast_3 locally took 4 ms
16:03:49.330 [subquery-0] DEBUG org.apache.spark.storage.BlockManager - Putting block broadcast_3 without replication took 4 ms
16:03:49.330 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_1_piece0 in memory on 192.168.31.69:58410 (current size: 9.1 KiB, original size: 9.1 KiB, free: 4.1 GiB)
16:03:49.330 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_1_piece0
16:03:49.330 [dispatcher-event-loop-1] DEBUG org.apache.spark.HeartbeatReceiver - Received heartbeat from unknown executor driver
16:03:49.330 [executor-heartbeater] INFO  org.apache.spark.executor.Executor - Told to re-register on heartbeat
16:03:49.330 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManager - BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None) re-registering with master
16:03:49.330 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.330 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.DefaultTopologyMapper - Got a request for 192.168.31.69
16:03:49.331 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.331 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManager - Reporting 7 blocks to the master.
16:03:49.331 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_0_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.331 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_0_piece0 in memory on 192.168.31.69:58410 (current size: 34.5 KiB, original size: 34.5 KiB, free: 4.1 GiB)
16:03:49.331 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_0_piece0
16:03:49.331 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_2_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.332 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_2_piece0 in memory on 192.168.31.69:58410 (current size: 206.0 B, original size: 206.0 B, free: 4.1 GiB)
16:03:49.332 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_2_piece0
16:03:49.332 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_1_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.332 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_1_piece0 in memory on 192.168.31.69:58410 (current size: 9.1 KiB, original size: 9.1 KiB, free: 4.1 GiB)
16:03:49.332 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_1_piece0
16:03:49.333 [dispatcher-event-loop-0] DEBUG org.apache.spark.HeartbeatReceiver - Received heartbeat from unknown executor driver
16:03:49.333 [executor-heartbeater] INFO  org.apache.spark.executor.Executor - Told to re-register on heartbeat
16:03:49.333 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManager - BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None) re-registering with master
16:03:49.333 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.333 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.DefaultTopologyMapper - Got a request for 192.168.31.69
16:03:49.333 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.333 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManager - Reporting 7 blocks to the master.
16:03:49.333 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_0_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.334 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_0_piece0 in memory on 192.168.31.69:58410 (current size: 34.5 KiB, original size: 34.5 KiB, free: 4.1 GiB)
16:03:49.334 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_0_piece0
16:03:49.334 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_2_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.334 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_2_piece0 in memory on 192.168.31.69:58410 (current size: 206.0 B, original size: 206.0 B, free: 4.1 GiB)
16:03:49.334 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_2_piece0
16:03:49.335 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_1_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.335 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_1_piece0 in memory on 192.168.31.69:58410 (current size: 9.1 KiB, original size: 9.1 KiB, free: 4.1 GiB)
16:03:49.335 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_1_piece0
16:03:49.335 [dispatcher-event-loop-1] DEBUG org.apache.spark.HeartbeatReceiver - Received heartbeat from unknown executor driver
16:03:49.335 [executor-heartbeater] INFO  org.apache.spark.executor.Executor - Told to re-register on heartbeat
16:03:49.335 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManager - BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None) re-registering with master
16:03:49.335 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.336 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.DefaultTopologyMapper - Got a request for 192.168.31.69
16:03:49.336 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.336 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManager - Reporting 7 blocks to the master.
16:03:49.336 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_0_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.336 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_0_piece0 in memory on 192.168.31.69:58410 (current size: 34.5 KiB, original size: 34.5 KiB, free: 4.1 GiB)
16:03:49.336 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_0_piece0
16:03:49.337 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_2_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.337 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_2_piece0 in memory on 192.168.31.69:58410 (current size: 206.0 B, original size: 206.0 B, free: 4.1 GiB)
16:03:49.337 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_2_piece0
16:03:49.337 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_1_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.337 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_1_piece0 in memory on 192.168.31.69:58410 (current size: 9.1 KiB, original size: 9.1 KiB, free: 4.1 GiB)
16:03:49.337 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_1_piece0
16:03:49.338 [dispatcher-event-loop-0] DEBUG org.apache.spark.HeartbeatReceiver - Received heartbeat from unknown executor driver
16:03:49.338 [executor-heartbeater] INFO  org.apache.spark.executor.Executor - Told to re-register on heartbeat
16:03:49.338 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManager - BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None) re-registering with master
16:03:49.338 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.338 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.DefaultTopologyMapper - Got a request for 192.168.31.69
16:03:49.338 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.338 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManager - Reporting 7 blocks to the master.
16:03:49.338 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_0_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.339 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_0_piece0 in memory on 192.168.31.69:58410 (current size: 34.5 KiB, original size: 34.5 KiB, free: 4.1 GiB)
16:03:49.339 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_0_piece0
16:03:49.339 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_2_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.339 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_2_piece0 in memory on 192.168.31.69:58410 (current size: 206.0 B, original size: 206.0 B, free: 4.1 GiB)
16:03:49.339 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_2_piece0
16:03:49.340 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_1_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.340 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_1_piece0 in memory on 192.168.31.69:58410 (current size: 9.1 KiB, original size: 9.1 KiB, free: 4.1 GiB)
16:03:49.340 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_1_piece0
16:03:49.340 [dispatcher-event-loop-1] DEBUG org.apache.spark.HeartbeatReceiver - Received heartbeat from unknown executor driver
16:03:49.341 [executor-heartbeater] INFO  org.apache.spark.executor.Executor - Told to re-register on heartbeat
16:03:49.341 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManager - BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None) re-registering with master
16:03:49.341 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.341 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.DefaultTopologyMapper - Got a request for 192.168.31.69
16:03:49.341 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.341 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManager - Reporting 7 blocks to the master.
16:03:49.341 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_0_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.341 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_0_piece0 in memory on 192.168.31.69:58410 (current size: 34.5 KiB, original size: 34.5 KiB, free: 4.1 GiB)
16:03:49.342 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_0_piece0
16:03:49.342 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_2_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.342 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_2_piece0 in memory on 192.168.31.69:58410 (current size: 206.0 B, original size: 206.0 B, free: 4.1 GiB)
16:03:49.342 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_2_piece0
16:03:49.342 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_1_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.343 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_1_piece0 in memory on 192.168.31.69:58410 (current size: 9.1 KiB, original size: 9.1 KiB, free: 4.1 GiB)
16:03:49.343 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_1_piece0
16:03:49.343 [dispatcher-event-loop-0] DEBUG org.apache.spark.HeartbeatReceiver - Received heartbeat from unknown executor driver
16:03:49.343 [executor-heartbeater] INFO  org.apache.spark.executor.Executor - Told to re-register on heartbeat
16:03:49.343 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManager - BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None) re-registering with master
16:03:49.343 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.343 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.DefaultTopologyMapper - Got a request for 192.168.31.69
16:03:49.344 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.344 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManager - Reporting 7 blocks to the master.
16:03:49.344 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_0_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.344 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_0_piece0 in memory on 192.168.31.69:58410 (current size: 34.5 KiB, original size: 34.5 KiB, free: 4.1 GiB)
16:03:49.344 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_0_piece0
16:03:49.345 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_2_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.345 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_2_piece0 in memory on 192.168.31.69:58410 (current size: 206.0 B, original size: 206.0 B, free: 4.1 GiB)
16:03:49.345 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_2_piece0
16:03:49.345 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_1_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.346 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_1_piece0 in memory on 192.168.31.69:58410 (current size: 9.1 KiB, original size: 9.1 KiB, free: 4.1 GiB)
16:03:49.346 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_1_piece0
16:03:49.347 [dispatcher-event-loop-1] DEBUG org.apache.spark.HeartbeatReceiver - Received heartbeat from unknown executor driver
16:03:49.347 [executor-heartbeater] INFO  org.apache.spark.executor.Executor - Told to re-register on heartbeat
16:03:49.347 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManager - BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None) re-registering with master
16:03:49.347 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.347 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.DefaultTopologyMapper - Got a request for 192.168.31.69
16:03:49.347 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.348 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManager - Reporting 7 blocks to the master.
16:03:49.348 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_0_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.349 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_0_piece0 in memory on 192.168.31.69:58410 (current size: 34.5 KiB, original size: 34.5 KiB, free: 4.1 GiB)
16:03:49.349 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_0_piece0
16:03:49.349 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_2_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.349 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_2_piece0 in memory on 192.168.31.69:58410 (current size: 206.0 B, original size: 206.0 B, free: 4.1 GiB)
16:03:49.350 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_2_piece0
16:03:49.350 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_1_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.350 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_1_piece0 in memory on 192.168.31.69:58410 (current size: 9.1 KiB, original size: 9.1 KiB, free: 4.1 GiB)
16:03:49.351 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_1_piece0
16:03:49.351 [dispatcher-event-loop-0] DEBUG org.apache.spark.HeartbeatReceiver - Received heartbeat from unknown executor driver
16:03:49.351 [executor-heartbeater] INFO  org.apache.spark.executor.Executor - Told to re-register on heartbeat
16:03:49.351 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManager - BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None) re-registering with master
16:03:49.351 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.351 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.DefaultTopologyMapper - Got a request for 192.168.31.69
16:03:49.351 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.351 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManager - Reporting 8 blocks to the master.
16:03:49.351 [subquery-0] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_3_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 4.0 GiB)
16:03:49.352 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_0_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.352 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_0_piece0 in memory on 192.168.31.69:58410 (current size: 34.5 KiB, original size: 34.5 KiB, free: 4.1 GiB)
16:03:49.352 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_0_piece0
16:03:49.352 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_3_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.352 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_3_piece0 in memory on 192.168.31.69:58410 (size: 34.5 KiB, free: 4.1 GiB)
16:03:49.352 [subquery-0] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_3_piece0
16:03:49.352 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_2_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.352 [subquery-0] DEBUG org.apache.spark.storage.BlockManager - Told master about block broadcast_3_piece0
16:03:49.352 [subquery-0] DEBUG org.apache.spark.storage.BlockManager - Put block broadcast_3_piece0 locally took 1 ms
16:03:49.352 [subquery-0] DEBUG org.apache.spark.storage.BlockManager - Putting block broadcast_3_piece0 without replication took 1 ms
16:03:49.353 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_2_piece0 in memory on 192.168.31.69:58410 (current size: 206.0 B, original size: 206.0 B, free: 4.1 GiB)
16:03:49.353 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_2_piece0
16:03:49.353 [subquery-0] INFO  org.apache.spark.SparkContext - Created broadcast 3 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266
16:03:49.353 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_1_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.353 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_1_piece0 in memory on 192.168.31.69:58410 (current size: 9.1 KiB, original size: 9.1 KiB, free: 4.1 GiB)
16:03:49.353 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_1_piece0
16:03:49.354 [subquery-0] INFO  org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 4195093 bytes, open cost is considered as scanning 4194304 bytes.
16:03:49.354 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_3_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.354 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_3_piece0 in memory on 192.168.31.69:58410 (current size: 34.5 KiB, original size: 34.5 KiB, free: 4.1 GiB)
16:03:49.354 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_3_piece0
16:03:49.355 [dispatcher-event-loop-1] DEBUG org.apache.spark.HeartbeatReceiver - Received heartbeat from unknown executor driver
16:03:49.355 [executor-heartbeater] INFO  org.apache.spark.executor.Executor - Told to re-register on heartbeat
16:03:49.355 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManager - BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None) re-registering with master
16:03:49.355 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.355 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.DefaultTopologyMapper - Got a request for 192.168.31.69
16:03:49.355 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.355 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManager - Reporting 8 blocks to the master.
16:03:49.355 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_0_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.356 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_0_piece0 in memory on 192.168.31.69:58410 (current size: 34.5 KiB, original size: 34.5 KiB, free: 4.1 GiB)
16:03:49.356 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_0_piece0
16:03:49.356 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_2_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.356 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_2_piece0 in memory on 192.168.31.69:58410 (current size: 206.0 B, original size: 206.0 B, free: 4.1 GiB)
16:03:49.356 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_2_piece0
16:03:49.356 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_1_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.357 [subquery-0] DEBUG org.apache.spark.util.ClosureCleaner - Cleaning indylambda closure: $anonfun$doExecute$1$adapted
16:03:49.357 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_1_piece0 in memory on 192.168.31.69:58410 (current size: 9.1 KiB, original size: 9.1 KiB, free: 4.1 GiB)
16:03:49.357 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_1_piece0
16:03:49.357 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_3_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.357 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_3_piece0 in memory on 192.168.31.69:58410 (current size: 34.5 KiB, original size: 34.5 KiB, free: 4.1 GiB)
16:03:49.357 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_3_piece0
16:03:49.358 [dispatcher-event-loop-0] DEBUG org.apache.spark.HeartbeatReceiver - Received heartbeat from unknown executor driver
16:03:49.358 [executor-heartbeater] INFO  org.apache.spark.executor.Executor - Told to re-register on heartbeat
16:03:49.358 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManager - BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None) re-registering with master
16:03:49.358 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.358 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.DefaultTopologyMapper - Got a request for 192.168.31.69
16:03:49.358 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.358 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManager - Reporting 8 blocks to the master.
16:03:49.358 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_0_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.359 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_0_piece0 in memory on 192.168.31.69:58410 (current size: 34.5 KiB, original size: 34.5 KiB, free: 4.1 GiB)
16:03:49.359 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_0_piece0
16:03:49.359 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_2_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.359 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_2_piece0 in memory on 192.168.31.69:58410 (current size: 206.0 B, original size: 206.0 B, free: 4.1 GiB)
16:03:49.359 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_2_piece0
16:03:49.360 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_1_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.360 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_1_piece0 in memory on 192.168.31.69:58410 (current size: 9.1 KiB, original size: 9.1 KiB, free: 4.1 GiB)
16:03:49.360 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_1_piece0
16:03:49.360 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_3_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.360 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_3_piece0 in memory on 192.168.31.69:58410 (current size: 34.5 KiB, original size: 34.5 KiB, free: 4.1 GiB)
16:03:49.360 [subquery-0] DEBUG org.apache.spark.util.ClosureCleaner -  +++ indylambda closure ($anonfun$doExecute$1$adapted) is now cleaned +++
16:03:49.360 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_3_piece0
16:03:49.361 [dispatcher-event-loop-1] DEBUG org.apache.spark.HeartbeatReceiver - Received heartbeat from unknown executor driver
16:03:49.361 [executor-heartbeater] INFO  org.apache.spark.executor.Executor - Told to re-register on heartbeat
16:03:49.361 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManager - BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None) re-registering with master
16:03:49.361 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.361 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.DefaultTopologyMapper - Got a request for 192.168.31.69
16:03:49.361 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.361 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManager - Reporting 8 blocks to the master.
16:03:49.361 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_0_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.362 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_0_piece0 in memory on 192.168.31.69:58410 (current size: 34.5 KiB, original size: 34.5 KiB, free: 4.1 GiB)
16:03:49.362 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_0_piece0
16:03:49.362 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_2_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.362 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_2_piece0 in memory on 192.168.31.69:58410 (current size: 206.0 B, original size: 206.0 B, free: 4.1 GiB)
16:03:49.362 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_2_piece0
16:03:49.363 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_1_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.363 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_1_piece0 in memory on 192.168.31.69:58410 (current size: 9.1 KiB, original size: 9.1 KiB, free: 4.1 GiB)
16:03:49.363 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_1_piece0
16:03:49.363 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_3_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.364 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_3_piece0 in memory on 192.168.31.69:58410 (current size: 34.5 KiB, original size: 34.5 KiB, free: 4.1 GiB)
16:03:49.364 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_3_piece0
16:03:49.364 [dispatcher-event-loop-0] DEBUG org.apache.spark.HeartbeatReceiver - Received heartbeat from unknown executor driver
16:03:49.364 [executor-heartbeater] INFO  org.apache.spark.executor.Executor - Told to re-register on heartbeat
16:03:49.364 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManager - BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None) re-registering with master
16:03:49.364 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.364 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.DefaultTopologyMapper - Got a request for 192.168.31.69
16:03:49.365 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.365 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManager - Reporting 8 blocks to the master.
16:03:49.365 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_0_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.365 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_0_piece0 in memory on 192.168.31.69:58410 (current size: 34.5 KiB, original size: 34.5 KiB, free: 4.1 GiB)
16:03:49.365 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_0_piece0
16:03:49.365 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_2_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.365 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_2_piece0 in memory on 192.168.31.69:58410 (current size: 206.0 B, original size: 206.0 B, free: 4.1 GiB)
16:03:49.365 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_2_piece0
16:03:49.366 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_1_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.366 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_1_piece0 in memory on 192.168.31.69:58410 (current size: 9.1 KiB, original size: 9.1 KiB, free: 4.1 GiB)
16:03:49.366 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_1_piece0
16:03:49.366 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_3_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.366 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_3_piece0 in memory on 192.168.31.69:58410 (current size: 34.5 KiB, original size: 34.5 KiB, free: 4.1 GiB)
16:03:49.366 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_3_piece0
16:03:49.366 [dispatcher-event-loop-1] DEBUG org.apache.spark.HeartbeatReceiver - Received heartbeat from unknown executor driver
16:03:49.366 [executor-heartbeater] INFO  org.apache.spark.executor.Executor - Told to re-register on heartbeat
16:03:49.366 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManager - BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None) re-registering with master
16:03:49.366 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.367 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.DefaultTopologyMapper - Got a request for 192.168.31.69
16:03:49.367 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.367 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManager - Reporting 8 blocks to the master.
16:03:49.367 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_0_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.367 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_0_piece0 in memory on 192.168.31.69:58410 (current size: 34.5 KiB, original size: 34.5 KiB, free: 4.1 GiB)
16:03:49.367 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_0_piece0
16:03:49.367 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_2_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.367 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_2_piece0 in memory on 192.168.31.69:58410 (current size: 206.0 B, original size: 206.0 B, free: 4.1 GiB)
16:03:49.368 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_2_piece0
16:03:49.368 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_1_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.368 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_1_piece0 in memory on 192.168.31.69:58410 (current size: 9.1 KiB, original size: 9.1 KiB, free: 4.1 GiB)
16:03:49.368 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_1_piece0
16:03:49.368 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_3_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.368 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_3_piece0 in memory on 192.168.31.69:58410 (current size: 34.5 KiB, original size: 34.5 KiB, free: 4.1 GiB)
16:03:49.369 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_3_piece0
16:03:49.369 [dispatcher-event-loop-0] DEBUG org.apache.spark.HeartbeatReceiver - Received heartbeat from unknown executor driver
16:03:49.369 [executor-heartbeater] INFO  org.apache.spark.executor.Executor - Told to re-register on heartbeat
16:03:49.369 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManager - BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None) re-registering with master
16:03:49.369 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.369 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.DefaultTopologyMapper - Got a request for 192.168.31.69
16:03:49.369 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.369 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManager - Reporting 8 blocks to the master.
16:03:49.369 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_0_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.370 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_0_piece0 in memory on 192.168.31.69:58410 (current size: 34.5 KiB, original size: 34.5 KiB, free: 4.1 GiB)
16:03:49.370 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_0_piece0
16:03:49.370 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_2_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.370 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_2_piece0 in memory on 192.168.31.69:58410 (current size: 206.0 B, original size: 206.0 B, free: 4.1 GiB)
16:03:49.370 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_2_piece0
16:03:49.371 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_1_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.371 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_1_piece0 in memory on 192.168.31.69:58410 (current size: 9.1 KiB, original size: 9.1 KiB, free: 4.1 GiB)
16:03:49.371 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_1_piece0
16:03:49.371 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_3_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.371 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_3_piece0 in memory on 192.168.31.69:58410 (current size: 34.5 KiB, original size: 34.5 KiB, free: 4.1 GiB)
16:03:49.371 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_3_piece0
16:03:49.372 [dispatcher-event-loop-1] DEBUG org.apache.spark.HeartbeatReceiver - Received heartbeat from unknown executor driver
16:03:49.372 [executor-heartbeater] INFO  org.apache.spark.executor.Executor - Told to re-register on heartbeat
16:03:49.372 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManager - BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None) re-registering with master
16:03:49.372 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.372 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.DefaultTopologyMapper - Got a request for 192.168.31.69
16:03:49.372 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.372 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManager - Reporting 8 blocks to the master.
16:03:49.372 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_0_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.373 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_0_piece0 in memory on 192.168.31.69:58410 (current size: 34.5 KiB, original size: 34.5 KiB, free: 4.1 GiB)
16:03:49.373 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_0_piece0
16:03:49.373 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_2_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.374 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_2_piece0 in memory on 192.168.31.69:58410 (current size: 206.0 B, original size: 206.0 B, free: 4.1 GiB)
16:03:49.374 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_2_piece0
16:03:49.374 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_1_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.374 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_1_piece0 in memory on 192.168.31.69:58410 (current size: 9.1 KiB, original size: 9.1 KiB, free: 4.1 GiB)
16:03:49.374 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_1_piece0
16:03:49.374 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_3_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.375 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_3_piece0 in memory on 192.168.31.69:58410 (current size: 34.5 KiB, original size: 34.5 KiB, free: 4.1 GiB)
16:03:49.375 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_3_piece0
16:03:49.375 [dispatcher-event-loop-0] DEBUG org.apache.spark.HeartbeatReceiver - Received heartbeat from unknown executor driver
16:03:49.375 [executor-heartbeater] INFO  org.apache.spark.executor.Executor - Told to re-register on heartbeat
16:03:49.375 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManager - BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None) re-registering with master
16:03:49.375 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.376 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.DefaultTopologyMapper - Got a request for 192.168.31.69
16:03:49.376 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.376 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManager - Reporting 8 blocks to the master.
16:03:49.376 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_0_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.376 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_0_piece0 in memory on 192.168.31.69:58410 (current size: 34.5 KiB, original size: 34.5 KiB, free: 4.1 GiB)
16:03:49.376 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_0_piece0
16:03:49.376 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_2_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.377 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_2_piece0 in memory on 192.168.31.69:58410 (current size: 206.0 B, original size: 206.0 B, free: 4.1 GiB)
16:03:49.377 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_2_piece0
16:03:49.377 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_1_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.377 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_1_piece0 in memory on 192.168.31.69:58410 (current size: 9.1 KiB, original size: 9.1 KiB, free: 4.1 GiB)
16:03:49.378 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_1_piece0
16:03:49.378 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_3_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.379 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_3_piece0 in memory on 192.168.31.69:58410 (current size: 34.5 KiB, original size: 34.5 KiB, free: 4.1 GiB)
16:03:49.379 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_3_piece0
16:03:49.380 [dispatcher-event-loop-1] DEBUG org.apache.spark.HeartbeatReceiver - Received heartbeat from unknown executor driver
16:03:49.380 [executor-heartbeater] INFO  org.apache.spark.executor.Executor - Told to re-register on heartbeat
16:03:49.380 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManager - BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None) re-registering with master
16:03:49.380 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.380 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.DefaultTopologyMapper - Got a request for 192.168.31.69
16:03:49.380 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.380 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManager - Reporting 8 blocks to the master.
16:03:49.380 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_0_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.380 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_0_piece0 in memory on 192.168.31.69:58410 (current size: 34.5 KiB, original size: 34.5 KiB, free: 4.1 GiB)
16:03:49.381 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_0_piece0
16:03:49.381 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_2_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.381 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_2_piece0 in memory on 192.168.31.69:58410 (current size: 206.0 B, original size: 206.0 B, free: 4.1 GiB)
16:03:49.381 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_2_piece0
16:03:49.381 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_1_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.381 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_1_piece0 in memory on 192.168.31.69:58410 (current size: 9.1 KiB, original size: 9.1 KiB, free: 4.1 GiB)
16:03:49.381 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_1_piece0
16:03:49.381 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_3_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.382 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_3_piece0 in memory on 192.168.31.69:58410 (current size: 34.5 KiB, original size: 34.5 KiB, free: 4.1 GiB)
16:03:49.382 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_3_piece0
16:03:49.382 [dispatcher-event-loop-0] DEBUG org.apache.spark.HeartbeatReceiver - Received heartbeat from unknown executor driver
16:03:49.382 [executor-heartbeater] INFO  org.apache.spark.executor.Executor - Told to re-register on heartbeat
16:03:49.382 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManager - BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None) re-registering with master
16:03:49.382 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.382 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.DefaultTopologyMapper - Got a request for 192.168.31.69
16:03:49.382 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.382 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManager - Reporting 8 blocks to the master.
16:03:49.382 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_0_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.383 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_0_piece0 in memory on 192.168.31.69:58410 (current size: 34.5 KiB, original size: 34.5 KiB, free: 4.1 GiB)
16:03:49.383 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_0_piece0
16:03:49.383 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_2_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.383 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_2_piece0 in memory on 192.168.31.69:58410 (current size: 206.0 B, original size: 206.0 B, free: 4.1 GiB)
16:03:49.383 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_2_piece0
16:03:49.383 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_1_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.384 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_1_piece0 in memory on 192.168.31.69:58410 (current size: 9.1 KiB, original size: 9.1 KiB, free: 4.1 GiB)
16:03:49.384 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_1_piece0
16:03:49.384 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_3_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.384 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_3_piece0 in memory on 192.168.31.69:58410 (current size: 34.5 KiB, original size: 34.5 KiB, free: 4.1 GiB)
16:03:49.384 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_3_piece0
16:03:49.385 [dispatcher-event-loop-1] DEBUG org.apache.spark.HeartbeatReceiver - Received heartbeat from unknown executor driver
16:03:49.385 [executor-heartbeater] INFO  org.apache.spark.executor.Executor - Told to re-register on heartbeat
16:03:49.385 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManager - BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None) re-registering with master
16:03:49.385 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.385 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.DefaultTopologyMapper - Got a request for 192.168.31.69
16:03:49.385 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.385 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManager - Reporting 8 blocks to the master.
16:03:49.385 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_0_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.386 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_0_piece0 in memory on 192.168.31.69:58410 (current size: 34.5 KiB, original size: 34.5 KiB, free: 4.1 GiB)
16:03:49.386 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_0_piece0
16:03:49.386 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_2_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.386 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_2_piece0 in memory on 192.168.31.69:58410 (current size: 206.0 B, original size: 206.0 B, free: 4.1 GiB)
16:03:49.386 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_2_piece0
16:03:49.386 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_1_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.387 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_1_piece0 in memory on 192.168.31.69:58410 (current size: 9.1 KiB, original size: 9.1 KiB, free: 4.1 GiB)
16:03:49.387 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_1_piece0
16:03:49.387 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_3_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.387 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_3_piece0 in memory on 192.168.31.69:58410 (current size: 34.5 KiB, original size: 34.5 KiB, free: 4.1 GiB)
16:03:49.387 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_3_piece0
16:03:49.388 [dispatcher-event-loop-0] DEBUG org.apache.spark.HeartbeatReceiver - Received heartbeat from unknown executor driver
16:03:49.388 [executor-heartbeater] INFO  org.apache.spark.executor.Executor - Told to re-register on heartbeat
16:03:49.388 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManager - BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None) re-registering with master
16:03:49.388 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.388 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.DefaultTopologyMapper - Got a request for 192.168.31.69
16:03:49.388 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.388 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManager - Reporting 8 blocks to the master.
16:03:49.388 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_0_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.388 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_0_piece0 in memory on 192.168.31.69:58410 (current size: 34.5 KiB, original size: 34.5 KiB, free: 4.1 GiB)
16:03:49.388 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_0_piece0
16:03:49.389 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_2_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.389 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_2_piece0 in memory on 192.168.31.69:58410 (current size: 206.0 B, original size: 206.0 B, free: 4.1 GiB)
16:03:49.389 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_2_piece0
16:03:49.389 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_1_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.389 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_1_piece0 in memory on 192.168.31.69:58410 (current size: 9.1 KiB, original size: 9.1 KiB, free: 4.1 GiB)
16:03:49.389 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_1_piece0
16:03:49.389 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_3_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.389 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_3_piece0 in memory on 192.168.31.69:58410 (current size: 34.5 KiB, original size: 34.5 KiB, free: 4.1 GiB)
16:03:49.390 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_3_piece0
16:03:49.390 [dispatcher-event-loop-1] DEBUG org.apache.spark.HeartbeatReceiver - Received heartbeat from unknown executor driver
16:03:49.390 [executor-heartbeater] INFO  org.apache.spark.executor.Executor - Told to re-register on heartbeat
16:03:49.390 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManager - BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None) re-registering with master
16:03:49.390 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.390 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.DefaultTopologyMapper - Got a request for 192.168.31.69
16:03:49.390 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.390 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManager - Reporting 8 blocks to the master.
16:03:49.390 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_0_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.391 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_0_piece0 in memory on 192.168.31.69:58410 (current size: 34.5 KiB, original size: 34.5 KiB, free: 4.1 GiB)
16:03:49.391 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_0_piece0
16:03:49.391 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_2_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.391 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_2_piece0 in memory on 192.168.31.69:58410 (current size: 206.0 B, original size: 206.0 B, free: 4.1 GiB)
16:03:49.391 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_2_piece0
16:03:49.391 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_1_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.391 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_1_piece0 in memory on 192.168.31.69:58410 (current size: 9.1 KiB, original size: 9.1 KiB, free: 4.1 GiB)
16:03:49.392 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_1_piece0
16:03:49.392 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_3_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.392 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_3_piece0 in memory on 192.168.31.69:58410 (current size: 34.5 KiB, original size: 34.5 KiB, free: 4.1 GiB)
16:03:49.392 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_3_piece0
16:03:49.392 [dispatcher-event-loop-0] DEBUG org.apache.spark.HeartbeatReceiver - Received heartbeat from unknown executor driver
16:03:49.393 [executor-heartbeater] INFO  org.apache.spark.executor.Executor - Told to re-register on heartbeat
16:03:49.393 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManager - BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None) re-registering with master
16:03:49.393 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.393 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.DefaultTopologyMapper - Got a request for 192.168.31.69
16:03:49.393 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.393 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManager - Reporting 8 blocks to the master.
16:03:49.393 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_0_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.394 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_0_piece0 in memory on 192.168.31.69:58410 (current size: 34.5 KiB, original size: 34.5 KiB, free: 4.1 GiB)
16:03:49.394 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_0_piece0
16:03:49.394 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_2_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.394 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_2_piece0 in memory on 192.168.31.69:58410 (current size: 206.0 B, original size: 206.0 B, free: 4.1 GiB)
16:03:49.394 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_2_piece0
16:03:49.394 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_1_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.395 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_1_piece0 in memory on 192.168.31.69:58410 (current size: 9.1 KiB, original size: 9.1 KiB, free: 4.1 GiB)
16:03:49.395 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_1_piece0
16:03:49.395 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_3_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.395 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_3_piece0 in memory on 192.168.31.69:58410 (current size: 34.5 KiB, original size: 34.5 KiB, free: 4.1 GiB)
16:03:49.395 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_3_piece0
16:03:49.395 [dispatcher-event-loop-1] DEBUG org.apache.spark.HeartbeatReceiver - Received heartbeat from unknown executor driver
16:03:49.396 [executor-heartbeater] INFO  org.apache.spark.executor.Executor - Told to re-register on heartbeat
16:03:49.396 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManager - BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None) re-registering with master
16:03:49.396 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.396 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.DefaultTopologyMapper - Got a request for 192.168.31.69
16:03:49.396 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.396 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManager - Reporting 8 blocks to the master.
16:03:49.396 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_0_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.396 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_0_piece0 in memory on 192.168.31.69:58410 (current size: 34.5 KiB, original size: 34.5 KiB, free: 4.1 GiB)
16:03:49.396 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_0_piece0
16:03:49.396 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_2_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.397 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_2_piece0 in memory on 192.168.31.69:58410 (current size: 206.0 B, original size: 206.0 B, free: 4.1 GiB)
16:03:49.397 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_2_piece0
16:03:49.397 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_1_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.397 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_1_piece0 in memory on 192.168.31.69:58410 (current size: 9.1 KiB, original size: 9.1 KiB, free: 4.1 GiB)
16:03:49.397 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_1_piece0
16:03:49.397 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_3_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.397 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_3_piece0 in memory on 192.168.31.69:58410 (current size: 34.5 KiB, original size: 34.5 KiB, free: 4.1 GiB)
16:03:49.398 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_3_piece0
16:03:49.398 [dispatcher-event-loop-0] DEBUG org.apache.spark.HeartbeatReceiver - Received heartbeat from unknown executor driver
16:03:49.398 [executor-heartbeater] INFO  org.apache.spark.executor.Executor - Told to re-register on heartbeat
16:03:49.398 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManager - BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None) re-registering with master
16:03:49.398 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.398 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.DefaultTopologyMapper - Got a request for 192.168.31.69
16:03:49.398 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.398 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManager - Reporting 8 blocks to the master.
16:03:49.398 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_0_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.398 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_0_piece0 in memory on 192.168.31.69:58410 (current size: 34.5 KiB, original size: 34.5 KiB, free: 4.1 GiB)
16:03:49.399 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_0_piece0
16:03:49.399 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_2_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.399 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_2_piece0 in memory on 192.168.31.69:58410 (current size: 206.0 B, original size: 206.0 B, free: 4.1 GiB)
16:03:49.399 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_2_piece0
16:03:49.399 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_1_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.399 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_1_piece0 in memory on 192.168.31.69:58410 (current size: 9.1 KiB, original size: 9.1 KiB, free: 4.1 GiB)
16:03:49.399 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_1_piece0
16:03:49.399 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_3_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.400 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_3_piece0 in memory on 192.168.31.69:58410 (current size: 34.5 KiB, original size: 34.5 KiB, free: 4.1 GiB)
16:03:49.400 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_3_piece0
16:03:49.400 [dispatcher-event-loop-1] DEBUG org.apache.spark.HeartbeatReceiver - Received heartbeat from unknown executor driver
16:03:49.400 [executor-heartbeater] INFO  org.apache.spark.executor.Executor - Told to re-register on heartbeat
16:03:49.400 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManager - BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None) re-registering with master
16:03:49.400 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.400 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.DefaultTopologyMapper - Got a request for 192.168.31.69
16:03:49.400 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.400 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManager - Reporting 8 blocks to the master.
16:03:49.401 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_0_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.401 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_0_piece0 in memory on 192.168.31.69:58410 (current size: 34.5 KiB, original size: 34.5 KiB, free: 4.1 GiB)
16:03:49.401 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_0_piece0
16:03:49.401 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_2_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.401 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_2_piece0 in memory on 192.168.31.69:58410 (current size: 206.0 B, original size: 206.0 B, free: 4.1 GiB)
16:03:49.401 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_2_piece0
16:03:49.402 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_1_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.402 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_1_piece0 in memory on 192.168.31.69:58410 (current size: 9.1 KiB, original size: 9.1 KiB, free: 4.1 GiB)
16:03:49.402 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_1_piece0
16:03:49.402 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_3_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.403 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_3_piece0 in memory on 192.168.31.69:58410 (current size: 34.5 KiB, original size: 34.5 KiB, free: 4.1 GiB)
16:03:49.403 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_3_piece0
16:03:49.403 [dispatcher-event-loop-0] DEBUG org.apache.spark.HeartbeatReceiver - Received heartbeat from unknown executor driver
16:03:49.403 [executor-heartbeater] INFO  org.apache.spark.executor.Executor - Told to re-register on heartbeat
16:03:49.403 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManager - BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None) re-registering with master
16:03:49.403 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.403 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.DefaultTopologyMapper - Got a request for 192.168.31.69
16:03:49.403 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.403 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManager - Reporting 8 blocks to the master.
16:03:49.404 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_0_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.404 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_0_piece0 in memory on 192.168.31.69:58410 (current size: 34.5 KiB, original size: 34.5 KiB, free: 4.1 GiB)
16:03:49.404 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_0_piece0
16:03:49.404 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_2_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.404 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_2_piece0 in memory on 192.168.31.69:58410 (current size: 206.0 B, original size: 206.0 B, free: 4.1 GiB)
16:03:49.404 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_2_piece0
16:03:49.404 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_1_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.405 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_1_piece0 in memory on 192.168.31.69:58410 (current size: 9.1 KiB, original size: 9.1 KiB, free: 4.1 GiB)
16:03:49.405 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_1_piece0
16:03:49.405 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_3_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.405 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_3_piece0 in memory on 192.168.31.69:58410 (current size: 34.5 KiB, original size: 34.5 KiB, free: 4.1 GiB)
16:03:49.405 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_3_piece0
16:03:49.405 [dispatcher-event-loop-1] DEBUG org.apache.spark.HeartbeatReceiver - Received heartbeat from unknown executor driver
16:03:49.405 [executor-heartbeater] INFO  org.apache.spark.executor.Executor - Told to re-register on heartbeat
16:03:49.405 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManager - BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None) re-registering with master
16:03:49.405 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.405 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.DefaultTopologyMapper - Got a request for 192.168.31.69
16:03:49.406 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.406 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManager - Reporting 8 blocks to the master.
16:03:49.406 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_0_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.407 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_0_piece0 in memory on 192.168.31.69:58410 (current size: 34.5 KiB, original size: 34.5 KiB, free: 4.1 GiB)
16:03:49.407 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_0_piece0
16:03:49.407 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_2_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.407 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_2_piece0 in memory on 192.168.31.69:58410 (current size: 206.0 B, original size: 206.0 B, free: 4.1 GiB)
16:03:49.407 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_2_piece0
16:03:49.407 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_1_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.407 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_1_piece0 in memory on 192.168.31.69:58410 (current size: 9.1 KiB, original size: 9.1 KiB, free: 4.1 GiB)
16:03:49.407 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_1_piece0
16:03:49.408 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_3_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.408 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_3_piece0 in memory on 192.168.31.69:58410 (current size: 34.5 KiB, original size: 34.5 KiB, free: 4.1 GiB)
16:03:49.408 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_3_piece0
16:03:49.408 [dispatcher-event-loop-0] DEBUG org.apache.spark.HeartbeatReceiver - Received heartbeat from unknown executor driver
16:03:49.408 [executor-heartbeater] INFO  org.apache.spark.executor.Executor - Told to re-register on heartbeat
16:03:49.408 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManager - BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None) re-registering with master
16:03:49.408 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.409 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.DefaultTopologyMapper - Got a request for 192.168.31.69
16:03:49.409 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.409 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManager - Reporting 8 blocks to the master.
16:03:49.409 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_0_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.409 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_0_piece0 in memory on 192.168.31.69:58410 (current size: 34.5 KiB, original size: 34.5 KiB, free: 4.1 GiB)
16:03:49.409 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_0_piece0
16:03:49.409 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_2_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.410 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_2_piece0 in memory on 192.168.31.69:58410 (current size: 206.0 B, original size: 206.0 B, free: 4.1 GiB)
16:03:49.410 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_2_piece0
16:03:49.410 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_1_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.410 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_1_piece0 in memory on 192.168.31.69:58410 (current size: 9.1 KiB, original size: 9.1 KiB, free: 4.1 GiB)
16:03:49.410 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_1_piece0
16:03:49.410 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_3_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.410 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_3_piece0 in memory on 192.168.31.69:58410 (current size: 34.5 KiB, original size: 34.5 KiB, free: 4.1 GiB)
16:03:49.410 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_3_piece0
16:03:49.411 [dispatcher-event-loop-1] DEBUG org.apache.spark.HeartbeatReceiver - Received heartbeat from unknown executor driver
16:03:49.411 [executor-heartbeater] INFO  org.apache.spark.executor.Executor - Told to re-register on heartbeat
16:03:49.411 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManager - BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None) re-registering with master
16:03:49.411 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.411 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.DefaultTopologyMapper - Got a request for 192.168.31.69
16:03:49.411 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.411 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManager - Reporting 8 blocks to the master.
16:03:49.412 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_0_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.412 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_0_piece0 in memory on 192.168.31.69:58410 (current size: 34.5 KiB, original size: 34.5 KiB, free: 4.1 GiB)
16:03:49.412 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_0_piece0
16:03:49.412 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_2_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.412 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_2_piece0 in memory on 192.168.31.69:58410 (current size: 206.0 B, original size: 206.0 B, free: 4.1 GiB)
16:03:49.412 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_2_piece0
16:03:49.412 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_1_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.412 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_1_piece0 in memory on 192.168.31.69:58410 (current size: 9.1 KiB, original size: 9.1 KiB, free: 4.1 GiB)
16:03:49.413 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_1_piece0
16:03:49.413 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_3_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.413 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_3_piece0 in memory on 192.168.31.69:58410 (current size: 34.5 KiB, original size: 34.5 KiB, free: 4.1 GiB)
16:03:49.413 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_3_piece0
16:03:49.413 [dispatcher-event-loop-0] DEBUG org.apache.spark.HeartbeatReceiver - Received heartbeat from unknown executor driver
16:03:49.413 [executor-heartbeater] INFO  org.apache.spark.executor.Executor - Told to re-register on heartbeat
16:03:49.413 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManager - BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None) re-registering with master
16:03:49.413 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.413 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.DefaultTopologyMapper - Got a request for 192.168.31.69
16:03:49.413 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.413 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManager - Reporting 8 blocks to the master.
16:03:49.414 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_0_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.414 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_0_piece0 in memory on 192.168.31.69:58410 (current size: 34.5 KiB, original size: 34.5 KiB, free: 4.1 GiB)
16:03:49.414 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_0_piece0
16:03:49.414 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_2_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.414 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_2_piece0 in memory on 192.168.31.69:58410 (current size: 206.0 B, original size: 206.0 B, free: 4.1 GiB)
16:03:49.414 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_2_piece0
16:03:49.414 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_1_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.414 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_1_piece0 in memory on 192.168.31.69:58410 (current size: 9.1 KiB, original size: 9.1 KiB, free: 4.1 GiB)
16:03:49.415 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_1_piece0
16:03:49.415 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_3_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.415 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_3_piece0 in memory on 192.168.31.69:58410 (current size: 34.5 KiB, original size: 34.5 KiB, free: 4.1 GiB)
16:03:49.415 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_3_piece0
16:03:49.415 [dispatcher-event-loop-1] DEBUG org.apache.spark.HeartbeatReceiver - Received heartbeat from unknown executor driver
16:03:49.415 [executor-heartbeater] INFO  org.apache.spark.executor.Executor - Told to re-register on heartbeat
16:03:49.415 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManager - BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None) re-registering with master
16:03:49.415 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.415 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.DefaultTopologyMapper - Got a request for 192.168.31.69
16:03:49.415 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.415 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManager - Reporting 8 blocks to the master.
16:03:49.416 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_0_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.416 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_0_piece0 in memory on 192.168.31.69:58410 (current size: 34.5 KiB, original size: 34.5 KiB, free: 4.1 GiB)
16:03:49.416 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_0_piece0
16:03:49.416 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_2_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.416 [subquery-0] DEBUG org.apache.spark.scheduler.DAGScheduler - eagerlyComputePartitionsForRddAndAncestors for RDD 8 took 0.000090 seconds
16:03:49.416 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_2_piece0 in memory on 192.168.31.69:58410 (current size: 206.0 B, original size: 206.0 B, free: 4.1 GiB)
16:03:49.416 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_2_piece0
16:03:49.416 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_1_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.416 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_1_piece0 in memory on 192.168.31.69:58410 (current size: 9.1 KiB, original size: 9.1 KiB, free: 4.1 GiB)
16:03:49.417 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_1_piece0
16:03:49.417 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_3_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.417 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_3_piece0 in memory on 192.168.31.69:58410 (current size: 34.5 KiB, original size: 34.5 KiB, free: 4.1 GiB)
16:03:49.417 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_3_piece0
16:03:49.418 [dispatcher-event-loop-0] DEBUG org.apache.spark.HeartbeatReceiver - Received heartbeat from unknown executor driver
16:03:49.418 [executor-heartbeater] INFO  org.apache.spark.executor.Executor - Told to re-register on heartbeat
16:03:49.418 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManager - BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None) re-registering with master
16:03:49.418 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.418 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.DefaultTopologyMapper - Got a request for 192.168.31.69
16:03:49.418 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.418 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManager - Reporting 8 blocks to the master.
16:03:49.418 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_0_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.418 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_0_piece0 in memory on 192.168.31.69:58410 (current size: 34.5 KiB, original size: 34.5 KiB, free: 4.1 GiB)
16:03:49.418 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_0_piece0
16:03:49.419 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - Merging stage rdd profiles: Set()
16:03:49.419 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_2_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.419 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_2_piece0 in memory on 192.168.31.69:58410 (current size: 206.0 B, original size: 206.0 B, free: 4.1 GiB)
16:03:49.419 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_2_piece0
16:03:49.419 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_1_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.419 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_1_piece0 in memory on 192.168.31.69:58410 (current size: 9.1 KiB, original size: 9.1 KiB, free: 4.1 GiB)
16:03:49.419 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_1_piece0
16:03:49.420 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_3_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.420 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_3_piece0 in memory on 192.168.31.69:58410 (current size: 34.5 KiB, original size: 34.5 KiB, free: 4.1 GiB)
16:03:49.420 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_3_piece0
16:03:49.420 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Registering RDD 8 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) as input to shuffle 0
16:03:49.420 [dispatcher-event-loop-1] DEBUG org.apache.spark.HeartbeatReceiver - Received heartbeat from unknown executor driver
16:03:49.420 [executor-heartbeater] INFO  org.apache.spark.executor.Executor - Told to re-register on heartbeat
16:03:49.420 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManager - BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None) re-registering with master
16:03:49.420 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.420 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.DefaultTopologyMapper - Got a request for 192.168.31.69
16:03:49.420 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.420 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManager - Reporting 8 blocks to the master.
16:03:49.421 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_0_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.421 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_0_piece0 in memory on 192.168.31.69:58410 (current size: 34.5 KiB, original size: 34.5 KiB, free: 4.1 GiB)
16:03:49.421 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_0_piece0
16:03:49.421 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_2_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.421 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_2_piece0 in memory on 192.168.31.69:58410 (current size: 206.0 B, original size: 206.0 B, free: 4.1 GiB)
16:03:49.421 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_2_piece0
16:03:49.421 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_1_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.421 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_1_piece0 in memory on 192.168.31.69:58410 (current size: 9.1 KiB, original size: 9.1 KiB, free: 4.1 GiB)
16:03:49.421 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_1_piece0
16:03:49.421 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_3_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.422 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_3_piece0 in memory on 192.168.31.69:58410 (current size: 34.5 KiB, original size: 34.5 KiB, free: 4.1 GiB)
16:03:49.422 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_3_piece0
16:03:49.422 [dispatcher-event-loop-0] DEBUG org.apache.spark.HeartbeatReceiver - Received heartbeat from unknown executor driver
16:03:49.422 [executor-heartbeater] INFO  org.apache.spark.executor.Executor - Told to re-register on heartbeat
16:03:49.422 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManager - BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None) re-registering with master
16:03:49.422 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.422 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.DefaultTopologyMapper - Got a request for 192.168.31.69
16:03:49.422 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.422 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManager - Reporting 8 blocks to the master.
16:03:49.423 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_0_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.423 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_0_piece0 in memory on 192.168.31.69:58410 (current size: 34.5 KiB, original size: 34.5 KiB, free: 4.1 GiB)
16:03:49.423 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_0_piece0
16:03:49.423 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_2_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.423 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_2_piece0 in memory on 192.168.31.69:58410 (current size: 206.0 B, original size: 206.0 B, free: 4.1 GiB)
16:03:49.423 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_2_piece0
16:03:49.423 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_1_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.423 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_1_piece0 in memory on 192.168.31.69:58410 (current size: 9.1 KiB, original size: 9.1 KiB, free: 4.1 GiB)
16:03:49.423 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_1_piece0
16:03:49.423 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_3_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.424 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_3_piece0 in memory on 192.168.31.69:58410 (current size: 34.5 KiB, original size: 34.5 KiB, free: 4.1 GiB)
16:03:49.424 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_3_piece0
16:03:49.424 [dispatcher-event-loop-1] DEBUG org.apache.spark.HeartbeatReceiver - Received heartbeat from unknown executor driver
16:03:49.424 [executor-heartbeater] INFO  org.apache.spark.executor.Executor - Told to re-register on heartbeat
16:03:49.424 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManager - BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None) re-registering with master
16:03:49.424 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.424 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.DefaultTopologyMapper - Got a request for 192.168.31.69
16:03:49.424 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.424 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManager - Reporting 8 blocks to the master.
16:03:49.425 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_0_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.425 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_0_piece0 in memory on 192.168.31.69:58410 (current size: 34.5 KiB, original size: 34.5 KiB, free: 4.1 GiB)
16:03:49.425 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_0_piece0
16:03:49.425 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_2_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.425 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_2_piece0 in memory on 192.168.31.69:58410 (current size: 206.0 B, original size: 206.0 B, free: 4.1 GiB)
16:03:49.425 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_2_piece0
16:03:49.425 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_1_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.426 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_1_piece0 in memory on 192.168.31.69:58410 (current size: 9.1 KiB, original size: 9.1 KiB, free: 4.1 GiB)
16:03:49.426 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_1_piece0
16:03:49.426 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_3_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.426 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_3_piece0 in memory on 192.168.31.69:58410 (current size: 34.5 KiB, original size: 34.5 KiB, free: 4.1 GiB)
16:03:49.426 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_3_piece0
16:03:49.426 [dispatcher-event-loop-0] DEBUG org.apache.spark.HeartbeatReceiver - Received heartbeat from unknown executor driver
16:03:49.426 [executor-heartbeater] INFO  org.apache.spark.executor.Executor - Told to re-register on heartbeat
16:03:49.426 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManager - BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None) re-registering with master
16:03:49.426 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.427 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.DefaultTopologyMapper - Got a request for 192.168.31.69
16:03:49.427 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.427 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManager - Reporting 8 blocks to the master.
16:03:49.427 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_0_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.427 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_0_piece0 in memory on 192.168.31.69:58410 (current size: 34.5 KiB, original size: 34.5 KiB, free: 4.1 GiB)
16:03:49.427 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_0_piece0
16:03:49.427 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_2_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.427 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_2_piece0 in memory on 192.168.31.69:58410 (current size: 206.0 B, original size: 206.0 B, free: 4.1 GiB)
16:03:49.427 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_2_piece0
16:03:49.427 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_1_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.428 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got map stage job 1 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) with 1 output partitions
16:03:49.428 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_1_piece0 in memory on 192.168.31.69:58410 (current size: 9.1 KiB, original size: 9.1 KiB, free: 4.1 GiB)
16:03:49.428 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_1_piece0
16:03:49.428 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_3_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.428 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_3_piece0 in memory on 192.168.31.69:58410 (current size: 34.5 KiB, original size: 34.5 KiB, free: 4.1 GiB)
16:03:49.428 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_3_piece0
16:03:49.428 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ShuffleMapStage 1 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266)
16:03:49.428 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
16:03:49.428 [dispatcher-event-loop-1] DEBUG org.apache.spark.HeartbeatReceiver - Received heartbeat from unknown executor driver
16:03:49.428 [executor-heartbeater] INFO  org.apache.spark.executor.Executor - Told to re-register on heartbeat
16:03:49.428 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManager - BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None) re-registering with master
16:03:49.428 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.428 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.DefaultTopologyMapper - Got a request for 192.168.31.69
16:03:49.429 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.429 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManager - Reporting 8 blocks to the master.
16:03:49.429 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
16:03:49.429 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_0_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.429 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_0_piece0 in memory on 192.168.31.69:58410 (current size: 34.5 KiB, original size: 34.5 KiB, free: 4.1 GiB)
16:03:49.429 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_0_piece0
16:03:49.429 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_2_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.429 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_2_piece0 in memory on 192.168.31.69:58410 (current size: 206.0 B, original size: 206.0 B, free: 4.1 GiB)
16:03:49.430 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_2_piece0
16:03:49.430 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - submitStage(ShuffleMapStage 1 (name=$anonfun$withThreadLocalCaptured$1 at FutureTask.java:266;jobs=1))
16:03:49.430 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - missing: List()
16:03:49.430 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_1_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.430 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 1 (MapPartitionsRDD[8] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266), which has no missing parents
16:03:49.430 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - submitMissingTasks(ShuffleMapStage 1)
16:03:49.430 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_1_piece0 in memory on 192.168.31.69:58410 (current size: 9.1 KiB, original size: 9.1 KiB, free: 4.1 GiB)
16:03:49.430 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_1_piece0
16:03:49.431 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_3_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.431 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_3_piece0 in memory on 192.168.31.69:58410 (current size: 34.5 KiB, original size: 34.5 KiB, free: 4.1 GiB)
16:03:49.431 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_3_piece0
16:03:49.431 [dispatcher-event-loop-0] DEBUG org.apache.spark.HeartbeatReceiver - Received heartbeat from unknown executor driver
16:03:49.431 [executor-heartbeater] INFO  org.apache.spark.executor.Executor - Told to re-register on heartbeat
16:03:49.431 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManager - BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None) re-registering with master
16:03:49.431 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.431 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.DefaultTopologyMapper - Got a request for 192.168.31.69
16:03:49.431 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.431 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManager - Reporting 8 blocks to the master.
16:03:49.431 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_0_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.432 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_0_piece0 in memory on 192.168.31.69:58410 (current size: 34.5 KiB, original size: 34.5 KiB, free: 4.1 GiB)
16:03:49.432 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_0_piece0
16:03:49.432 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_2_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.432 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_2_piece0 in memory on 192.168.31.69:58410 (current size: 206.0 B, original size: 206.0 B, free: 4.1 GiB)
16:03:49.432 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_2_piece0
16:03:49.432 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_1_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.432 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_1_piece0 in memory on 192.168.31.69:58410 (current size: 9.1 KiB, original size: 9.1 KiB, free: 4.1 GiB)
16:03:49.432 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_1_piece0
16:03:49.432 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_3_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.433 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_3_piece0 in memory on 192.168.31.69:58410 (current size: 34.5 KiB, original size: 34.5 KiB, free: 4.1 GiB)
16:03:49.433 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_3_piece0
16:03:49.433 [dispatcher-event-loop-1] DEBUG org.apache.spark.HeartbeatReceiver - Received heartbeat from unknown executor driver
16:03:49.433 [executor-heartbeater] INFO  org.apache.spark.executor.Executor - Told to re-register on heartbeat
16:03:49.433 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManager - BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None) re-registering with master
16:03:49.433 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.433 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.DefaultTopologyMapper - Got a request for 192.168.31.69
16:03:49.433 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.433 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManager - Reporting 8 blocks to the master.
16:03:49.433 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_0_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.433 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_0_piece0 in memory on 192.168.31.69:58410 (current size: 34.5 KiB, original size: 34.5 KiB, free: 4.1 GiB)
16:03:49.434 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_0_piece0
16:03:49.434 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_2_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.434 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_2_piece0 in memory on 192.168.31.69:58410 (current size: 206.0 B, original size: 206.0 B, free: 4.1 GiB)
16:03:49.434 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_2_piece0
16:03:49.434 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_1_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.434 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_1_piece0 in memory on 192.168.31.69:58410 (current size: 9.1 KiB, original size: 9.1 KiB, free: 4.1 GiB)
16:03:49.434 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_1_piece0
16:03:49.434 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_3_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.434 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_3_piece0 in memory on 192.168.31.69:58410 (current size: 34.5 KiB, original size: 34.5 KiB, free: 4.1 GiB)
16:03:49.434 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_3_piece0
16:03:49.435 [dispatcher-event-loop-0] DEBUG org.apache.spark.HeartbeatReceiver - Received heartbeat from unknown executor driver
16:03:49.435 [executor-heartbeater] INFO  org.apache.spark.executor.Executor - Told to re-register on heartbeat
16:03:49.435 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManager - BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None) re-registering with master
16:03:49.435 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.435 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.DefaultTopologyMapper - Got a request for 192.168.31.69
16:03:49.435 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.435 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManager - Reporting 8 blocks to the master.
16:03:49.435 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_0_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.436 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_0_piece0 in memory on 192.168.31.69:58410 (current size: 34.5 KiB, original size: 34.5 KiB, free: 4.1 GiB)
16:03:49.436 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_0_piece0
16:03:49.436 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_2_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.436 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_2_piece0 in memory on 192.168.31.69:58410 (current size: 206.0 B, original size: 206.0 B, free: 4.1 GiB)
16:03:49.436 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_2_piece0
16:03:49.437 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_1_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.437 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_1_piece0 in memory on 192.168.31.69:58410 (current size: 9.1 KiB, original size: 9.1 KiB, free: 4.1 GiB)
16:03:49.437 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_1_piece0
16:03:49.437 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_3_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.437 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_3_piece0 in memory on 192.168.31.69:58410 (current size: 34.5 KiB, original size: 34.5 KiB, free: 4.1 GiB)
16:03:49.437 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_3_piece0
16:03:49.437 [dispatcher-event-loop-1] DEBUG org.apache.spark.HeartbeatReceiver - Received heartbeat from unknown executor driver
16:03:49.437 [executor-heartbeater] INFO  org.apache.spark.executor.Executor - Told to re-register on heartbeat
16:03:49.437 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManager - BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None) re-registering with master
16:03:49.437 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.438 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.DefaultTopologyMapper - Got a request for 192.168.31.69
16:03:49.438 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.438 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManager - Reporting 8 blocks to the master.
16:03:49.438 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_0_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.438 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_0_piece0 in memory on 192.168.31.69:58410 (current size: 34.5 KiB, original size: 34.5 KiB, free: 4.1 GiB)
16:03:49.438 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_0_piece0
16:03:49.438 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_2_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.439 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_2_piece0 in memory on 192.168.31.69:58410 (current size: 206.0 B, original size: 206.0 B, free: 4.1 GiB)
16:03:49.439 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_2_piece0
16:03:49.439 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_1_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.439 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_1_piece0 in memory on 192.168.31.69:58410 (current size: 9.1 KiB, original size: 9.1 KiB, free: 4.1 GiB)
16:03:49.439 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_1_piece0
16:03:49.439 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_3_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.439 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_3_piece0 in memory on 192.168.31.69:58410 (current size: 34.5 KiB, original size: 34.5 KiB, free: 4.1 GiB)
16:03:49.439 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_3_piece0
16:03:49.440 [dispatcher-event-loop-0] DEBUG org.apache.spark.HeartbeatReceiver - Received heartbeat from unknown executor driver
16:03:49.440 [executor-heartbeater] INFO  org.apache.spark.executor.Executor - Told to re-register on heartbeat
16:03:49.440 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManager - BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None) re-registering with master
16:03:49.440 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.440 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.DefaultTopologyMapper - Got a request for 192.168.31.69
16:03:49.440 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.440 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManager - Reporting 8 blocks to the master.
16:03:49.440 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_0_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.440 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_0_piece0 in memory on 192.168.31.69:58410 (current size: 34.5 KiB, original size: 34.5 KiB, free: 4.1 GiB)
16:03:49.441 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_0_piece0
16:03:49.441 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_2_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.441 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_2_piece0 in memory on 192.168.31.69:58410 (current size: 206.0 B, original size: 206.0 B, free: 4.1 GiB)
16:03:49.441 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_2_piece0
16:03:49.441 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_1_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.442 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_1_piece0 in memory on 192.168.31.69:58410 (current size: 9.1 KiB, original size: 9.1 KiB, free: 4.1 GiB)
16:03:49.442 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_1_piece0
16:03:49.442 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_3_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.442 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_3_piece0 in memory on 192.168.31.69:58410 (current size: 34.5 KiB, original size: 34.5 KiB, free: 4.1 GiB)
16:03:49.442 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_3_piece0
16:03:49.442 [dispatcher-event-loop-1] DEBUG org.apache.spark.HeartbeatReceiver - Received heartbeat from unknown executor driver
16:03:49.442 [executor-heartbeater] INFO  org.apache.spark.executor.Executor - Told to re-register on heartbeat
16:03:49.442 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManager - BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None) re-registering with master
16:03:49.442 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.442 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.DefaultTopologyMapper - Got a request for 192.168.31.69
16:03:49.443 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.443 [executor-heartbeater] INFO  org.apache.spark.storage.BlockManager - Reporting 8 blocks to the master.
16:03:49.443 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_0_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.443 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_0_piece0 in memory on 192.168.31.69:58410 (current size: 34.5 KiB, original size: 34.5 KiB, free: 4.1 GiB)
16:03:49.443 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_0_piece0
16:03:49.443 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_2_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.443 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_2_piece0 in memory on 192.168.31.69:58410 (current size: 206.0 B, original size: 206.0 B, free: 4.1 GiB)
16:03:49.443 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_2_piece0
16:03:49.443 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_1_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.443 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_1_piece0 in memory on 192.168.31.69:58410 (current size: 9.1 KiB, original size: 9.1 KiB, free: 4.1 GiB)
16:03:49.443 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_1_piece0
16:03:49.444 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_3_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.444 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Updated broadcast_3_piece0 in memory on 192.168.31.69:58410 (current size: 34.5 KiB, original size: 34.5 KiB, free: 4.1 GiB)
16:03:49.444 [executor-heartbeater] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_3_piece0
16:03:49.449 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_4 stored as values in memory (estimated size 23.5 KiB, free 4.0 GiB)
16:03:49.450 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager - Put block broadcast_4 locally took 0 ms
16:03:49.450 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager - Putting block broadcast_4 without replication took 0 ms
16:03:49.450 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_4_piece0 stored as bytes in memory (estimated size 11.2 KiB, free 4.0 GiB)
16:03:49.451 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_4_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.451 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_4_piece0 in memory on 192.168.31.69:58410 (size: 11.2 KiB, free: 4.1 GiB)
16:03:49.451 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_4_piece0
16:03:49.451 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager - Told master about block broadcast_4_piece0
16:03:49.451 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager - Put block broadcast_4_piece0 locally took 0 ms
16:03:49.451 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager - Putting block broadcast_4_piece0 without replication took 0 ms
16:03:49.451 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 4 from broadcast at DAGScheduler.scala:1513
16:03:49.453 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[8] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) (first 15 tasks are for partitions Vector(0))
16:03:49.453 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks resource profile 0
16:03:49.453 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.TaskSetManager - Epoch for TaskSet 1.0: 0
16:03:49.453 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.TaskSetManager - Adding pending tasks took 0 ms
16:03:49.453 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.TaskSetManager - Valid locality levels for TaskSet 1.0: NO_PREF, ANY
16:03:49.453 [dispatcher-event-loop-0] DEBUG org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_1.0, runningTasks: 0
16:03:49.454 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 1) (192.168.31.69, executor driver, partition 0, PROCESS_LOCAL, 5015 bytes) taskResourceAssignments Map()
16:03:49.455 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 1)
16:03:49.456 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] DEBUG org.apache.spark.executor.ExecutorMetricsPoller - stageTCMP: (1, 0) -> 1
16:03:49.457 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] DEBUG org.apache.spark.storage.BlockManager - Getting local block broadcast_4
16:03:49.457 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] DEBUG org.apache.spark.storage.BlockManager - Level for block broadcast_4 is StorageLevel(disk, memory, deserialized, 1 replicas)
16:03:49.500 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-warehouse/dim_table/part-00000-392e1f87-219f-4557-a467-6e64b17b45b2-c000.snappy.parquet, range: 0-789, partition values: [empty row]
16:03:49.500 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] DEBUG org.apache.spark.storage.BlockManager - Getting local block broadcast_3
16:03:49.501 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] DEBUG org.apache.spark.storage.BlockManager - Level for block broadcast_3 is StorageLevel(disk, memory, deserialized, 1 replicas)
16:03:49.509 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] DEBUG org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport - Going to read the following fields from the Parquet file with the following schema:
Parquet file schema:
message spark_schema {
  optional binary date (STRING);
  optional binary event (STRING);
}

Parquet clipped schema:
message spark_schema {
  optional binary date (STRING);
  optional binary event (STRING);
}

Parquet requested schema:
message spark_schema {
  optional binary date (STRING);
  optional binary event (STRING);
}

Catalyst requested schema:
root
-- date: string (nullable = true)
-- event: string (nullable = true)

       
16:03:49.510 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] DEBUG org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat - Appending StructType() [empty row]
16:03:49.542 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 6.004777 ms
org.apache.spark.sql.catalyst.expressions.codegen.package$ExpressionCanonicalizer$@7238e327 org.apache.spark.sql.catalyst.expressions.codegen.package$ExpressionCanonicalizer$CleanExpressions --> old【
input[0, string, false] AS date#33#54】--> new【
input[0, string, false]】


RuleExecutor.execute[org.apache.spark.sql.catalyst.expressions.codegen.package$ExpressionCanonicalizer$@7238e327] final result ==> old【
input[0, string, false] AS date#33#54】==> new【
input[0, string, false]】


16:03:49.559 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 8.931989 ms
16:03:49.568 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 5.945259 ms
16:03:49.573 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] DEBUG org.apache.spark.memory.TaskMemoryManager - Task 1 acquired 256.0 KiB for org.apache.spark.unsafe.map.BytesToBytesMap@65ee6c7f
16:03:49.578 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] DEBUG org.apache.spark.memory.TaskMemoryManager - Task 1 acquired 64.0 MiB for org.apache.spark.unsafe.map.BytesToBytesMap@65ee6c7f
16:03:49.584 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] DEBUG org.apache.spark.memory.TaskMemoryManager - Task 1 release 256.0 KiB from org.apache.spark.unsafe.map.BytesToBytesMap@65ee6c7f
16:03:49.605 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 16.893542 ms
16:03:49.644 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] DEBUG org.apache.spark.memory.TaskMemoryManager - Task 1 release 64.0 MiB from org.apache.spark.unsafe.map.BytesToBytesMap@65ee6c7f
16:03:49.662 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] DEBUG org.apache.spark.shuffle.sort.io.LocalDiskShuffleMapOutputWriter - Writing shuffle index file for mapId 1 with length 200
16:03:49.668 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] DEBUG org.apache.spark.shuffle.IndexShuffleBlockResolver - Shuffle index for mapId 1: [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,76,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]
16:03:49.675 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 1). 2759 bytes result sent to driver
16:03:49.675 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] DEBUG org.apache.spark.executor.ExecutorMetricsPoller - stageTCMP: (1, 0) -> 0
16:03:49.676 [dispatcher-event-loop-0] DEBUG org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_1.0, runningTasks: 0
16:03:49.676 [dispatcher-event-loop-0] DEBUG org.apache.spark.scheduler.TaskSetManager - No tasks for locality level NO_PREF, so moving to locality level ANY
16:03:49.677 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 1) in 224 ms on 192.168.31.69 (executor driver) (1/1)
16:03:49.677 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
16:03:49.678 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - ShuffleMapTask finished on driver
16:03:49.678 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 1 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) finished in 0.243 s
16:03:49.679 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
16:03:49.679 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - running: Set()
16:03:49.679 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - waiting: Set()
16:03:49.679 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - failed: Set()
16:03:49.680 [dag-scheduler-event-loop] DEBUG org.apache.spark.MapOutputTrackerMaster - Increasing epoch to 1
16:03:49.685 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - After removal of stage 1, remaining stages = 0
SparkPlanner(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2@67b3960b) - org.apache.spark.sql.execution.SparkStrategies$SpecialLimits$ -->【
ReturnAnswer
+- LogicalQueryStage Aggregate [date#33 AS date#33#54], [date#33 AS date#33#54], HashAggregate(keys=[date#33#54], functions=[])
】 --> 【
PlanLater LogicalQueryStage Aggregate [date#33 AS date#33#54], [date#33 AS date#33#54], HashAggregate(keys=[date#33#54], functions=[])
】

SparkPlanner(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2@67b3960b) - org.apache.spark.sql.execution.adaptive.LogicalQueryStageStrategy$ -->【
LogicalQueryStage Aggregate [date#33 AS date#33#54], [date#33 AS date#33#54], HashAggregate(keys=[date#33#54], functions=[])
】 --> 【
HashAggregate(keys=[date#33#54], functions=[], output=[date#33#54])
+- ShuffleQueryStage 0
   +- Exchange hashpartitioning(date#33#54, 200), ENSURE_REQUIREMENTS, [plan_id=116]
      +- HashAggregate(keys=[date#33 AS date#33#54], functions=[], output=[date#33#54])
         +- Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
            +- FileScan parquet default.dim_table[date#33,event#34] Batched: false, DataFilters: [isnotnull(event#34), (event#34 = New Year), isnotnull(date#33)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(event), EqualTo(event,New Year), IsNotNull(date)], ReadSchema: struct<date:string,event:string>
】

16:03:49.918 [Spark Context Cleaner] DEBUG org.apache.spark.broadcast.TorrentBroadcast - Unpersisting TorrentBroadcast 1
16:03:49.922 [subquery-0] INFO  org.apache.spark.sql.execution.adaptive.ShufflePartitionsUtil - For shuffle(0), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
optimizeQueryStage - org.apache.spark.sql.execution.adaptive.CoalesceShufflePartitions -> old【
HashAggregate(keys=[date#33#54], functions=[], output=[date#33#54])
+- ShuffleQueryStage 0
   +- Exchange hashpartitioning(date#33#54, 200), ENSURE_REQUIREMENTS, [plan_id=116]
      +- HashAggregate(keys=[date#33 AS date#33#54], functions=[], output=[date#33#54])
         +- Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
            +- FileScan parquet default.dim_table[date#33,event#34] Batched: false, DataFilters: [isnotnull(event#34), (event#34 = New Year), isnotnull(date#33)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(event), EqualTo(event,New Year), IsNotNull(date)], ReadSchema: struct<date:string,event:string>
】AQE --> new【
HashAggregate(keys=[date#33#54], functions=[], output=[date#33#54])
+- AQEShuffleRead coalesced
   +- ShuffleQueryStage 0
      +- Exchange hashpartitioning(date#33#54, 200), ENSURE_REQUIREMENTS, [plan_id=116]
         +- HashAggregate(keys=[date#33 AS date#33#54], functions=[], output=[date#33#54])
            +- Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
               +- FileScan parquet default.dim_table[date#33,event#34] Batched: false, DataFilters: [isnotnull(event#34), (event#34 = New Year), isnotnull(date#33)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(event), EqualTo(event,New Year), IsNotNull(date)], ReadSchema: struct<date:string,event:string>
】


16:03:49.935 [block-manager-storage-async-thread-pool-0] DEBUG org.apache.spark.storage.BlockManagerStorageEndpoint - removing broadcast 1
16:03:49.936 [block-manager-storage-async-thread-pool-0] DEBUG org.apache.spark.storage.BlockManager - Removing broadcast 1
16:03:49.939 [block-manager-storage-async-thread-pool-0] DEBUG org.apache.spark.storage.BlockManager - Removing block broadcast_1
16:03:49.940 [block-manager-storage-async-thread-pool-0] DEBUG org.apache.spark.storage.memory.MemoryStore - Block broadcast_1 of size 19160 dropped from memory (free 4324733043)
16:03:49.941 [block-manager-storage-async-thread-pool-0] DEBUG org.apache.spark.storage.BlockManager - Removing block broadcast_1_piece0
16:03:49.941 [subquery-0] DEBUG org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec - withFinalPlanUpdate.executeCollect ==> after get final plan: 
HashAggregate(keys=[date#33#54], functions=[], output=[date#33#54])
+- AQEShuffleRead coalesced
   +- ShuffleQueryStage 0
      +- Exchange hashpartitioning(date#33#54, 200), ENSURE_REQUIREMENTS, [plan_id=116]
         +- HashAggregate(keys=[date#33 AS date#33#54], functions=[], output=[date#33#54])
            +- Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
               +- FileScan parquet default.dim_table[date#33,event#34] Batched: false, DataFilters: [isnotnull(event#34), (event#34 = New Year), isnotnull(date#33)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(event), EqualTo(event,New Year), IsNotNull(date)], ReadSchema: struct<date:string,event:string>

16:03:49.942 [block-manager-storage-async-thread-pool-0] DEBUG org.apache.spark.storage.memory.MemoryStore - Block broadcast_1_piece0 of size 9353 dropped from memory (free 4324742396)
16:03:49.942 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_1_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.943 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_1_piece0 on 192.168.31.69:58410 in memory (size: 9.1 KiB, free: 4.1 GiB)
16:03:49.943 [block-manager-storage-async-thread-pool-0] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_1_piece0
16:03:49.943 [block-manager-storage-async-thread-pool-0] DEBUG org.apache.spark.storage.BlockManager - Told master about block broadcast_1_piece0
16:03:49.944 [block-manager-storage-async-thread-pool-2] DEBUG org.apache.spark.storage.BlockManagerStorageEndpoint - Done removing broadcast 1, response is 0
16:03:49.945 [block-manager-storage-async-thread-pool-2] DEBUG org.apache.spark.storage.BlockManagerStorageEndpoint - Sent response: 0 to 192.168.31.69:58409
16:03:49.947 [subquery-0] DEBUG org.apache.spark.util.ClosureCleaner - Cleaning indylambda closure: $anonfun$doExecute$1$adapted
16:03:49.948 [subquery-0] DEBUG org.apache.spark.util.ClosureCleaner -  +++ indylambda closure ($anonfun$doExecute$1$adapted) is now cleaned +++
16:03:49.970 [subquery-0] DEBUG org.apache.spark.util.ClosureCleaner - Cleaning indylambda closure: $anonfun$collect$2
16:03:49.973 [subquery-0] DEBUG org.apache.spark.util.ClosureCleaner -  +++ indylambda closure ($anonfun$collect$2) is now cleaned +++
16:03:49.974 [subquery-0] DEBUG org.apache.spark.util.ClosureCleaner - Cleaning indylambda closure: $anonfun$runJob$5
16:03:49.977 [subquery-0] DEBUG org.apache.spark.util.ClosureCleaner -  +++ indylambda closure ($anonfun$runJob$5) is now cleaned +++
16:03:49.978 [subquery-0] INFO  org.apache.spark.SparkContext - Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266
16:03:49.978 [subquery-0] DEBUG org.apache.spark.scheduler.DAGScheduler - eagerlyComputePartitionsForRddAndAncestors for RDD 11 took 0.000150 seconds
16:03:49.979 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - Merging stage rdd profiles: Set()
16:03:49.979 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - Merging stage rdd profiles: Set()
16:03:49.980 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 2 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) with 1 output partitions
16:03:49.980 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 3 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266)
16:03:49.980 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 2)
16:03:49.980 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
16:03:49.980 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - submitStage(ResultStage 3 (name=$anonfun$withThreadLocalCaptured$1 at FutureTask.java:266;jobs=2))
16:03:49.980 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - missing: List()
16:03:49.980 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 3 (MapPartitionsRDD[11] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266), which has no missing parents
16:03:49.980 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - submitMissingTasks(ResultStage 3)
16:03:49.992 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_5 stored as values in memory (estimated size 27.4 KiB, free 4.0 GiB)
16:03:49.992 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager - Put block broadcast_5 locally took 0 ms
16:03:49.992 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager - Putting block broadcast_5 without replication took 0 ms
16:03:49.993 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_5_piece0 stored as bytes in memory (estimated size 13.0 KiB, free 4.0 GiB)
16:03:49.993 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_5_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:49.993 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_5_piece0 in memory on 192.168.31.69:58410 (size: 13.0 KiB, free: 4.1 GiB)
16:03:49.993 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_5_piece0
16:03:49.993 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager - Told master about block broadcast_5_piece0
16:03:49.993 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager - Put block broadcast_5_piece0 locally took 0 ms
16:03:49.993 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager - Putting block broadcast_5_piece0 without replication took 0 ms
16:03:49.993 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 5 from broadcast at DAGScheduler.scala:1513
16:03:49.993 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[11] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) (first 15 tasks are for partitions Vector(0))
16:03:49.993 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 3.0 with 1 tasks resource profile 0
16:03:49.994 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.TaskSetManager - Epoch for TaskSet 3.0: 1
16:03:49.995 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.TaskSetManager - Adding pending tasks took 1 ms
16:03:49.996 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.TaskSetManager - Valid locality levels for TaskSet 3.0: NODE_LOCAL, ANY
16:03:49.996 [dispatcher-event-loop-1] DEBUG org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_3.0, runningTasks: 0
16:03:49.998 [dispatcher-event-loop-1] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 3.0 (TID 2) (192.168.31.69, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
16:03:49.998 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 3.0 (TID 2)
16:03:49.998 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] DEBUG org.apache.spark.executor.ExecutorMetricsPoller - stageTCMP: (3, 0) -> 1
16:03:49.999 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] DEBUG org.apache.spark.storage.BlockManager - Getting local block broadcast_5
16:03:49.999 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] DEBUG org.apache.spark.storage.BlockManager - Level for block broadcast_5 is StorageLevel(disk, memory, deserialized, 1 replicas)
16:03:50.005 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] DEBUG org.apache.spark.MapOutputTrackerMaster - Fetching outputs for shuffle 0
16:03:50.008 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] DEBUG org.apache.spark.MapOutputTrackerMaster - Convert map statuses for shuffle 0, mappers 0-1, partitions 0-200
16:03:50.027 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] DEBUG org.apache.spark.storage.ShuffleBlockFetcherIterator - maxBytesInFlight: 50331648, targetRemoteRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
16:03:50.033 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 (80.0 B) non-empty blocks including 1 (80.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
16:03:50.034 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 11 ms
16:03:50.034 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] DEBUG org.apache.spark.storage.ShuffleBlockFetcherIterator - Start fetching local blocks: (shuffle_0_1_55_56,0)
16:03:50.034 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] DEBUG org.apache.spark.storage.BlockManager - Getting local shuffle block shuffle_0_1_55_56
16:03:50.039 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] DEBUG org.apache.spark.storage.ShuffleBlockFetcherIterator - Got local blocks in 16 ms
16:03:50.052 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] DEBUG org.apache.spark.memory.TaskMemoryManager - Task 2 acquired 256.0 KiB for org.apache.spark.unsafe.map.BytesToBytesMap@79b08597
16:03:50.054 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] DEBUG org.apache.spark.memory.TaskMemoryManager - Task 2 acquired 64.0 MiB for org.apache.spark.unsafe.map.BytesToBytesMap@79b08597
16:03:50.061 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] DEBUG org.apache.spark.memory.TaskMemoryManager - Task 2 release 256.0 KiB from org.apache.spark.unsafe.map.BytesToBytesMap@79b08597
16:03:50.062 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] DEBUG org.apache.spark.memory.TaskMemoryManager - Task 2 release 64.0 MiB from org.apache.spark.unsafe.map.BytesToBytesMap@79b08597
16:03:50.062 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 3.0 (TID 2). 3734 bytes result sent to driver
16:03:50.062 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] DEBUG org.apache.spark.executor.ExecutorMetricsPoller - stageTCMP: (3, 0) -> 0
16:03:50.063 [dispatcher-event-loop-1] DEBUG org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_3.0, runningTasks: 0
16:03:50.063 [dispatcher-event-loop-1] DEBUG org.apache.spark.scheduler.TaskSetManager - No tasks for locality level NODE_LOCAL, so moving to locality level ANY
16:03:50.063 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 3.0 (TID 2) in 67 ms on 192.168.31.69 (executor driver) (1/1)
16:03:50.063 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 3.0, whose tasks have all completed, from pool 
16:03:50.064 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 3 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) finished in 0.074 s
16:03:50.064 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - After removal of stage 2, remaining stages = 1
16:03:50.064 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - After removal of stage 3, remaining stages = 0
16:03:50.064 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
16:03:50.064 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 3: Stage finished
16:03:50.064 [subquery-0] INFO  org.apache.spark.scheduler.DAGScheduler - Job 2 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266, took 0.085871 s
16:03:50.066 [subquery-0] DEBUG org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec - withFinalPlanUpdate.executeCollect was finished.
16:03:50.067 [subquery-0] DEBUG org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec - Final plan: HashAggregate(keys=[date#33#54], functions=[], output=[date#33#54])
+- AQEShuffleRead coalesced
   +- ShuffleQueryStage 0
      +- Exchange hashpartitioning(date#33#54, 200), ENSURE_REQUIREMENTS, [plan_id=116]
         +- HashAggregate(keys=[date#33 AS date#33#54], functions=[], output=[date#33#54])
            +- Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
               +- FileScan parquet default.dim_table[date#33,event#34] Batched: false, DataFilters: [isnotnull(event#34), (event#34 = New Year), isnotnull(date#33)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(event), EqualTo(event,New Year), IsNotNull(date)], ReadSchema: struct<date:string,event:string>

16:03:50.067 [subquery-0] DEBUG org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec - withFinalPlanUpdate.executeCollect.finalPlanUpdate was finished.
16:03:50.073 [ScalaTest-run-running-PhysicalPlanSpec] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_6 stored as values in memory (estimated size 359.3 KiB, free 4.0 GiB)
16:03:50.073 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.storage.BlockManager - Put block broadcast_6 locally took 2 ms
16:03:50.073 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.storage.BlockManager - Putting block broadcast_6 without replication took 2 ms
16:03:50.081 [ScalaTest-run-running-PhysicalPlanSpec] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_6_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 4.0 GiB)
16:03:50.082 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_6_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:50.082 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_6_piece0 in memory on 192.168.31.69:58410 (size: 34.5 KiB, free: 4.1 GiB)
16:03:50.082 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_6_piece0
16:03:50.082 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.storage.BlockManager - Told master about block broadcast_6_piece0
16:03:50.082 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.storage.BlockManager - Put block broadcast_6_piece0 locally took 0 ms
16:03:50.082 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.storage.BlockManager - Putting block broadcast_6_piece0 without replication took 0 ms
16:03:50.082 [ScalaTest-run-running-PhysicalPlanSpec] INFO  org.apache.spark.SparkContext - Created broadcast 6 from show at PhysicalPlanSpec.scala:191
16:03:50.098 [ScalaTest-run-running-PhysicalPlanSpec] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 11.093126 ms
16:03:50.102 [ScalaTest-run-running-PhysicalPlanSpec] INFO  org.apache.spark.sql.execution.datasources.InMemoryFileIndex - Selected 3 partitions out of 3, pruned 0.0% partitions.
16:03:50.103 [ScalaTest-run-running-PhysicalPlanSpec] INFO  org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 4194970 bytes, open cost is considered as scanning 4194304 bytes.
16:03:50.105 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.util.ClosureCleaner - Cleaning indylambda closure: $anonfun$doExecute$3
16:03:50.108 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.util.ClosureCleaner -  +++ indylambda closure ($anonfun$doExecute$3) is now cleaned +++
16:03:50.133 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.util.ClosureCleaner - Cleaning indylambda closure: $anonfun$executeTake$2
16:03:50.136 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.util.ClosureCleaner -  +++ indylambda closure ($anonfun$executeTake$2) is now cleaned +++
16:03:50.136 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.util.ClosureCleaner - Cleaning indylambda closure: $anonfun$runJob$5
16:03:50.140 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.util.ClosureCleaner -  +++ indylambda closure ($anonfun$runJob$5) is now cleaned +++
16:03:50.140 [ScalaTest-run-running-PhysicalPlanSpec] INFO  org.apache.spark.SparkContext - Starting job: show at PhysicalPlanSpec.scala:191
16:03:50.140 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.scheduler.DAGScheduler - eagerlyComputePartitionsForRddAndAncestors for RDD 16 took 0.000055 seconds
16:03:50.141 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - Merging stage rdd profiles: Set()
16:03:50.141 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 3 (show at PhysicalPlanSpec.scala:191) with 1 output partitions
16:03:50.141 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 4 (show at PhysicalPlanSpec.scala:191)
16:03:50.141 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
16:03:50.141 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
16:03:50.142 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - submitStage(ResultStage 4 (name=show at PhysicalPlanSpec.scala:191;jobs=3))
16:03:50.142 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - missing: List()
16:03:50.142 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 4 (MapPartitionsRDD[16] at show at PhysicalPlanSpec.scala:191), which has no missing parents
16:03:50.142 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - submitMissingTasks(ResultStage 4)
16:03:50.152 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_7 stored as values in memory (estimated size 38.2 KiB, free 4.0 GiB)
16:03:50.152 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager - Put block broadcast_7 locally took 0 ms
16:03:50.152 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager - Putting block broadcast_7 without replication took 0 ms
16:03:50.153 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_7_piece0 stored as bytes in memory (estimated size 18.2 KiB, free 4.0 GiB)
16:03:50.153 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_7_piece0 for BlockManagerId(driver, 192.168.31.69, 58410, None)
16:03:50.153 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_7_piece0 in memory on 192.168.31.69:58410 (size: 18.2 KiB, free: 4.1 GiB)
16:03:50.153 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_7_piece0
16:03:50.153 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager - Told master about block broadcast_7_piece0
16:03:50.153 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager - Put block broadcast_7_piece0 locally took 0 ms
16:03:50.153 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager - Putting block broadcast_7_piece0 without replication took 0 ms
16:03:50.153 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 7 from broadcast at DAGScheduler.scala:1513
16:03:50.153 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[16] at show at PhysicalPlanSpec.scala:191) (first 15 tasks are for partitions Vector(0))
16:03:50.153 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 4.0 with 1 tasks resource profile 0
16:03:50.154 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.TaskSetManager - Epoch for TaskSet 4.0: 1
16:03:50.154 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.TaskSetManager - Adding pending tasks took 0 ms
16:03:50.154 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.TaskSetManager - Valid locality levels for TaskSet 4.0: NO_PREF, ANY
16:03:50.155 [dispatcher-event-loop-0] DEBUG org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_4.0, runningTasks: 0
16:03:50.155 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 4.0 (TID 3) (192.168.31.69, executor driver, partition 0, PROCESS_LOCAL, 5117 bytes) taskResourceAssignments Map()
16:03:50.155 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 4.0 (TID 3)
16:03:50.155 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] DEBUG org.apache.spark.executor.ExecutorMetricsPoller - stageTCMP: (4, 0) -> 1
16:03:50.156 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] DEBUG org.apache.spark.storage.BlockManager - Getting local block broadcast_7
16:03:50.156 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] DEBUG org.apache.spark.storage.BlockManager - Level for block broadcast_7 is StorageLevel(disk, memory, deserialized, 1 replicas)
16:03:50.188 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 8.170452 ms
16:03:50.189 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] DEBUG org.apache.spark.storage.BlockManager - Getting local block broadcast_2
16:03:50.189 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] DEBUG org.apache.spark.storage.BlockManager - Level for block broadcast_2 is StorageLevel(disk, memory, deserialized, 1 replicas)
16:03:50.197 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 7.208525 ms
16:03:50.215 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 11.373274 ms
org.apache.spark.sql.catalyst.expressions.codegen.package$ExpressionCanonicalizer$@7238e327 org.apache.spark.sql.catalyst.expressions.codegen.package$ExpressionCanonicalizer$CleanExpressions --> old【
cast(input[0, int, true] as string) AS id#43】--> new【
cast(input[0, int, true] as string)】


RuleExecutor.execute[org.apache.spark.sql.catalyst.expressions.codegen.package$ExpressionCanonicalizer$@7238e327] final result ==> old【
cast(input[0, int, true] as string) AS id#43】==> new【
cast(input[0, int, true] as string)】


org.apache.spark.sql.catalyst.expressions.codegen.package$ExpressionCanonicalizer$@7238e327 org.apache.spark.sql.catalyst.expressions.codegen.package$ExpressionCanonicalizer$CleanExpressions --> old【
cast(input[1, int, true] as string) AS value#44】--> new【
cast(input[1, int, true] as string)】


RuleExecutor.execute[org.apache.spark.sql.catalyst.expressions.codegen.package$ExpressionCanonicalizer$@7238e327] final result ==> old【
cast(input[1, int, true] as string) AS value#44】==> new【
cast(input[1, int, true] as string)】


16:03:50.230 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 10.459069 ms
16:03:50.232 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-warehouse/fact_table/date=2023-01-01/part-00000-a8c14554-6e1b-4585-bb2b-b356b699142a.c000.snappy.parquet, range: 0-666, partition values: [2023-01-01]
16:03:50.232 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] DEBUG org.apache.spark.storage.BlockManager - Getting local block broadcast_6
16:03:50.232 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] DEBUG org.apache.spark.storage.BlockManager - Level for block broadcast_6 is StorageLevel(disk, memory, deserialized, 1 replicas)
16:03:50.237 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] DEBUG org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport - Going to read the following fields from the Parquet file with the following schema:
Parquet file schema:
message spark_schema {
  required int32 id;
  required int32 value;
}

Parquet clipped schema:
message spark_schema {
  required int32 id;
  required int32 value;
}

Parquet requested schema:
message spark_schema {
  required int32 id;
  required int32 value;
}

Catalyst requested schema:
root
-- id: integer (nullable = true)
-- value: integer (nullable = true)

       
16:03:50.237 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] DEBUG org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat - Appending StructType(StructField(date,StringType,true)) [2023-01-01]
16:03:50.245 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 4.0 (TID 3). 4858 bytes result sent to driver
16:03:50.245 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] DEBUG org.apache.spark.executor.ExecutorMetricsPoller - stageTCMP: (4, 0) -> 0
16:03:50.246 [dispatcher-event-loop-0] DEBUG org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_4.0, runningTasks: 0
16:03:50.246 [dispatcher-event-loop-0] DEBUG org.apache.spark.scheduler.TaskSetManager - No tasks for locality level NO_PREF, so moving to locality level ANY
16:03:50.246 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 4.0 (TID 3) in 91 ms on 192.168.31.69 (executor driver) (1/1)
16:03:50.246 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 4.0, whose tasks have all completed, from pool 
16:03:50.247 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 4 (show at PhysicalPlanSpec.scala:191) finished in 0.105 s
16:03:50.247 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - After removal of stage 4, remaining stages = 0
16:03:50.247 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
16:03:50.247 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 4: Stage finished
16:03:50.247 [ScalaTest-run-running-PhysicalPlanSpec] INFO  org.apache.spark.scheduler.DAGScheduler - Job 3 finished: show at PhysicalPlanSpec.scala:191, took 0.106889 s
16:03:50.248 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec - withFinalPlanUpdate.executeCollect was finished.
16:03:50.253 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec - Final plan: CollectLimit 21
+- Project [cast(id#30 as string) AS id#43, cast(value#31 as string) AS value#44, event#34]
   +- BroadcastHashJoin [date#32], [date#33], Inner, BuildRight, false
      :- FileScan parquet default.fact_table[id#30,value#31,date#32] Batched: false, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(3 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [isnotnull(date#32), dynamicpruningexpression(date#32 IN dynamicpruning#49)], PushedFilters: [], ReadSchema: struct<id:int,value:int>
      :     +- Subquery dynamicpruning#49, [id=#118]
      :        +- AdaptiveSparkPlan isFinalPlan=true
                  +- == Final Plan ==
                     HashAggregate(keys=[date#33#54], functions=[], output=[date#33#54])
                     +- AQEShuffleRead coalesced
                        +- ShuffleQueryStage 0
                           +- Exchange hashpartitioning(date#33#54, 200), ENSURE_REQUIREMENTS, [plan_id=116]
                              +- HashAggregate(keys=[date#33 AS date#33#54], functions=[], output=[date#33#54])
                                 +- Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
                                    +- FileScan parquet default.dim_table[date#33,event#34] Batched: false, DataFilters: [isnotnull(event#34), (event#34 = New Year), isnotnull(date#33)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(event), EqualTo(event,New Year), IsNotNull(date)], ReadSchema: struct<date:string,event:string>
                  +- == Initial Plan ==
                     HashAggregate(keys=[date#33#54], functions=[], output=[date#33#54])
                     +- Exchange hashpartitioning(date#33#54, 200), ENSURE_REQUIREMENTS, [plan_id=116]
                        +- HashAggregate(keys=[date#33 AS date#33#54], functions=[], output=[date#33#54])
                           +- Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
                              +- FileScan parquet default.dim_table[date#33,event#34] Batched: false, DataFilters: [isnotnull(event#34), (event#34 = New Year), isnotnull(date#33)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(event), EqualTo(event,New Year), IsNotNull(date)], ReadSchema: struct<date:string,event:string>
      +- BroadcastQueryStage 0
         +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, false]),false), [plan_id=70]
            +- Filter ((isnotnull(event#34) AND (event#34 = New Year)) AND isnotnull(date#33))
               +- FileScan parquet default.dim_table[date#33,event#34] Batched: false, DataFilters: [isnotnull(event#34), (event#34 = New Year), isnotnull(date#33)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(event), EqualTo(event,New Year), IsNotNull(date)], ReadSchema: struct<date:string,event:string>

16:03:50.253 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec - withFinalPlanUpdate.executeCollect.finalPlanUpdate was finished.
16:03:50.273 [ScalaTest-run-running-PhysicalPlanSpec] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 9.287248 ms
+---+-----+--------+
| id|value|   event|
+---+-----+--------+
|  1|  100|New Year|
+---+-----+--------+
