21:32:43.301 [ScalaTest-run-running-PhysicalPlanSpec] INFO  org.apache.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
21:32:43.303 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.internal.SharedState - Applying other initial session options to HadoopConf: spark.app.name -> LogicalPlanTest
21:32:43.303 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.internal.SharedState - Applying other initial session options to HadoopConf: spark.master -> local
21:32:43.303 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.internal.SharedState - Applying static initial session options to SparkConf: spark.sql.catalogImplementation -> hive
21:32:43.310 [ScalaTest-run-running-PhysicalPlanSpec] INFO  org.apache.spark.sql.internal.SharedState - Warehouse path is 'file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-warehouse'.
21:32:44.514 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.execution.SparkSqlParser - Parsing command: drop table if exists order_all
org.apache.spark.sql.catalyst.parser.SqlBaseParser$SingleStatementContext =>  droptableifexistsorder_all <EOF>
org.apache.spark.sql.catalyst.parser.SqlBaseParser$SingleStatementContext =>  droptableifexistsorder_all <EOF>
org.apache.spark.sql.catalyst.parser.SqlBaseParser$DropTableContext =>  drop table if exists order_all
org.apache.spark.sql.catalyst.parser.SqlBaseParser$MultipartIdentifierContext =>  order_all
org.apache.spark.sql.catalyst.parser.SqlBaseParser$MultipartIdentifierContext =>  order_all
Unresolved Logical Plan ====>
'DropTable true, false
+- 'UnresolvedTableOrView [order_all], DROP TABLE, true

21:32:44.824 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: spark_grouping_id
org.apache.spark.sql.catalyst.parser.SqlBaseParser$SingleMultipartIdentifierContext =>  spark_grouping_id <EOF>
org.apache.spark.sql.catalyst.parser.SqlBaseParser$MultipartIdentifierContext =>  spark_grouping_id
21:32:45.079 [ScalaTest-run-running-PhysicalPlanSpec] INFO  org.apache.spark.sql.hive.HiveUtils - Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.
21:32:45.590 [ScalaTest-run-running-PhysicalPlanSpec] INFO  org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 2.3.9) is file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-warehouse
21:32:45.826 [ScalaTest-run-running-PhysicalPlanSpec] WARN  org.apache.hadoop.hive.conf.HiveConf - HiveConf of name hive.stats.jdbc.timeout does not exist
21:32:45.827 [ScalaTest-run-running-PhysicalPlanSpec] WARN  org.apache.hadoop.hive.conf.HiveConf - HiveConf of name hive.stats.retries.wait does not exist
21:32:46.327 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG com.jolbox.bonecp.BoneCPDataSource - JDBC URL = jdbc:derby:;databaseName=metastore_db;create=true, Username = APP, partitions = 1, max (per partition) = 10, min (per partition) = 0, idle max age = 60 min, idle test period = 240 min, strategy = DEFAULT
21:32:47.833 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG com.jolbox.bonecp.BoneCPDataSource - JDBC URL = jdbc:derby:;databaseName=metastore_db;create=true, Username = APP, partitions = 1, max (per partition) = 10, min (per partition) = 0, idle max age = 60 min, idle test period = 240 min, strategy = DEFAULT
21:32:48.253 [ScalaTest-run-running-PhysicalPlanSpec] WARN  org.apache.hadoop.hive.metastore.ObjectStore - Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 2.3.0
21:32:48.253 [ScalaTest-run-running-PhysicalPlanSpec] WARN  org.apache.hadoop.hive.metastore.ObjectStore - setMetaStoreSchemaVersion called but recording version is disabled: version = 2.3.0, comment = Set by MetaStore juntao@192.168.31.69
21:32:48.693 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
org.apache.spark.sql.catalyst.parser.SqlBaseParser$SingleDataTypeContext =>  bigint <EOF>
org.apache.spark.sql.catalyst.parser.SqlBaseParser$PrimitiveDataTypeContext =>  bigint
21:32:48.700 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
org.apache.spark.sql.catalyst.parser.SqlBaseParser$SingleDataTypeContext =>  bigint <EOF>
org.apache.spark.sql.catalyst.parser.SqlBaseParser$PrimitiveDataTypeContext =>  bigint
21:32:48.700 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
org.apache.spark.sql.catalyst.parser.SqlBaseParser$SingleDataTypeContext =>  bigint <EOF>
org.apache.spark.sql.catalyst.parser.SqlBaseParser$PrimitiveDataTypeContext =>  bigint
21:32:48.701 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
org.apache.spark.sql.catalyst.parser.SqlBaseParser$SingleDataTypeContext =>  string <EOF>
org.apache.spark.sql.catalyst.parser.SqlBaseParser$PrimitiveDataTypeContext =>  string
21:32:48.701 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
org.apache.spark.sql.catalyst.parser.SqlBaseParser$SingleDataTypeContext =>  string <EOF>
org.apache.spark.sql.catalyst.parser.SqlBaseParser$PrimitiveDataTypeContext =>  string
21:32:48.701 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
org.apache.spark.sql.catalyst.parser.SqlBaseParser$SingleDataTypeContext =>  string <EOF>
org.apache.spark.sql.catalyst.parser.SqlBaseParser$PrimitiveDataTypeContext =>  string
21:32:48.724 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: array<string>
org.apache.spark.sql.catalyst.parser.SqlBaseParser$SingleDataTypeContext =>  array<string> <EOF>
org.apache.spark.sql.catalyst.parser.SqlBaseParser$ComplexDataTypeContext =>  array < string >
org.apache.spark.sql.catalyst.parser.SqlBaseParser$PrimitiveDataTypeContext =>  string
Analyzer(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$1@3180131e) org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations --> old【
'DropTable true, false
+- 'UnresolvedTableOrView [order_all], DROP TABLE, true
】--> new【
DropTable true, false
+- ResolvedTable V2SessionCatalog(spark_catalog), default.order_all, V1Table(default.order_all), [order_id#0L, user_id#1L, region_id#2L, order_num#3, user_name#4, region_name#5]
】


Analyzer(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$1@3180131e) org.apache.spark.sql.catalyst.analysis.ResolveSessionCatalog --> old【
DropTable true, false
+- ResolvedTable V2SessionCatalog(spark_catalog), default.order_all, V1Table(default.order_all), [order_id#0L, user_id#1L, region_id#2L, order_num#3, user_name#4, region_name#5]
】--> new【
DropTableCommand `default`.`order_all`, true, false, false
】


RuleExecutor.execute[Analyzer(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$1@3180131e)] final result ==> old【
'DropTable true, false
+- 'UnresolvedTableOrView [order_all], DROP TABLE, true
】==> new【
DropTableCommand `default`.`order_all`, true, false, false
】


analyzed ====>
DropTableCommand `default`.`order_all`, true, false, false

analyzed ====>
DropTableCommand `default`.`order_all`, true, false, false

optimizedPlan ====>
DropTableCommand `default`.`order_all`, true, false, false

SparkPlanner(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2@321bf4b1) - org.apache.spark.sql.execution.SparkStrategies$SpecialLimits$ -->【
ReturnAnswer
+- DropTableCommand `default`.`order_all`, true, false, false
】 --> 【
PlanLater DropTableCommand `default`.`order_all`, true, false, false
】

SparkPlanner(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2@321bf4b1) - org.apache.spark.sql.execution.SparkStrategies$BasicOperators$ -->【
DropTableCommand `default`.`order_all`, true, false, false
】 --> 【
Execute DropTableCommand
   +- DropTableCommand `default`.`order_all`, true, false, false
】

sparkPlan ====>
Execute DropTableCommand
   +- DropTableCommand `default`.`order_all`, true, false, false

org.apache.spark.sql.execution.QueryExecution$ prepareForExecution final result ==>【
Execute DropTableCommand
   +- DropTableCommand `default`.`order_all`, true, false, false
】 ==> 【
Execute DropTableCommand
   +- DropTableCommand `default`.`order_all`, true, false, false
】

executedPlan ====>
Execute DropTableCommand
   +- DropTableCommand `default`.`order_all`, true, false, false

21:32:51.013 [ScalaTest-run-running-PhysicalPlanSpec] WARN  org.apache.hadoop.hive.metastore.ObjectStore - Failed to get database global_temp, returning NoSuchObjectException
21:32:51.045 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
org.apache.spark.sql.catalyst.parser.SqlBaseParser$SingleDataTypeContext =>  bigint <EOF>
org.apache.spark.sql.catalyst.parser.SqlBaseParser$PrimitiveDataTypeContext =>  bigint
21:32:51.046 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
org.apache.spark.sql.catalyst.parser.SqlBaseParser$SingleDataTypeContext =>  bigint <EOF>
org.apache.spark.sql.catalyst.parser.SqlBaseParser$PrimitiveDataTypeContext =>  bigint
21:32:51.046 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
org.apache.spark.sql.catalyst.parser.SqlBaseParser$SingleDataTypeContext =>  bigint <EOF>
org.apache.spark.sql.catalyst.parser.SqlBaseParser$PrimitiveDataTypeContext =>  bigint
21:32:51.047 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
org.apache.spark.sql.catalyst.parser.SqlBaseParser$SingleDataTypeContext =>  string <EOF>
org.apache.spark.sql.catalyst.parser.SqlBaseParser$PrimitiveDataTypeContext =>  string
21:32:51.047 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
org.apache.spark.sql.catalyst.parser.SqlBaseParser$SingleDataTypeContext =>  string <EOF>
org.apache.spark.sql.catalyst.parser.SqlBaseParser$PrimitiveDataTypeContext =>  string
21:32:51.047 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
org.apache.spark.sql.catalyst.parser.SqlBaseParser$SingleDataTypeContext =>  string <EOF>
org.apache.spark.sql.catalyst.parser.SqlBaseParser$PrimitiveDataTypeContext =>  string
21:32:51.145 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
org.apache.spark.sql.catalyst.parser.SqlBaseParser$SingleDataTypeContext =>  bigint <EOF>
org.apache.spark.sql.catalyst.parser.SqlBaseParser$PrimitiveDataTypeContext =>  bigint
21:32:51.145 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
org.apache.spark.sql.catalyst.parser.SqlBaseParser$SingleDataTypeContext =>  bigint <EOF>
org.apache.spark.sql.catalyst.parser.SqlBaseParser$PrimitiveDataTypeContext =>  bigint
21:32:51.145 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
org.apache.spark.sql.catalyst.parser.SqlBaseParser$SingleDataTypeContext =>  bigint <EOF>
org.apache.spark.sql.catalyst.parser.SqlBaseParser$PrimitiveDataTypeContext =>  bigint
21:32:51.146 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
org.apache.spark.sql.catalyst.parser.SqlBaseParser$SingleDataTypeContext =>  string <EOF>
org.apache.spark.sql.catalyst.parser.SqlBaseParser$PrimitiveDataTypeContext =>  string
21:32:51.146 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
org.apache.spark.sql.catalyst.parser.SqlBaseParser$SingleDataTypeContext =>  string <EOF>
org.apache.spark.sql.catalyst.parser.SqlBaseParser$PrimitiveDataTypeContext =>  string
21:32:51.146 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
org.apache.spark.sql.catalyst.parser.SqlBaseParser$SingleDataTypeContext =>  string <EOF>
org.apache.spark.sql.catalyst.parser.SqlBaseParser$PrimitiveDataTypeContext =>  string
Analyzer(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$1@3180131e) org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations --> old【
'UnresolvedRelation [default, order_all], [], false
】--> new【
'SubqueryAlias spark_catalog.default.order_all
+- 'UnresolvedCatalogRelation `default`.`order_all`, org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, [], false
】


21:32:51.382 [ScalaTest-run-running-PhysicalPlanSpec] INFO  org.apache.spark.sql.execution.datasources.InMemoryFileIndex - It took 38 ms to list leaf files for 1 paths.
Analyzer(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$1@3180131e) org.apache.spark.sql.execution.datasources.FindDataSourceTable --> old【
'SubqueryAlias spark_catalog.default.order_all
+- 'UnresolvedCatalogRelation `default`.`order_all`, org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, [], false
】--> new【
SubqueryAlias spark_catalog.default.order_all
+- Relation default.order_all[order_id#6L,user_id#7L,region_id#8L,order_num#9,user_name#10,region_name#11] parquet
】


RuleExecutor.execute[Analyzer(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$1@3180131e)] final result ==> old【
'UnresolvedRelation [default, order_all], [], false
】==> new【
SubqueryAlias spark_catalog.default.order_all
+- Relation default.order_all[order_id#6L,user_id#7L,region_id#8L,order_num#9,user_name#10,region_name#11] parquet
】


analyzed ====>
SubqueryAlias spark_catalog.default.order_all
+- Relation default.order_all[order_id#6L,user_id#7L,region_id#8L,order_num#9,user_name#10,region_name#11] parquet

21:32:52.038 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.execution.SparkSqlParser - Parsing command: drop table if exists order
org.apache.spark.sql.catalyst.parser.SqlBaseParser$SingleStatementContext =>  droptableifexistsorder <EOF>
org.apache.spark.sql.catalyst.parser.SqlBaseParser$SingleStatementContext =>  droptableifexistsorder <EOF>
org.apache.spark.sql.catalyst.parser.SqlBaseParser$DropTableContext =>  drop table if exists order
org.apache.spark.sql.catalyst.parser.SqlBaseParser$MultipartIdentifierContext =>  order
org.apache.spark.sql.catalyst.parser.SqlBaseParser$MultipartIdentifierContext =>  order
Unresolved Logical Plan ====>
'DropTable true, false
+- 'UnresolvedTableOrView [order], DROP TABLE, true

Analyzer(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$1@3180131e) org.apache.spark.sql.catalyst.analysis.ResolveCommandsWithIfExists --> old【
'DropTable true, false
+- 'UnresolvedTableOrView [order], DROP TABLE, true
】--> new【
NoopCommand DROP TABLE, [order]
】


RuleExecutor.execute[Analyzer(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$1@3180131e)] final result ==> old【
'DropTable true, false
+- 'UnresolvedTableOrView [order], DROP TABLE, true
】==> new【
NoopCommand DROP TABLE, [order]
】


analyzed ====>
NoopCommand DROP TABLE, [order]

analyzed ====>
NoopCommand DROP TABLE, [order]

optimizedPlan ====>
NoopCommand DROP TABLE, [order]

SparkPlanner(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2@321bf4b1) - org.apache.spark.sql.execution.SparkStrategies$SpecialLimits$ -->【
ReturnAnswer
+- NoopCommand DROP TABLE, [order]
】 --> 【
PlanLater NoopCommand DROP TABLE, [order]
】

SparkPlanner(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2@321bf4b1) - org.apache.spark.sql.execution.datasources.v2.DataSourceV2Strategy -->【
NoopCommand DROP TABLE, [order]
】 --> 【
LocalTableScan <empty>
】

sparkPlan ====>
LocalTableScan <empty>

org.apache.spark.sql.execution.QueryExecution$ prepareForExecution final result ==>【
LocalTableScan <empty>
】 ==> 【
LocalTableScan <empty>
】

executedPlan ====>
LocalTableScan <empty>

21:32:52.070 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.execution.SparkSqlParser - Parsing command: drop table if exists user
org.apache.spark.sql.catalyst.parser.SqlBaseParser$SingleStatementContext =>  droptableifexistsuser <EOF>
org.apache.spark.sql.catalyst.parser.SqlBaseParser$SingleStatementContext =>  droptableifexistsuser <EOF>
org.apache.spark.sql.catalyst.parser.SqlBaseParser$DropTableContext =>  drop table if exists user
org.apache.spark.sql.catalyst.parser.SqlBaseParser$MultipartIdentifierContext =>  user
org.apache.spark.sql.catalyst.parser.SqlBaseParser$MultipartIdentifierContext =>  user
Unresolved Logical Plan ====>
'DropTable true, false
+- 'UnresolvedTableOrView [user], DROP TABLE, true

Analyzer(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$1@3180131e) org.apache.spark.sql.catalyst.analysis.ResolveCommandsWithIfExists --> old【
'DropTable true, false
+- 'UnresolvedTableOrView [user], DROP TABLE, true
】--> new【
NoopCommand DROP TABLE, [user]
】


RuleExecutor.execute[Analyzer(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$1@3180131e)] final result ==> old【
'DropTable true, false
+- 'UnresolvedTableOrView [user], DROP TABLE, true
】==> new【
NoopCommand DROP TABLE, [user]
】


analyzed ====>
NoopCommand DROP TABLE, [user]

analyzed ====>
NoopCommand DROP TABLE, [user]

optimizedPlan ====>
NoopCommand DROP TABLE, [user]

SparkPlanner(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2@321bf4b1) - org.apache.spark.sql.execution.SparkStrategies$SpecialLimits$ -->【
ReturnAnswer
+- NoopCommand DROP TABLE, [user]
】 --> 【
PlanLater NoopCommand DROP TABLE, [user]
】

SparkPlanner(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2@321bf4b1) - org.apache.spark.sql.execution.datasources.v2.DataSourceV2Strategy -->【
NoopCommand DROP TABLE, [user]
】 --> 【
LocalTableScan <empty>
】

sparkPlan ====>
LocalTableScan <empty>

org.apache.spark.sql.execution.QueryExecution$ prepareForExecution final result ==>【
LocalTableScan <empty>
】 ==> 【
LocalTableScan <empty>
】

executedPlan ====>
LocalTableScan <empty>

21:32:52.083 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.execution.SparkSqlParser - Parsing command: drop table if exists region
org.apache.spark.sql.catalyst.parser.SqlBaseParser$SingleStatementContext =>  droptableifexistsregion <EOF>
org.apache.spark.sql.catalyst.parser.SqlBaseParser$SingleStatementContext =>  droptableifexistsregion <EOF>
org.apache.spark.sql.catalyst.parser.SqlBaseParser$DropTableContext =>  drop table if exists region
org.apache.spark.sql.catalyst.parser.SqlBaseParser$MultipartIdentifierContext =>  region
org.apache.spark.sql.catalyst.parser.SqlBaseParser$MultipartIdentifierContext =>  region
Unresolved Logical Plan ====>
'DropTable true, false
+- 'UnresolvedTableOrView [region], DROP TABLE, true

Analyzer(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$1@3180131e) org.apache.spark.sql.catalyst.analysis.ResolveCommandsWithIfExists --> old【
'DropTable true, false
+- 'UnresolvedTableOrView [region], DROP TABLE, true
】--> new【
NoopCommand DROP TABLE, [region]
】


RuleExecutor.execute[Analyzer(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$1@3180131e)] final result ==> old【
'DropTable true, false
+- 'UnresolvedTableOrView [region], DROP TABLE, true
】==> new【
NoopCommand DROP TABLE, [region]
】


analyzed ====>
NoopCommand DROP TABLE, [region]

analyzed ====>
NoopCommand DROP TABLE, [region]

optimizedPlan ====>
NoopCommand DROP TABLE, [region]

SparkPlanner(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2@321bf4b1) - org.apache.spark.sql.execution.SparkStrategies$SpecialLimits$ -->【
ReturnAnswer
+- NoopCommand DROP TABLE, [region]
】 --> 【
PlanLater NoopCommand DROP TABLE, [region]
】

SparkPlanner(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2@321bf4b1) - org.apache.spark.sql.execution.datasources.v2.DataSourceV2Strategy -->【
NoopCommand DROP TABLE, [region]
】 --> 【
LocalTableScan <empty>
】

sparkPlan ====>
LocalTableScan <empty>

org.apache.spark.sql.execution.QueryExecution$ prepareForExecution final result ==>【
LocalTableScan <empty>
】 ==> 【
LocalTableScan <empty>
】

executedPlan ====>
LocalTableScan <empty>

21:32:52.095 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.execution.SparkSqlParser - Parsing command: SET spark.sql.autoBroadcastJoinThreshold = -1
org.apache.spark.sql.catalyst.parser.SqlBaseParser$SingleStatementContext =>  SETspark.sql.autoBroadcastJoinThreshold=-1 <EOF>
org.apache.spark.sql.catalyst.parser.SqlBaseParser$SingleStatementContext =>  SETspark.sql.autoBroadcastJoinThreshold=-1 <EOF>
org.apache.spark.sql.catalyst.parser.SqlBaseParser$SetConfigurationContext =>  SET spark . sql . autoBroadcastJoinThreshold = - 1
Unresolved Logical Plan ====>
SetCommand (spark.sql.autoBroadcastJoinThreshold,Some(-1))

analyzed ====>
SetCommand (spark.sql.autoBroadcastJoinThreshold,Some(-1))

analyzed ====>
SetCommand (spark.sql.autoBroadcastJoinThreshold,Some(-1))

optimizedPlan ====>
SetCommand (spark.sql.autoBroadcastJoinThreshold,Some(-1))

SparkPlanner(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2@321bf4b1) - org.apache.spark.sql.execution.SparkStrategies$SpecialLimits$ -->【
ReturnAnswer
+- SetCommand (spark.sql.autoBroadcastJoinThreshold,Some(-1))
】 --> 【
PlanLater SetCommand (spark.sql.autoBroadcastJoinThreshold,Some(-1))
】

SparkPlanner(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2@321bf4b1) - org.apache.spark.sql.execution.SparkStrategies$BasicOperators$ -->【
SetCommand (spark.sql.autoBroadcastJoinThreshold,Some(-1))
】 --> 【
Execute SetCommand
   +- SetCommand (spark.sql.autoBroadcastJoinThreshold,Some(-1))
】

sparkPlan ====>
Execute SetCommand
   +- SetCommand (spark.sql.autoBroadcastJoinThreshold,Some(-1))

org.apache.spark.sql.execution.QueryExecution$ prepareForExecution final result ==>【
Execute SetCommand
   +- SetCommand (spark.sql.autoBroadcastJoinThreshold,Some(-1))
】 ==> 【
Execute SetCommand
   +- SetCommand (spark.sql.autoBroadcastJoinThreshold,Some(-1))
】

executedPlan ====>
Execute SetCommand
   +- SetCommand (spark.sql.autoBroadcastJoinThreshold,Some(-1))

21:32:52.120 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.execution.SparkSqlParser - Parsing command: SET spark.sql.shuffle.partitions = 8
org.apache.spark.sql.catalyst.parser.SqlBaseParser$SingleStatementContext =>  SETspark.sql.shuffle.partitions=8 <EOF>
org.apache.spark.sql.catalyst.parser.SqlBaseParser$SingleStatementContext =>  SETspark.sql.shuffle.partitions=8 <EOF>
org.apache.spark.sql.catalyst.parser.SqlBaseParser$SetConfigurationContext =>  SET spark . sql . shuffle . partitions = 8
Unresolved Logical Plan ====>
SetCommand (spark.sql.shuffle.partitions,Some(8))

analyzed ====>
SetCommand (spark.sql.shuffle.partitions,Some(8))

analyzed ====>
SetCommand (spark.sql.shuffle.partitions,Some(8))

optimizedPlan ====>
SetCommand (spark.sql.shuffle.partitions,Some(8))

SparkPlanner(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2@321bf4b1) - org.apache.spark.sql.execution.SparkStrategies$SpecialLimits$ -->【
ReturnAnswer
+- SetCommand (spark.sql.shuffle.partitions,Some(8))
】 --> 【
PlanLater SetCommand (spark.sql.shuffle.partitions,Some(8))
】

SparkPlanner(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2@321bf4b1) - org.apache.spark.sql.execution.SparkStrategies$BasicOperators$ -->【
SetCommand (spark.sql.shuffle.partitions,Some(8))
】 --> 【
Execute SetCommand
   +- SetCommand (spark.sql.shuffle.partitions,Some(8))
】

sparkPlan ====>
Execute SetCommand
   +- SetCommand (spark.sql.shuffle.partitions,Some(8))

org.apache.spark.sql.execution.QueryExecution$ prepareForExecution final result ==>【
Execute SetCommand
   +- SetCommand (spark.sql.shuffle.partitions,Some(8))
】 ==> 【
Execute SetCommand
   +- SetCommand (spark.sql.shuffle.partitions,Some(8))
】

executedPlan ====>
Execute SetCommand
   +- SetCommand (spark.sql.shuffle.partitions,Some(8))

21:32:52.130 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.execution.SparkSqlParser - Parsing command: SET spark.sql.cbo.enabled = true
org.apache.spark.sql.catalyst.parser.SqlBaseParser$SingleStatementContext =>  SETspark.sql.cbo.enabled=true <EOF>
org.apache.spark.sql.catalyst.parser.SqlBaseParser$SingleStatementContext =>  SETspark.sql.cbo.enabled=true <EOF>
org.apache.spark.sql.catalyst.parser.SqlBaseParser$SetConfigurationContext =>  SET spark . sql . cbo . enabled = true
Unresolved Logical Plan ====>
SetCommand (spark.sql.cbo.enabled,Some(true))

analyzed ====>
SetCommand (spark.sql.cbo.enabled,Some(true))

analyzed ====>
SetCommand (spark.sql.cbo.enabled,Some(true))

optimizedPlan ====>
SetCommand (spark.sql.cbo.enabled,Some(true))

SparkPlanner(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2@321bf4b1) - org.apache.spark.sql.execution.SparkStrategies$SpecialLimits$ -->【
ReturnAnswer
+- SetCommand (spark.sql.cbo.enabled,Some(true))
】 --> 【
PlanLater SetCommand (spark.sql.cbo.enabled,Some(true))
】

SparkPlanner(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2@321bf4b1) - org.apache.spark.sql.execution.SparkStrategies$BasicOperators$ -->【
SetCommand (spark.sql.cbo.enabled,Some(true))
】 --> 【
Execute SetCommand
   +- SetCommand (spark.sql.cbo.enabled,Some(true))
】

sparkPlan ====>
Execute SetCommand
   +- SetCommand (spark.sql.cbo.enabled,Some(true))

org.apache.spark.sql.execution.QueryExecution$ prepareForExecution final result ==>【
Execute SetCommand
   +- SetCommand (spark.sql.cbo.enabled,Some(true))
】 ==> 【
Execute SetCommand
   +- SetCommand (spark.sql.cbo.enabled,Some(true))
】

executedPlan ====>
Execute SetCommand
   +- SetCommand (spark.sql.cbo.enabled,Some(true))

21:32:52.139 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.execution.SparkSqlParser - Parsing command: SET spark.sql.cbo.joinReorder.enabled = true
org.apache.spark.sql.catalyst.parser.SqlBaseParser$SingleStatementContext =>  SETspark.sql.cbo.joinReorder.enabled=true <EOF>
org.apache.spark.sql.catalyst.parser.SqlBaseParser$SingleStatementContext =>  SETspark.sql.cbo.joinReorder.enabled=true <EOF>
org.apache.spark.sql.catalyst.parser.SqlBaseParser$SetConfigurationContext =>  SET spark . sql . cbo . joinReorder . enabled = true
Unresolved Logical Plan ====>
SetCommand (spark.sql.cbo.joinReorder.enabled,Some(true))

analyzed ====>
SetCommand (spark.sql.cbo.joinReorder.enabled,Some(true))

analyzed ====>
SetCommand (spark.sql.cbo.joinReorder.enabled,Some(true))

optimizedPlan ====>
SetCommand (spark.sql.cbo.joinReorder.enabled,Some(true))

SparkPlanner(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2@321bf4b1) - org.apache.spark.sql.execution.SparkStrategies$SpecialLimits$ -->【
ReturnAnswer
+- SetCommand (spark.sql.cbo.joinReorder.enabled,Some(true))
】 --> 【
PlanLater SetCommand (spark.sql.cbo.joinReorder.enabled,Some(true))
】

SparkPlanner(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2@321bf4b1) - org.apache.spark.sql.execution.SparkStrategies$BasicOperators$ -->【
SetCommand (spark.sql.cbo.joinReorder.enabled,Some(true))
】 --> 【
Execute SetCommand
   +- SetCommand (spark.sql.cbo.joinReorder.enabled,Some(true))
】

sparkPlan ====>
Execute SetCommand
   +- SetCommand (spark.sql.cbo.joinReorder.enabled,Some(true))

org.apache.spark.sql.execution.QueryExecution$ prepareForExecution final result ==>【
Execute SetCommand
   +- SetCommand (spark.sql.cbo.joinReorder.enabled,Some(true))
】 ==> 【
Execute SetCommand
   +- SetCommand (spark.sql.cbo.joinReorder.enabled,Some(true))
】

executedPlan ====>
Execute SetCommand
   +- SetCommand (spark.sql.cbo.joinReorder.enabled,Some(true))

analyzed ====>
Range (0, 10000, step=1, splits=Some(10))

21:32:52.252 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.execution.SparkSqlParser - Parsing command: id AS order_id
org.apache.spark.sql.catalyst.parser.SqlBaseParser$SingleExpressionContext =>  idASorder_id <EOF>
org.apache.spark.sql.catalyst.parser.SqlBaseParser$SingleExpressionContext =>  idASorder_id <EOF>
org.apache.spark.sql.catalyst.parser.SqlBaseParser$NamedExpressionContext =>  id AS order_id
org.apache.spark.sql.catalyst.parser.SqlBaseParser$PredicatedContext =>  id
org.apache.spark.sql.catalyst.parser.SqlBaseParser$ColumnReferenceContext =>  id
21:32:52.278 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.execution.SparkSqlParser - Parsing command: id % 1000 AS user_id
org.apache.spark.sql.catalyst.parser.SqlBaseParser$SingleExpressionContext =>  id%1000ASuser_id <EOF>
org.apache.spark.sql.catalyst.parser.SqlBaseParser$SingleExpressionContext =>  id%1000ASuser_id <EOF>
org.apache.spark.sql.catalyst.parser.SqlBaseParser$NamedExpressionContext =>  id%1000 AS user_id
org.apache.spark.sql.catalyst.parser.SqlBaseParser$PredicatedContext =>  id%1000
org.apache.spark.sql.catalyst.parser.SqlBaseParser$ArithmeticBinaryContext =>  id % 1000
org.apache.spark.sql.catalyst.parser.SqlBaseParser$ColumnReferenceContext =>  id
org.apache.spark.sql.catalyst.parser.SqlBaseParser$IntegerLiteralContext =>  1000
21:32:52.286 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.execution.SparkSqlParser - Parsing command: id % 10 AS region_id
org.apache.spark.sql.catalyst.parser.SqlBaseParser$SingleExpressionContext =>  id%10ASregion_id <EOF>
org.apache.spark.sql.catalyst.parser.SqlBaseParser$SingleExpressionContext =>  id%10ASregion_id <EOF>
org.apache.spark.sql.catalyst.parser.SqlBaseParser$NamedExpressionContext =>  id%10 AS region_id
org.apache.spark.sql.catalyst.parser.SqlBaseParser$PredicatedContext =>  id%10
org.apache.spark.sql.catalyst.parser.SqlBaseParser$ArithmeticBinaryContext =>  id % 10
org.apache.spark.sql.catalyst.parser.SqlBaseParser$ColumnReferenceContext =>  id
org.apache.spark.sql.catalyst.parser.SqlBaseParser$IntegerLiteralContext =>  10
21:32:52.287 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.execution.SparkSqlParser - Parsing command: CAST(id AS STRING) AS order_num
org.apache.spark.sql.catalyst.parser.SqlBaseParser$SingleExpressionContext =>  CAST(idASSTRING)ASorder_num <EOF>
org.apache.spark.sql.catalyst.parser.SqlBaseParser$SingleExpressionContext =>  CAST(idASSTRING)ASorder_num <EOF>
org.apache.spark.sql.catalyst.parser.SqlBaseParser$NamedExpressionContext =>  CAST(idASSTRING) AS order_num
org.apache.spark.sql.catalyst.parser.SqlBaseParser$PredicatedContext =>  CAST(idASSTRING)
org.apache.spark.sql.catalyst.parser.SqlBaseParser$CastContext =>  CAST ( id AS STRING )
org.apache.spark.sql.catalyst.parser.SqlBaseParser$PrimitiveDataTypeContext =>  STRING
org.apache.spark.sql.catalyst.parser.SqlBaseParser$PredicatedContext =>  id
org.apache.spark.sql.catalyst.parser.SqlBaseParser$ColumnReferenceContext =>  id
Analyzer(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$1@3180131e) org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences --> old【
'Project ['id AS order_id#60, ('id % 1000) AS user_id#61, ('id % 10) AS region_id#62, cast('id as string) AS order_num#63]
+- Range (0, 10000, step=1, splits=Some(10))
】--> new【
'Project [id#58L AS order_id#60L, (id#58L % 1000) AS user_id#61, (id#58L % 10) AS region_id#62, cast(id#58L as string) AS order_num#63]
+- Range (0, 10000, step=1, splits=Some(10))
】


Analyzer(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$1@3180131e) org.apache.spark.sql.catalyst.analysis.ResolveTimeZone --> old【
'Project [id#58L AS order_id#60L, (id#58L % 1000) AS user_id#61, (id#58L % 10) AS region_id#62, cast(id#58L as string) AS order_num#63]
+- Range (0, 10000, step=1, splits=Some(10))
】--> new【
'Project [id#58L AS order_id#60L, (id#58L % 1000) AS user_id#61, (id#58L % 10) AS region_id#62, cast(id#58L as string) AS order_num#63]
+- Range (0, 10000, step=1, splits=Some(10))
】


Analyzer(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$1@3180131e) org.apache.spark.sql.catalyst.analysis.TypeCoercionBase$CombinedTypeCoercionRule --> old【
'Project [id#58L AS order_id#60L, (id#58L % 1000) AS user_id#61, (id#58L % 10) AS region_id#62, cast(id#58L as string) AS order_num#63]
+- Range (0, 10000, step=1, splits=Some(10))
】--> new【
Project [id#58L AS order_id#60L, (id#58L % cast(1000 as bigint)) AS user_id#61L, (id#58L % cast(10 as bigint)) AS region_id#62L, cast(id#58L as string) AS order_num#63]
+- Range (0, 10000, step=1, splits=Some(10))
】


Analyzer(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$1@3180131e) org.apache.spark.sql.catalyst.analysis.ResolveTimeZone --> old【
Project [id#58L AS order_id#60L, (id#58L % cast(1000 as bigint)) AS user_id#61L, (id#58L % cast(10 as bigint)) AS region_id#62L, cast(id#58L as string) AS order_num#63]
+- Range (0, 10000, step=1, splits=Some(10))
】--> new【
Project [id#58L AS order_id#60L, (id#58L % cast(1000 as bigint)) AS user_id#61L, (id#58L % cast(10 as bigint)) AS region_id#62L, cast(id#58L as string) AS order_num#63]
+- Range (0, 10000, step=1, splits=Some(10))
】


RuleExecutor.execute[Analyzer(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$1@3180131e)] final result ==> old【
'Project ['id AS order_id#60, ('id % 1000) AS user_id#61, ('id % 10) AS region_id#62, cast('id as string) AS order_num#63]
+- Range (0, 10000, step=1, splits=Some(10))
】==> new【
Project [id#58L AS order_id#60L, (id#58L % cast(1000 as bigint)) AS user_id#61L, (id#58L % cast(10 as bigint)) AS region_id#62L, cast(id#58L as string) AS order_num#63]
+- Range (0, 10000, step=1, splits=Some(10))
】


analyzed ====>
Project [id#58L AS order_id#60L, (id#58L % cast(1000 as bigint)) AS user_id#61L, (id#58L % cast(10 as bigint)) AS region_id#62L, cast(id#58L as string) AS order_num#63]
+- Range (0, 10000, step=1, splits=Some(10))

21:32:52.379 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.execution.SparkSqlParser - Parsing command: order
org.apache.spark.sql.catalyst.parser.SqlBaseParser$SingleTableIdentifierContext =>  order <EOF>
org.apache.spark.sql.catalyst.parser.SqlBaseParser$TableIdentifierContext =>  order
analyzed ====>
CreateViewCommand `order`, false, true, LocalTempView, true
   +- Project [id#58L AS order_id#60L, (id#58L % cast(1000 as bigint)) AS user_id#61L, (id#58L % cast(10 as bigint)) AS region_id#62L, cast(id#58L as string) AS order_num#63]
      +- Range (0, 10000, step=1, splits=Some(10))

analyzed ====>
CreateViewCommand `order`, false, true, LocalTempView, true
   +- Project [id#58L AS order_id#60L, (id#58L % cast(1000 as bigint)) AS user_id#61L, (id#58L % cast(10 as bigint)) AS region_id#62L, cast(id#58L as string) AS order_num#63]
      +- Range (0, 10000, step=1, splits=Some(10))

optimizedPlan ====>
CreateViewCommand `order`, false, true, LocalTempView, true
   +- Project [id#58L AS order_id#60L, (id#58L % cast(1000 as bigint)) AS user_id#61L, (id#58L % cast(10 as bigint)) AS region_id#62L, cast(id#58L as string) AS order_num#63]
      +- Range (0, 10000, step=1, splits=Some(10))

SparkPlanner(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2@321bf4b1) - org.apache.spark.sql.execution.SparkStrategies$SpecialLimits$ -->【
ReturnAnswer
+- CreateViewCommand `order`, false, true, LocalTempView, true
      +- Project [id#58L AS order_id#60L, (id#58L % cast(1000 as bigint)) AS user_id#61L, (id#58L % cast(10 as bigint)) AS region_id#62L, cast(id#58L as string) AS order_num#63]
         +- Range (0, 10000, step=1, splits=Some(10))
】 --> 【
PlanLater CreateViewCommand `order`, false, true, LocalTempView, true
】

SparkPlanner(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2@321bf4b1) - org.apache.spark.sql.execution.SparkStrategies$BasicOperators$ -->【
CreateViewCommand `order`, false, true, LocalTempView, true
   +- Project [id#58L AS order_id#60L, (id#58L % cast(1000 as bigint)) AS user_id#61L, (id#58L % cast(10 as bigint)) AS region_id#62L, cast(id#58L as string) AS order_num#63]
      +- Range (0, 10000, step=1, splits=Some(10))
】 --> 【
Execute CreateViewCommand
   +- CreateViewCommand `order`, false, true, LocalTempView, true
         +- Project [id#58L AS order_id#60L, (id#58L % cast(1000 as bigint)) AS user_id#61L, (id#58L % cast(10 as bigint)) AS region_id#62L, cast(id#58L as string) AS order_num#63]
            +- Range (0, 10000, step=1, splits=Some(10))
】

sparkPlan ====>
Execute CreateViewCommand
   +- CreateViewCommand `order`, false, true, LocalTempView, true
         +- Project [id#58L AS order_id#60L, (id#58L % cast(1000 as bigint)) AS user_id#61L, (id#58L % cast(10 as bigint)) AS region_id#62L, cast(id#58L as string) AS order_num#63]
            +- Range (0, 10000, step=1, splits=Some(10))

org.apache.spark.sql.execution.QueryExecution$ prepareForExecution final result ==>【
Execute CreateViewCommand
   +- CreateViewCommand `order`, false, true, LocalTempView, true
         +- Project [id#58L AS order_id#60L, (id#58L % cast(1000 as bigint)) AS user_id#61L, (id#58L % cast(10 as bigint)) AS region_id#62L, cast(id#58L as string) AS order_num#63]
            +- Range (0, 10000, step=1, splits=Some(10))
】 ==> 【
Execute CreateViewCommand
   +- CreateViewCommand `order`, false, true, LocalTempView, true
         +- Project [id#58L AS order_id#60L, (id#58L % cast(1000 as bigint)) AS user_id#61L, (id#58L % cast(10 as bigint)) AS region_id#62L, cast(id#58L as string) AS order_num#63]
            +- Range (0, 10000, step=1, splits=Some(10))
】

executedPlan ====>
Execute CreateViewCommand
   +- CreateViewCommand `order`, false, true, LocalTempView, true
         +- Project [id#58L AS order_id#60L, (id#58L % cast(1000 as bigint)) AS user_id#61L, (id#58L % cast(10 as bigint)) AS region_id#62L, cast(id#58L as string) AS order_num#63]
            +- Range (0, 10000, step=1, splits=Some(10))

analyzed ====>
Range (0, 1000, step=1, splits=Some(5))

21:32:52.417 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.execution.SparkSqlParser - Parsing command: id AS user_id
org.apache.spark.sql.catalyst.parser.SqlBaseParser$SingleExpressionContext =>  idASuser_id <EOF>
org.apache.spark.sql.catalyst.parser.SqlBaseParser$SingleExpressionContext =>  idASuser_id <EOF>
org.apache.spark.sql.catalyst.parser.SqlBaseParser$NamedExpressionContext =>  id AS user_id
org.apache.spark.sql.catalyst.parser.SqlBaseParser$PredicatedContext =>  id
org.apache.spark.sql.catalyst.parser.SqlBaseParser$ColumnReferenceContext =>  id
21:32:52.417 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.execution.SparkSqlParser - Parsing command: id % 10 AS region_id
org.apache.spark.sql.catalyst.parser.SqlBaseParser$SingleExpressionContext =>  id%10ASregion_id <EOF>
org.apache.spark.sql.catalyst.parser.SqlBaseParser$SingleExpressionContext =>  id%10ASregion_id <EOF>
org.apache.spark.sql.catalyst.parser.SqlBaseParser$NamedExpressionContext =>  id%10 AS region_id
org.apache.spark.sql.catalyst.parser.SqlBaseParser$PredicatedContext =>  id%10
org.apache.spark.sql.catalyst.parser.SqlBaseParser$ArithmeticBinaryContext =>  id % 10
org.apache.spark.sql.catalyst.parser.SqlBaseParser$ColumnReferenceContext =>  id
org.apache.spark.sql.catalyst.parser.SqlBaseParser$IntegerLiteralContext =>  10
21:32:52.418 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.execution.SparkSqlParser - Parsing command: CAST(id AS STRING) AS user_name
org.apache.spark.sql.catalyst.parser.SqlBaseParser$SingleExpressionContext =>  CAST(idASSTRING)ASuser_name <EOF>
org.apache.spark.sql.catalyst.parser.SqlBaseParser$SingleExpressionContext =>  CAST(idASSTRING)ASuser_name <EOF>
org.apache.spark.sql.catalyst.parser.SqlBaseParser$NamedExpressionContext =>  CAST(idASSTRING) AS user_name
org.apache.spark.sql.catalyst.parser.SqlBaseParser$PredicatedContext =>  CAST(idASSTRING)
org.apache.spark.sql.catalyst.parser.SqlBaseParser$CastContext =>  CAST ( id AS STRING )
org.apache.spark.sql.catalyst.parser.SqlBaseParser$PrimitiveDataTypeContext =>  STRING
org.apache.spark.sql.catalyst.parser.SqlBaseParser$PredicatedContext =>  id
org.apache.spark.sql.catalyst.parser.SqlBaseParser$ColumnReferenceContext =>  id
Analyzer(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$1@3180131e) org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences --> old【
'Project ['id AS user_id#70, ('id % 10) AS region_id#71, cast('id as string) AS user_name#72]
+- Range (0, 1000, step=1, splits=Some(5))
】--> new【
'Project [id#68L AS user_id#70L, (id#68L % 10) AS region_id#71, cast(id#68L as string) AS user_name#72]
+- Range (0, 1000, step=1, splits=Some(5))
】


Analyzer(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$1@3180131e) org.apache.spark.sql.catalyst.analysis.ResolveTimeZone --> old【
'Project [id#68L AS user_id#70L, (id#68L % 10) AS region_id#71, cast(id#68L as string) AS user_name#72]
+- Range (0, 1000, step=1, splits=Some(5))
】--> new【
'Project [id#68L AS user_id#70L, (id#68L % 10) AS region_id#71, cast(id#68L as string) AS user_name#72]
+- Range (0, 1000, step=1, splits=Some(5))
】


Analyzer(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$1@3180131e) org.apache.spark.sql.catalyst.analysis.TypeCoercionBase$CombinedTypeCoercionRule --> old【
'Project [id#68L AS user_id#70L, (id#68L % 10) AS region_id#71, cast(id#68L as string) AS user_name#72]
+- Range (0, 1000, step=1, splits=Some(5))
】--> new【
Project [id#68L AS user_id#70L, (id#68L % cast(10 as bigint)) AS region_id#71L, cast(id#68L as string) AS user_name#72]
+- Range (0, 1000, step=1, splits=Some(5))
】


Analyzer(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$1@3180131e) org.apache.spark.sql.catalyst.analysis.ResolveTimeZone --> old【
Project [id#68L AS user_id#70L, (id#68L % cast(10 as bigint)) AS region_id#71L, cast(id#68L as string) AS user_name#72]
+- Range (0, 1000, step=1, splits=Some(5))
】--> new【
Project [id#68L AS user_id#70L, (id#68L % cast(10 as bigint)) AS region_id#71L, cast(id#68L as string) AS user_name#72]
+- Range (0, 1000, step=1, splits=Some(5))
】


RuleExecutor.execute[Analyzer(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$1@3180131e)] final result ==> old【
'Project ['id AS user_id#70, ('id % 10) AS region_id#71, cast('id as string) AS user_name#72]
+- Range (0, 1000, step=1, splits=Some(5))
】==> new【
Project [id#68L AS user_id#70L, (id#68L % cast(10 as bigint)) AS region_id#71L, cast(id#68L as string) AS user_name#72]
+- Range (0, 1000, step=1, splits=Some(5))
】


analyzed ====>
Project [id#68L AS user_id#70L, (id#68L % cast(10 as bigint)) AS region_id#71L, cast(id#68L as string) AS user_name#72]
+- Range (0, 1000, step=1, splits=Some(5))

21:32:52.426 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.execution.SparkSqlParser - Parsing command: user
org.apache.spark.sql.catalyst.parser.SqlBaseParser$SingleTableIdentifierContext =>  user <EOF>
org.apache.spark.sql.catalyst.parser.SqlBaseParser$TableIdentifierContext =>  user
analyzed ====>
CreateViewCommand `user`, false, true, LocalTempView, true
   +- Project [id#68L AS user_id#70L, (id#68L % cast(10 as bigint)) AS region_id#71L, cast(id#68L as string) AS user_name#72]
      +- Range (0, 1000, step=1, splits=Some(5))

analyzed ====>
CreateViewCommand `user`, false, true, LocalTempView, true
   +- Project [id#68L AS user_id#70L, (id#68L % cast(10 as bigint)) AS region_id#71L, cast(id#68L as string) AS user_name#72]
      +- Range (0, 1000, step=1, splits=Some(5))

optimizedPlan ====>
CreateViewCommand `user`, false, true, LocalTempView, true
   +- Project [id#68L AS user_id#70L, (id#68L % cast(10 as bigint)) AS region_id#71L, cast(id#68L as string) AS user_name#72]
      +- Range (0, 1000, step=1, splits=Some(5))

SparkPlanner(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2@321bf4b1) - org.apache.spark.sql.execution.SparkStrategies$SpecialLimits$ -->【
ReturnAnswer
+- CreateViewCommand `user`, false, true, LocalTempView, true
      +- Project [id#68L AS user_id#70L, (id#68L % cast(10 as bigint)) AS region_id#71L, cast(id#68L as string) AS user_name#72]
         +- Range (0, 1000, step=1, splits=Some(5))
】 --> 【
PlanLater CreateViewCommand `user`, false, true, LocalTempView, true
】

SparkPlanner(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2@321bf4b1) - org.apache.spark.sql.execution.SparkStrategies$BasicOperators$ -->【
CreateViewCommand `user`, false, true, LocalTempView, true
   +- Project [id#68L AS user_id#70L, (id#68L % cast(10 as bigint)) AS region_id#71L, cast(id#68L as string) AS user_name#72]
      +- Range (0, 1000, step=1, splits=Some(5))
】 --> 【
Execute CreateViewCommand
   +- CreateViewCommand `user`, false, true, LocalTempView, true
         +- Project [id#68L AS user_id#70L, (id#68L % cast(10 as bigint)) AS region_id#71L, cast(id#68L as string) AS user_name#72]
            +- Range (0, 1000, step=1, splits=Some(5))
】

sparkPlan ====>
Execute CreateViewCommand
   +- CreateViewCommand `user`, false, true, LocalTempView, true
         +- Project [id#68L AS user_id#70L, (id#68L % cast(10 as bigint)) AS region_id#71L, cast(id#68L as string) AS user_name#72]
            +- Range (0, 1000, step=1, splits=Some(5))

org.apache.spark.sql.execution.QueryExecution$ prepareForExecution final result ==>【
Execute CreateViewCommand
   +- CreateViewCommand `user`, false, true, LocalTempView, true
         +- Project [id#68L AS user_id#70L, (id#68L % cast(10 as bigint)) AS region_id#71L, cast(id#68L as string) AS user_name#72]
            +- Range (0, 1000, step=1, splits=Some(5))
】 ==> 【
Execute CreateViewCommand
   +- CreateViewCommand `user`, false, true, LocalTempView, true
         +- Project [id#68L AS user_id#70L, (id#68L % cast(10 as bigint)) AS region_id#71L, cast(id#68L as string) AS user_name#72]
            +- Range (0, 1000, step=1, splits=Some(5))
】

executedPlan ====>
Execute CreateViewCommand
   +- CreateViewCommand `user`, false, true, LocalTempView, true
         +- Project [id#68L AS user_id#70L, (id#68L % cast(10 as bigint)) AS region_id#71L, cast(id#68L as string) AS user_name#72]
            +- Range (0, 1000, step=1, splits=Some(5))

analyzed ====>
Range (0, 10, step=1, splits=Some(1))

21:32:52.440 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.execution.SparkSqlParser - Parsing command: id AS region_id
org.apache.spark.sql.catalyst.parser.SqlBaseParser$SingleExpressionContext =>  idASregion_id <EOF>
org.apache.spark.sql.catalyst.parser.SqlBaseParser$SingleExpressionContext =>  idASregion_id <EOF>
org.apache.spark.sql.catalyst.parser.SqlBaseParser$NamedExpressionContext =>  id AS region_id
org.apache.spark.sql.catalyst.parser.SqlBaseParser$PredicatedContext =>  id
org.apache.spark.sql.catalyst.parser.SqlBaseParser$ColumnReferenceContext =>  id
21:32:52.440 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.execution.SparkSqlParser - Parsing command: CAST(id AS STRING) AS region_name
org.apache.spark.sql.catalyst.parser.SqlBaseParser$SingleExpressionContext =>  CAST(idASSTRING)ASregion_name <EOF>
org.apache.spark.sql.catalyst.parser.SqlBaseParser$SingleExpressionContext =>  CAST(idASSTRING)ASregion_name <EOF>
org.apache.spark.sql.catalyst.parser.SqlBaseParser$NamedExpressionContext =>  CAST(idASSTRING) AS region_name
org.apache.spark.sql.catalyst.parser.SqlBaseParser$PredicatedContext =>  CAST(idASSTRING)
org.apache.spark.sql.catalyst.parser.SqlBaseParser$CastContext =>  CAST ( id AS STRING )
org.apache.spark.sql.catalyst.parser.SqlBaseParser$PrimitiveDataTypeContext =>  STRING
org.apache.spark.sql.catalyst.parser.SqlBaseParser$PredicatedContext =>  id
org.apache.spark.sql.catalyst.parser.SqlBaseParser$ColumnReferenceContext =>  id
Analyzer(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$1@3180131e) org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences --> old【
'Project ['id AS region_id#78, cast('id as string) AS region_name#79]
+- Range (0, 10, step=1, splits=Some(1))
】--> new【
Project [id#76L AS region_id#78L, cast(id#76L as string) AS region_name#79]
+- Range (0, 10, step=1, splits=Some(1))
】


Analyzer(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$1@3180131e) org.apache.spark.sql.catalyst.analysis.ResolveTimeZone --> old【
Project [id#76L AS region_id#78L, cast(id#76L as string) AS region_name#79]
+- Range (0, 10, step=1, splits=Some(1))
】--> new【
Project [id#76L AS region_id#78L, cast(id#76L as string) AS region_name#79]
+- Range (0, 10, step=1, splits=Some(1))
】


RuleExecutor.execute[Analyzer(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$1@3180131e)] final result ==> old【
'Project ['id AS region_id#78, cast('id as string) AS region_name#79]
+- Range (0, 10, step=1, splits=Some(1))
】==> new【
Project [id#76L AS region_id#78L, cast(id#76L as string) AS region_name#79]
+- Range (0, 10, step=1, splits=Some(1))
】


analyzed ====>
Project [id#76L AS region_id#78L, cast(id#76L as string) AS region_name#79]
+- Range (0, 10, step=1, splits=Some(1))

21:32:52.447 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.execution.SparkSqlParser - Parsing command: region
org.apache.spark.sql.catalyst.parser.SqlBaseParser$SingleTableIdentifierContext =>  region <EOF>
org.apache.spark.sql.catalyst.parser.SqlBaseParser$TableIdentifierContext =>  region
analyzed ====>
CreateViewCommand `region`, false, true, LocalTempView, true
   +- Project [id#76L AS region_id#78L, cast(id#76L as string) AS region_name#79]
      +- Range (0, 10, step=1, splits=Some(1))

analyzed ====>
CreateViewCommand `region`, false, true, LocalTempView, true
   +- Project [id#76L AS region_id#78L, cast(id#76L as string) AS region_name#79]
      +- Range (0, 10, step=1, splits=Some(1))

optimizedPlan ====>
CreateViewCommand `region`, false, true, LocalTempView, true
   +- Project [id#76L AS region_id#78L, cast(id#76L as string) AS region_name#79]
      +- Range (0, 10, step=1, splits=Some(1))

SparkPlanner(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2@321bf4b1) - org.apache.spark.sql.execution.SparkStrategies$SpecialLimits$ -->【
ReturnAnswer
+- CreateViewCommand `region`, false, true, LocalTempView, true
      +- Project [id#76L AS region_id#78L, cast(id#76L as string) AS region_name#79]
         +- Range (0, 10, step=1, splits=Some(1))
】 --> 【
PlanLater CreateViewCommand `region`, false, true, LocalTempView, true
】

SparkPlanner(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2@321bf4b1) - org.apache.spark.sql.execution.SparkStrategies$BasicOperators$ -->【
CreateViewCommand `region`, false, true, LocalTempView, true
   +- Project [id#76L AS region_id#78L, cast(id#76L as string) AS region_name#79]
      +- Range (0, 10, step=1, splits=Some(1))
】 --> 【
Execute CreateViewCommand
   +- CreateViewCommand `region`, false, true, LocalTempView, true
         +- Project [id#76L AS region_id#78L, cast(id#76L as string) AS region_name#79]
            +- Range (0, 10, step=1, splits=Some(1))
】

sparkPlan ====>
Execute CreateViewCommand
   +- CreateViewCommand `region`, false, true, LocalTempView, true
         +- Project [id#76L AS region_id#78L, cast(id#76L as string) AS region_name#79]
            +- Range (0, 10, step=1, splits=Some(1))

org.apache.spark.sql.execution.QueryExecution$ prepareForExecution final result ==>【
Execute CreateViewCommand
   +- CreateViewCommand `region`, false, true, LocalTempView, true
         +- Project [id#76L AS region_id#78L, cast(id#76L as string) AS region_name#79]
            +- Range (0, 10, step=1, splits=Some(1))
】 ==> 【
Execute CreateViewCommand
   +- CreateViewCommand `region`, false, true, LocalTempView, true
         +- Project [id#76L AS region_id#78L, cast(id#76L as string) AS region_name#79]
            +- Range (0, 10, step=1, splits=Some(1))
】

executedPlan ====>
Execute CreateViewCommand
   +- CreateViewCommand `region`, false, true, LocalTempView, true
         +- Project [id#76L AS region_id#78L, cast(id#76L as string) AS region_name#79]
            +- Range (0, 10, step=1, splits=Some(1))

21:32:52.456 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.execution.SparkSqlParser - Parsing command:
SELECT o.order_id, o.user_id, o.region_id, order_num, user_name, region_name
FROM order o
JOIN user u ON o.user_id = u.user_id and o.region_id = u.region_id
JOIN region r ON o.region_id = r.region_id and u.region_id = r.region_id

org.apache.spark.sql.catalyst.parser.SqlBaseParser$SingleStatementContext =>  SELECTo.order_id,o.user_id,o.region_id,order_num,user_name,region_nameFROMorderoJOINuseruONo.user_id=u.user_idando.region_id=u.region_idJOINregionrONo.region_id=r.region_idandu.region_id=r.region_id <EOF>
org.apache.spark.sql.catalyst.parser.SqlBaseParser$SingleStatementContext =>  SELECTo.order_id,o.user_id,o.region_id,order_num,user_name,region_nameFROMorderoJOINuseruONo.user_id=u.user_idando.region_id=u.region_idJOINregionrONo.region_id=r.region_idandu.region_id=r.region_id <EOF>
org.apache.spark.sql.catalyst.parser.SqlBaseParser$QueryContext =>  SELECTo.order_id,o.user_id,o.region_id,order_num,user_name,region_nameFROMorderoJOINuseruONo.user_id=u.user_idando.region_id=u.region_idJOINregionrONo.region_id=r.region_idandu.region_id=r.region_id
org.apache.spark.sql.catalyst.parser.SqlBaseParser$RegularQuerySpecificationContext =>  SELECTo.order_id,o.user_id,o.region_id,order_num,user_name,region_name FROMorderoJOINuseruONo.user_id=u.user_idando.region_id=u.region_idJOINregionrONo.region_id=r.region_idandu.region_id=r.region_id
org.apache.spark.sql.catalyst.parser.SqlBaseParser$FromClauseContext =>  FROM orderoJOINuseruONo.user_id=u.user_idando.region_id=u.region_idJOINregionrONo.region_id=r.region_idandu.region_id=r.region_id
org.apache.spark.sql.catalyst.parser.SqlBaseParser$TableNameContext =>  order o
org.apache.spark.sql.catalyst.parser.SqlBaseParser$MultipartIdentifierContext =>  order
org.apache.spark.sql.catalyst.parser.SqlBaseParser$JoinRelationContext =>   JOIN useru ONo.user_id=u.user_idando.region_id=u.region_id
org.apache.spark.sql.catalyst.parser.SqlBaseParser$LogicalBinaryContext =>  o.user_id=u.user_id and o.region_id=u.region_id
org.apache.spark.sql.catalyst.parser.SqlBaseParser$PredicatedContext =>  o.user_id=u.user_id
org.apache.spark.sql.catalyst.parser.SqlBaseParser$ComparisonContext =>  o.user_id = u.user_id
org.apache.spark.sql.catalyst.parser.SqlBaseParser$DereferenceContext =>  o . user_id
org.apache.spark.sql.catalyst.parser.SqlBaseParser$ColumnReferenceContext =>  o
org.apache.spark.sql.catalyst.parser.SqlBaseParser$DereferenceContext =>  u . user_id
org.apache.spark.sql.catalyst.parser.SqlBaseParser$ColumnReferenceContext =>  u
org.apache.spark.sql.catalyst.parser.SqlBaseParser$PredicatedContext =>  o.region_id=u.region_id
org.apache.spark.sql.catalyst.parser.SqlBaseParser$ComparisonContext =>  o.region_id = u.region_id
org.apache.spark.sql.catalyst.parser.SqlBaseParser$DereferenceContext =>  o . region_id
org.apache.spark.sql.catalyst.parser.SqlBaseParser$ColumnReferenceContext =>  o
org.apache.spark.sql.catalyst.parser.SqlBaseParser$DereferenceContext =>  u . region_id
org.apache.spark.sql.catalyst.parser.SqlBaseParser$ColumnReferenceContext =>  u
org.apache.spark.sql.catalyst.parser.SqlBaseParser$TableNameContext =>  user u
org.apache.spark.sql.catalyst.parser.SqlBaseParser$MultipartIdentifierContext =>  user
org.apache.spark.sql.catalyst.parser.SqlBaseParser$JoinRelationContext =>   JOIN regionr ONo.region_id=r.region_idandu.region_id=r.region_id
org.apache.spark.sql.catalyst.parser.SqlBaseParser$LogicalBinaryContext =>  o.region_id=r.region_id and u.region_id=r.region_id
org.apache.spark.sql.catalyst.parser.SqlBaseParser$PredicatedContext =>  o.region_id=r.region_id
org.apache.spark.sql.catalyst.parser.SqlBaseParser$ComparisonContext =>  o.region_id = r.region_id
org.apache.spark.sql.catalyst.parser.SqlBaseParser$DereferenceContext =>  o . region_id
org.apache.spark.sql.catalyst.parser.SqlBaseParser$ColumnReferenceContext =>  o
org.apache.spark.sql.catalyst.parser.SqlBaseParser$DereferenceContext =>  r . region_id
org.apache.spark.sql.catalyst.parser.SqlBaseParser$ColumnReferenceContext =>  r
org.apache.spark.sql.catalyst.parser.SqlBaseParser$PredicatedContext =>  u.region_id=r.region_id
org.apache.spark.sql.catalyst.parser.SqlBaseParser$ComparisonContext =>  u.region_id = r.region_id
org.apache.spark.sql.catalyst.parser.SqlBaseParser$DereferenceContext =>  u . region_id
org.apache.spark.sql.catalyst.parser.SqlBaseParser$ColumnReferenceContext =>  u
org.apache.spark.sql.catalyst.parser.SqlBaseParser$DereferenceContext =>  r . region_id
org.apache.spark.sql.catalyst.parser.SqlBaseParser$ColumnReferenceContext =>  r
org.apache.spark.sql.catalyst.parser.SqlBaseParser$TableNameContext =>  region r
org.apache.spark.sql.catalyst.parser.SqlBaseParser$MultipartIdentifierContext =>  region
org.apache.spark.sql.catalyst.parser.SqlBaseParser$RegularQuerySpecificationContext =>  SELECTo.order_id,o.user_id,o.region_id,order_num,user_name,region_name FROMorderoJOINuseruONo.user_id=u.user_idando.region_id=u.region_idJOINregionrONo.region_id=r.region_idandu.region_id=r.region_id
org.apache.spark.sql.catalyst.parser.SqlBaseParser$NamedExpressionContext =>  o.order_id
org.apache.spark.sql.catalyst.parser.SqlBaseParser$PredicatedContext =>  o.order_id
org.apache.spark.sql.catalyst.parser.SqlBaseParser$DereferenceContext =>  o . order_id
org.apache.spark.sql.catalyst.parser.SqlBaseParser$ColumnReferenceContext =>  o
org.apache.spark.sql.catalyst.parser.SqlBaseParser$NamedExpressionContext =>  o.user_id
org.apache.spark.sql.catalyst.parser.SqlBaseParser$PredicatedContext =>  o.user_id
org.apache.spark.sql.catalyst.parser.SqlBaseParser$DereferenceContext =>  o . user_id
org.apache.spark.sql.catalyst.parser.SqlBaseParser$ColumnReferenceContext =>  o
org.apache.spark.sql.catalyst.parser.SqlBaseParser$NamedExpressionContext =>  o.region_id
org.apache.spark.sql.catalyst.parser.SqlBaseParser$PredicatedContext =>  o.region_id
org.apache.spark.sql.catalyst.parser.SqlBaseParser$DereferenceContext =>  o . region_id
org.apache.spark.sql.catalyst.parser.SqlBaseParser$ColumnReferenceContext =>  o
org.apache.spark.sql.catalyst.parser.SqlBaseParser$NamedExpressionContext =>  order_num
org.apache.spark.sql.catalyst.parser.SqlBaseParser$PredicatedContext =>  order_num
org.apache.spark.sql.catalyst.parser.SqlBaseParser$ColumnReferenceContext =>  order_num
org.apache.spark.sql.catalyst.parser.SqlBaseParser$NamedExpressionContext =>  user_name
org.apache.spark.sql.catalyst.parser.SqlBaseParser$PredicatedContext =>  user_name
org.apache.spark.sql.catalyst.parser.SqlBaseParser$ColumnReferenceContext =>  user_name
org.apache.spark.sql.catalyst.parser.SqlBaseParser$NamedExpressionContext =>  region_name
org.apache.spark.sql.catalyst.parser.SqlBaseParser$PredicatedContext =>  region_name
org.apache.spark.sql.catalyst.parser.SqlBaseParser$ColumnReferenceContext =>  region_name
org.apache.spark.sql.catalyst.parser.SqlBaseParser$QueryOrganizationContext =>
Unresolved Logical Plan ====>
'Project ['o.order_id, 'o.user_id, 'o.region_id, 'order_num, 'user_name, 'region_name]
+- 'Join Inner, (('o.region_id = 'r.region_id) AND ('u.region_id = 'r.region_id))
   :- 'Join Inner, (('o.user_id = 'u.user_id) AND ('o.region_id = 'u.region_id))
   :  :- 'SubqueryAlias o
   :  :  +- 'UnresolvedRelation [order], [], false
   :  +- 'SubqueryAlias u
   :     +- 'UnresolvedRelation [user], [], false
   +- 'SubqueryAlias r
      +- 'UnresolvedRelation [region], [], false

Analyzer(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$1@3180131e) org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations --> old【
'Project ['o.order_id, 'o.user_id, 'o.region_id, 'order_num, 'user_name, 'region_name]
+- 'Join Inner, (('o.region_id = 'r.region_id) AND ('u.region_id = 'r.region_id))
   :- 'Join Inner, (('o.user_id = 'u.user_id) AND ('o.region_id = 'u.region_id))
   :  :- 'SubqueryAlias o
   :  :  +- 'UnresolvedRelation [order], [], false
   :  +- 'SubqueryAlias u
   :     +- 'UnresolvedRelation [user], [], false
   +- 'SubqueryAlias r
      +- 'UnresolvedRelation [region], [], false
】--> new【
'Project ['o.order_id, 'o.user_id, 'o.region_id, 'order_num, 'user_name, 'region_name]
+- 'Join Inner, (('o.region_id = 'r.region_id) AND ('u.region_id = 'r.region_id))
   :- 'Join Inner, (('o.user_id = 'u.user_id) AND ('o.region_id = 'u.region_id))
   :  :- SubqueryAlias o
   :  :  +- SubqueryAlias order
   :  :     +- View (`order`, [order_id#60L,user_id#61L,region_id#62L,order_num#63])
   :  :        +- Project [id#58L AS order_id#60L, (id#58L % cast(1000 as bigint)) AS user_id#61L, (id#58L % cast(10 as bigint)) AS region_id#62L, cast(id#58L as string) AS order_num#63]
   :  :           +- Range (0, 10000, step=1, splits=Some(10))
   :  +- SubqueryAlias u
   :     +- SubqueryAlias user
   :        +- View (`user`, [user_id#70L,region_id#71L,user_name#72])
   :           +- Project [id#68L AS user_id#70L, (id#68L % cast(10 as bigint)) AS region_id#71L, cast(id#68L as string) AS user_name#72]
   :              +- Range (0, 1000, step=1, splits=Some(5))
   +- SubqueryAlias r
      +- SubqueryAlias region
         +- View (`region`, [region_id#78L,region_name#79])
            +- Project [id#76L AS region_id#78L, cast(id#76L as string) AS region_name#79]
               +- Range (0, 10, step=1, splits=Some(1))
】


Analyzer(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$1@3180131e) org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences --> old【
'Project ['o.order_id, 'o.user_id, 'o.region_id, 'order_num, 'user_name, 'region_name]
+- 'Join Inner, (('o.region_id = 'r.region_id) AND ('u.region_id = 'r.region_id))
   :- 'Join Inner, (('o.user_id = 'u.user_id) AND ('o.region_id = 'u.region_id))
   :  :- SubqueryAlias o
   :  :  +- SubqueryAlias order
   :  :     +- View (`order`, [order_id#60L,user_id#61L,region_id#62L,order_num#63])
   :  :        +- Project [id#58L AS order_id#60L, (id#58L % cast(1000 as bigint)) AS user_id#61L, (id#58L % cast(10 as bigint)) AS region_id#62L, cast(id#58L as string) AS order_num#63]
   :  :           +- Range (0, 10000, step=1, splits=Some(10))
   :  +- SubqueryAlias u
   :     +- SubqueryAlias user
   :        +- View (`user`, [user_id#70L,region_id#71L,user_name#72])
   :           +- Project [id#68L AS user_id#70L, (id#68L % cast(10 as bigint)) AS region_id#71L, cast(id#68L as string) AS user_name#72]
   :              +- Range (0, 1000, step=1, splits=Some(5))
   +- SubqueryAlias r
      +- SubqueryAlias region
         +- View (`region`, [region_id#78L,region_name#79])
            +- Project [id#76L AS region_id#78L, cast(id#76L as string) AS region_name#79]
               +- Range (0, 10, step=1, splits=Some(1))
】--> new【
Project [order_id#60L, user_id#61L, region_id#62L, order_num#63, user_name#72, region_name#79]
+- Join Inner, ((region_id#62L = region_id#78L) AND (region_id#71L = region_id#78L))
   :- Join Inner, ((user_id#61L = user_id#70L) AND (region_id#62L = region_id#71L))
   :  :- SubqueryAlias o
   :  :  +- SubqueryAlias order
   :  :     +- View (`order`, [order_id#60L,user_id#61L,region_id#62L,order_num#63])
   :  :        +- Project [id#58L AS order_id#60L, (id#58L % cast(1000 as bigint)) AS user_id#61L, (id#58L % cast(10 as bigint)) AS region_id#62L, cast(id#58L as string) AS order_num#63]
   :  :           +- Range (0, 10000, step=1, splits=Some(10))
   :  +- SubqueryAlias u
   :     +- SubqueryAlias user
   :        +- View (`user`, [user_id#70L,region_id#71L,user_name#72])
   :           +- Project [id#68L AS user_id#70L, (id#68L % cast(10 as bigint)) AS region_id#71L, cast(id#68L as string) AS user_name#72]
   :              +- Range (0, 1000, step=1, splits=Some(5))
   +- SubqueryAlias r
      +- SubqueryAlias region
         +- View (`region`, [region_id#78L,region_name#79])
            +- Project [id#76L AS region_id#78L, cast(id#76L as string) AS region_name#79]
               +- Range (0, 10, step=1, splits=Some(1))
】


RuleExecutor.execute[Analyzer(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$1@3180131e)] final result ==> old【
'Project ['o.order_id, 'o.user_id, 'o.region_id, 'order_num, 'user_name, 'region_name]
+- 'Join Inner, (('o.region_id = 'r.region_id) AND ('u.region_id = 'r.region_id))
   :- 'Join Inner, (('o.user_id = 'u.user_id) AND ('o.region_id = 'u.region_id))
   :  :- 'SubqueryAlias o
   :  :  +- 'UnresolvedRelation [order], [], false
   :  +- 'SubqueryAlias u
   :     +- 'UnresolvedRelation [user], [], false
   +- 'SubqueryAlias r
      +- 'UnresolvedRelation [region], [], false
】==> new【
Project [order_id#60L, user_id#61L, region_id#62L, order_num#63, user_name#72, region_name#79]
+- Join Inner, ((region_id#62L = region_id#78L) AND (region_id#71L = region_id#78L))
   :- Join Inner, ((user_id#61L = user_id#70L) AND (region_id#62L = region_id#71L))
   :  :- SubqueryAlias o
   :  :  +- SubqueryAlias order
   :  :     +- View (`order`, [order_id#60L,user_id#61L,region_id#62L,order_num#63])
   :  :        +- Project [id#58L AS order_id#60L, (id#58L % cast(1000 as bigint)) AS user_id#61L, (id#58L % cast(10 as bigint)) AS region_id#62L, cast(id#58L as string) AS order_num#63]
   :  :           +- Range (0, 10000, step=1, splits=Some(10))
   :  +- SubqueryAlias u
   :     +- SubqueryAlias user
   :        +- View (`user`, [user_id#70L,region_id#71L,user_name#72])
   :           +- Project [id#68L AS user_id#70L, (id#68L % cast(10 as bigint)) AS region_id#71L, cast(id#68L as string) AS user_name#72]
   :              +- Range (0, 1000, step=1, splits=Some(5))
   +- SubqueryAlias r
      +- SubqueryAlias region
         +- View (`region`, [region_id#78L,region_name#79])
            +- Project [id#76L AS region_id#78L, cast(id#76L as string) AS region_name#79]
               +- Range (0, 10, step=1, splits=Some(1))
】


analyzed ====>
Project [order_id#60L, user_id#61L, region_id#62L, order_num#63, user_name#72, region_name#79]
+- Join Inner, ((region_id#62L = region_id#78L) AND (region_id#71L = region_id#78L))
   :- Join Inner, ((user_id#61L = user_id#70L) AND (region_id#62L = region_id#71L))
   :  :- SubqueryAlias o
   :  :  +- SubqueryAlias order
   :  :     +- View (`order`, [order_id#60L,user_id#61L,region_id#62L,order_num#63])
   :  :        +- Project [id#58L AS order_id#60L, (id#58L % cast(1000 as bigint)) AS user_id#61L, (id#58L % cast(10 as bigint)) AS region_id#62L, cast(id#58L as string) AS order_num#63]
   :  :           +- Range (0, 10000, step=1, splits=Some(10))
   :  +- SubqueryAlias u
   :     +- SubqueryAlias user
   :        +- View (`user`, [user_id#70L,region_id#71L,user_name#72])
   :           +- Project [id#68L AS user_id#70L, (id#68L % cast(10 as bigint)) AS region_id#71L, cast(id#68L as string) AS user_name#72]
   :              +- Range (0, 1000, step=1, splits=Some(5))
   +- SubqueryAlias r
      +- SubqueryAlias region
         +- View (`region`, [region_id#78L,region_name#79])
            +- Project [id#76L AS region_id#78L, cast(id#76L as string) AS region_name#79]
               +- Range (0, 10, step=1, splits=Some(1))

21:32:52.590 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.execution.SparkSqlParser - Parsing command: order_all
org.apache.spark.sql.catalyst.parser.SqlBaseParser$SingleMultipartIdentifierContext =>  order_all <EOF>
org.apache.spark.sql.catalyst.parser.SqlBaseParser$MultipartIdentifierContext =>  order_all
Analyzer(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$1@3180131e) org.apache.spark.sql.execution.datasources.DataSourceAnalysis --> old【
'CreateTable `order_all`, Overwrite
+- Project [order_id#60L, user_id#61L, region_id#62L, order_num#63, user_name#72, region_name#79]
   +- Join Inner, ((region_id#62L = region_id#78L) AND (region_id#71L = region_id#78L))
      :- Join Inner, ((user_id#61L = user_id#70L) AND (region_id#62L = region_id#71L))
      :  :- SubqueryAlias o
      :  :  +- SubqueryAlias order
      :  :     +- View (`order`, [order_id#60L,user_id#61L,region_id#62L,order_num#63])
      :  :        +- Project [id#58L AS order_id#60L, (id#58L % cast(1000 as bigint)) AS user_id#61L, (id#58L % cast(10 as bigint)) AS region_id#62L, cast(id#58L as string) AS order_num#63]
      :  :           +- Range (0, 10000, step=1, splits=Some(10))
      :  +- SubqueryAlias u
      :     +- SubqueryAlias user
      :        +- View (`user`, [user_id#70L,region_id#71L,user_name#72])
      :           +- Project [id#68L AS user_id#70L, (id#68L % cast(10 as bigint)) AS region_id#71L, cast(id#68L as string) AS user_name#72]
      :              +- Range (0, 1000, step=1, splits=Some(5))
      +- SubqueryAlias r
         +- SubqueryAlias region
            +- View (`region`, [region_id#78L,region_name#79])
               +- Project [id#76L AS region_id#78L, cast(id#76L as string) AS region_name#79]
                  +- Range (0, 10, step=1, splits=Some(1))
】--> new【
CreateDataSourceTableAsSelectCommand `order_all`, Overwrite, [order_id, user_id, region_id, order_num, user_name, region_name]
+- Project [order_id#60L, user_id#61L, region_id#62L, order_num#63, user_name#72, region_name#79]
   +- Join Inner, ((region_id#62L = region_id#78L) AND (region_id#71L = region_id#78L))
      :- Join Inner, ((user_id#61L = user_id#70L) AND (region_id#62L = region_id#71L))
      :  :- SubqueryAlias o
      :  :  +- SubqueryAlias order
      :  :     +- View (`order`, [order_id#60L,user_id#61L,region_id#62L,order_num#63])
      :  :        +- Project [id#58L AS order_id#60L, (id#58L % cast(1000 as bigint)) AS user_id#61L, (id#58L % cast(10 as bigint)) AS region_id#62L, cast(id#58L as string) AS order_num#63]
      :  :           +- Range (0, 10000, step=1, splits=Some(10))
      :  +- SubqueryAlias u
      :     +- SubqueryAlias user
      :        +- View (`user`, [user_id#70L,region_id#71L,user_name#72])
      :           +- Project [id#68L AS user_id#70L, (id#68L % cast(10 as bigint)) AS region_id#71L, cast(id#68L as string) AS user_name#72]
      :              +- Range (0, 1000, step=1, splits=Some(5))
      +- SubqueryAlias r
         +- SubqueryAlias region
            +- View (`region`, [region_id#78L,region_name#79])
               +- Project [id#76L AS region_id#78L, cast(id#76L as string) AS region_name#79]
                  +- Range (0, 10, step=1, splits=Some(1))
】


RuleExecutor.execute[Analyzer(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$1@3180131e)] final result ==> old【
'CreateTable `order_all`, Overwrite
+- Project [order_id#60L, user_id#61L, region_id#62L, order_num#63, user_name#72, region_name#79]
   +- Join Inner, ((region_id#62L = region_id#78L) AND (region_id#71L = region_id#78L))
      :- Join Inner, ((user_id#61L = user_id#70L) AND (region_id#62L = region_id#71L))
      :  :- SubqueryAlias o
      :  :  +- SubqueryAlias order
      :  :     +- View (`order`, [order_id#60L,user_id#61L,region_id#62L,order_num#63])
      :  :        +- Project [id#58L AS order_id#60L, (id#58L % cast(1000 as bigint)) AS user_id#61L, (id#58L % cast(10 as bigint)) AS region_id#62L, cast(id#58L as string) AS order_num#63]
      :  :           +- Range (0, 10000, step=1, splits=Some(10))
      :  +- SubqueryAlias u
      :     +- SubqueryAlias user
      :        +- View (`user`, [user_id#70L,region_id#71L,user_name#72])
      :           +- Project [id#68L AS user_id#70L, (id#68L % cast(10 as bigint)) AS region_id#71L, cast(id#68L as string) AS user_name#72]
      :              +- Range (0, 1000, step=1, splits=Some(5))
      +- SubqueryAlias r
         +- SubqueryAlias region
            +- View (`region`, [region_id#78L,region_name#79])
               +- Project [id#76L AS region_id#78L, cast(id#76L as string) AS region_name#79]
                  +- Range (0, 10, step=1, splits=Some(1))
】==> new【
CreateDataSourceTableAsSelectCommand `order_all`, Overwrite, [order_id, user_id, region_id, order_num, user_name, region_name]
+- Project [order_id#60L, user_id#61L, region_id#62L, order_num#63, user_name#72, region_name#79]
   +- Join Inner, ((region_id#62L = region_id#78L) AND (region_id#71L = region_id#78L))
      :- Join Inner, ((user_id#61L = user_id#70L) AND (region_id#62L = region_id#71L))
      :  :- SubqueryAlias o
      :  :  +- SubqueryAlias order
      :  :     +- View (`order`, [order_id#60L,user_id#61L,region_id#62L,order_num#63])
      :  :        +- Project [id#58L AS order_id#60L, (id#58L % cast(1000 as bigint)) AS user_id#61L, (id#58L % cast(10 as bigint)) AS region_id#62L, cast(id#58L as string) AS order_num#63]
      :  :           +- Range (0, 10000, step=1, splits=Some(10))
      :  +- SubqueryAlias u
      :     +- SubqueryAlias user
      :        +- View (`user`, [user_id#70L,region_id#71L,user_name#72])
      :           +- Project [id#68L AS user_id#70L, (id#68L % cast(10 as bigint)) AS region_id#71L, cast(id#68L as string) AS user_name#72]
      :              +- Range (0, 1000, step=1, splits=Some(5))
      +- SubqueryAlias r
         +- SubqueryAlias region
            +- View (`region`, [region_id#78L,region_name#79])
               +- Project [id#76L AS region_id#78L, cast(id#76L as string) AS region_name#79]
                  +- Range (0, 10, step=1, splits=Some(1))
】


analyzed ====>
CreateDataSourceTableAsSelectCommand `order_all`, Overwrite, [order_id, user_id, region_id, order_num, user_name, region_name]
+- Project [order_id#60L, user_id#61L, region_id#62L, order_num#63, user_name#72, region_name#79]
   +- Join Inner, ((region_id#62L = region_id#78L) AND (region_id#71L = region_id#78L))
      :- Join Inner, ((user_id#61L = user_id#70L) AND (region_id#62L = region_id#71L))
      :  :- SubqueryAlias o
      :  :  +- SubqueryAlias order
      :  :     +- View (`order`, [order_id#60L,user_id#61L,region_id#62L,order_num#63])
      :  :        +- Project [id#58L AS order_id#60L, (id#58L % cast(1000 as bigint)) AS user_id#61L, (id#58L % cast(10 as bigint)) AS region_id#62L, cast(id#58L as string) AS order_num#63]
      :  :           +- Range (0, 10000, step=1, splits=Some(10))
      :  +- SubqueryAlias u
      :     +- SubqueryAlias user
      :        +- View (`user`, [user_id#70L,region_id#71L,user_name#72])
      :           +- Project [id#68L AS user_id#70L, (id#68L % cast(10 as bigint)) AS region_id#71L, cast(id#68L as string) AS user_name#72]
      :              +- Range (0, 1000, step=1, splits=Some(5))
      +- SubqueryAlias r
         +- SubqueryAlias region
            +- View (`region`, [region_id#78L,region_name#79])
               +- Project [id#76L AS region_id#78L, cast(id#76L as string) AS region_name#79]
                  +- Range (0, 10, step=1, splits=Some(1))

analyzed ====>
CreateDataSourceTableAsSelectCommand `order_all`, Overwrite, [order_id, user_id, region_id, order_num, user_name, region_name]
+- Project [order_id#60L, user_id#61L, region_id#62L, order_num#63, user_name#72, region_name#79]
   +- Join Inner, ((region_id#62L = region_id#78L) AND (region_id#71L = region_id#78L))
      :- Join Inner, ((user_id#61L = user_id#70L) AND (region_id#62L = region_id#71L))
      :  :- SubqueryAlias o
      :  :  +- SubqueryAlias order
      :  :     +- View (`order`, [order_id#60L,user_id#61L,region_id#62L,order_num#63])
      :  :        +- Project [id#58L AS order_id#60L, (id#58L % cast(1000 as bigint)) AS user_id#61L, (id#58L % cast(10 as bigint)) AS region_id#62L, cast(id#58L as string) AS order_num#63]
      :  :           +- Range (0, 10000, step=1, splits=Some(10))
      :  +- SubqueryAlias u
      :     +- SubqueryAlias user
      :        +- View (`user`, [user_id#70L,region_id#71L,user_name#72])
      :           +- Project [id#68L AS user_id#70L, (id#68L % cast(10 as bigint)) AS region_id#71L, cast(id#68L as string) AS user_name#72]
      :              +- Range (0, 1000, step=1, splits=Some(5))
      +- SubqueryAlias r
         +- SubqueryAlias region
            +- View (`region`, [region_id#78L,region_name#79])
               +- Project [id#76L AS region_id#78L, cast(id#76L as string) AS region_name#79]
                  +- Range (0, 10, step=1, splits=Some(1))

SparkOptimizer(org.apache.spark.sql.internal.BaseSessionStateBuilder$$anon$2@445265b0) org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis --> old【
CreateDataSourceTableAsSelectCommand `order_all`, Overwrite, [order_id, user_id, region_id, order_num, user_name, region_name]
+- Project [order_id#60L, user_id#61L, region_id#62L, order_num#63, user_name#72, region_name#79]
   +- Join Inner, ((region_id#62L = region_id#78L) AND (region_id#71L = region_id#78L))
      :- Join Inner, ((user_id#61L = user_id#70L) AND (region_id#62L = region_id#71L))
      :  :- SubqueryAlias o
      :  :  +- SubqueryAlias order
      :  :     +- View (`order`, [order_id#60L,user_id#61L,region_id#62L,order_num#63])
      :  :        +- Project [id#58L AS order_id#60L, (id#58L % cast(1000 as bigint)) AS user_id#61L, (id#58L % cast(10 as bigint)) AS region_id#62L, cast(id#58L as string) AS order_num#63]
      :  :           +- Range (0, 10000, step=1, splits=Some(10))
      :  +- SubqueryAlias u
      :     +- SubqueryAlias user
      :        +- View (`user`, [user_id#70L,region_id#71L,user_name#72])
      :           +- Project [id#68L AS user_id#70L, (id#68L % cast(10 as bigint)) AS region_id#71L, cast(id#68L as string) AS user_name#72]
      :              +- Range (0, 1000, step=1, splits=Some(5))
      +- SubqueryAlias r
         +- SubqueryAlias region
            +- View (`region`, [region_id#78L,region_name#79])
               +- Project [id#76L AS region_id#78L, cast(id#76L as string) AS region_name#79]
                  +- Range (0, 10, step=1, splits=Some(1))
】--> new【
CreateDataSourceTableAsSelectCommand `order_all`, Overwrite, [order_id, user_id, region_id, order_num, user_name, region_name]
+- Project [order_id#60L, user_id#61L, region_id#62L, order_num#63, user_name#72, region_name#79]
   +- Join Inner, ((region_id#62L = region_id#78L) AND (region_id#71L = region_id#78L))
      :- Join Inner, ((user_id#61L = user_id#70L) AND (region_id#62L = region_id#71L))
      :  :- Project [id#58L AS order_id#60L, (id#58L % cast(1000 as bigint)) AS user_id#61L, (id#58L % cast(10 as bigint)) AS region_id#62L, cast(id#58L as string) AS order_num#63]
      :  :  +- Range (0, 10000, step=1, splits=Some(10))
      :  +- Project [id#68L AS user_id#70L, (id#68L % cast(10 as bigint)) AS region_id#71L, cast(id#68L as string) AS user_name#72]
      :     +- Range (0, 1000, step=1, splits=Some(5))
      +- Project [id#76L AS region_id#78L, cast(id#76L as string) AS region_name#79]
         +- Range (0, 10, step=1, splits=Some(1))
】


SparkOptimizer(org.apache.spark.sql.internal.BaseSessionStateBuilder$$anon$2@445265b0) org.apache.spark.sql.catalyst.optimizer.ColumnPruning --> old【
CreateDataSourceTableAsSelectCommand `order_all`, Overwrite, [order_id, user_id, region_id, order_num, user_name, region_name]
+- Project [order_id#60L, user_id#61L, region_id#62L, order_num#63, user_name#72, region_name#79]
   +- Join Inner, ((region_id#62L = region_id#78L) AND (region_id#71L = region_id#78L))
      :- Join Inner, ((user_id#61L = user_id#70L) AND (region_id#62L = region_id#71L))
      :  :- Project [id#58L AS order_id#60L, (id#58L % cast(1000 as bigint)) AS user_id#61L, (id#58L % cast(10 as bigint)) AS region_id#62L, cast(id#58L as string) AS order_num#63]
      :  :  +- Range (0, 10000, step=1, splits=Some(10))
      :  +- Project [id#68L AS user_id#70L, (id#68L % cast(10 as bigint)) AS region_id#71L, cast(id#68L as string) AS user_name#72]
      :     +- Range (0, 1000, step=1, splits=Some(5))
      +- Project [id#76L AS region_id#78L, cast(id#76L as string) AS region_name#79]
         +- Range (0, 10, step=1, splits=Some(1))
】--> new【
CreateDataSourceTableAsSelectCommand `order_all`, Overwrite, [order_id, user_id, region_id, order_num, user_name, region_name]
+- Project [order_id#60L, user_id#61L, region_id#62L, order_num#63, user_name#72, region_name#79]
   +- Join Inner, ((region_id#62L = region_id#78L) AND (region_id#71L = region_id#78L))
      :- Project [order_id#60L, user_id#61L, region_id#62L, order_num#63, region_id#71L, user_name#72]
      :  +- Join Inner, ((user_id#61L = user_id#70L) AND (region_id#62L = region_id#71L))
      :     :- Project [id#58L AS order_id#60L, (id#58L % cast(1000 as bigint)) AS user_id#61L, (id#58L % cast(10 as bigint)) AS region_id#62L, cast(id#58L as string) AS order_num#63]
      :     :  +- Range (0, 10000, step=1, splits=Some(10))
      :     +- Project [id#68L AS user_id#70L, (id#68L % cast(10 as bigint)) AS region_id#71L, cast(id#68L as string) AS user_name#72]
      :        +- Range (0, 1000, step=1, splits=Some(5))
      +- Project [id#76L AS region_id#78L, cast(id#76L as string) AS region_name#79]
         +- Range (0, 10, step=1, splits=Some(1))
】


SparkOptimizer(org.apache.spark.sql.internal.BaseSessionStateBuilder$$anon$2@445265b0) org.apache.spark.sql.catalyst.optimizer.ConstantFolding --> old【
CreateDataSourceTableAsSelectCommand `order_all`, Overwrite, [order_id, user_id, region_id, order_num, user_name, region_name]
+- Project [order_id#60L, user_id#61L, region_id#62L, order_num#63, user_name#72, region_name#79]
   +- Join Inner, ((region_id#62L = region_id#78L) AND (region_id#71L = region_id#78L))
      :- Project [order_id#60L, user_id#61L, region_id#62L, order_num#63, region_id#71L, user_name#72]
      :  +- Join Inner, ((user_id#61L = user_id#70L) AND (region_id#62L = region_id#71L))
      :     :- Project [id#58L AS order_id#60L, (id#58L % cast(1000 as bigint)) AS user_id#61L, (id#58L % cast(10 as bigint)) AS region_id#62L, cast(id#58L as string) AS order_num#63]
      :     :  +- Range (0, 10000, step=1, splits=Some(10))
      :     +- Project [id#68L AS user_id#70L, (id#68L % cast(10 as bigint)) AS region_id#71L, cast(id#68L as string) AS user_name#72]
      :        +- Range (0, 1000, step=1, splits=Some(5))
      +- Project [id#76L AS region_id#78L, cast(id#76L as string) AS region_name#79]
         +- Range (0, 10, step=1, splits=Some(1))
】--> new【
CreateDataSourceTableAsSelectCommand `order_all`, Overwrite, [order_id, user_id, region_id, order_num, user_name, region_name]
+- Project [order_id#60L, user_id#61L, region_id#62L, order_num#63, user_name#72, region_name#79]
   +- Join Inner, ((region_id#62L = region_id#78L) AND (region_id#71L = region_id#78L))
      :- Project [order_id#60L, user_id#61L, region_id#62L, order_num#63, region_id#71L, user_name#72]
      :  +- Join Inner, ((user_id#61L = user_id#70L) AND (region_id#62L = region_id#71L))
      :     :- Project [id#58L AS order_id#60L, (id#58L % 1000) AS user_id#61L, (id#58L % 10) AS region_id#62L, cast(id#58L as string) AS order_num#63]
      :     :  +- Range (0, 10000, step=1, splits=Some(10))
      :     +- Project [id#68L AS user_id#70L, (id#68L % 10) AS region_id#71L, cast(id#68L as string) AS user_name#72]
      :        +- Range (0, 1000, step=1, splits=Some(5))
      +- Project [id#76L AS region_id#78L, cast(id#76L as string) AS region_name#79]
         +- Range (0, 10, step=1, splits=Some(1))
】


SparkOptimizer(org.apache.spark.sql.internal.BaseSessionStateBuilder$$anon$2@445265b0) org.apache.spark.sql.catalyst.optimizer.InferFiltersFromConstraints --> old【
CreateDataSourceTableAsSelectCommand `order_all`, Overwrite, [order_id, user_id, region_id, order_num, user_name, region_name]
+- Project [order_id#60L, user_id#61L, region_id#62L, order_num#63, user_name#72, region_name#79]
   +- Join Inner, ((region_id#62L = region_id#78L) AND (region_id#71L = region_id#78L))
      :- Project [order_id#60L, user_id#61L, region_id#62L, order_num#63, region_id#71L, user_name#72]
      :  +- Join Inner, ((user_id#61L = user_id#70L) AND (region_id#62L = region_id#71L))
      :     :- Project [id#58L AS order_id#60L, (id#58L % 1000) AS user_id#61L, (id#58L % 10) AS region_id#62L, cast(id#58L as string) AS order_num#63]
      :     :  +- Range (0, 10000, step=1, splits=Some(10))
      :     +- Project [id#68L AS user_id#70L, (id#68L % 10) AS region_id#71L, cast(id#68L as string) AS user_name#72]
      :        +- Range (0, 1000, step=1, splits=Some(5))
      +- Project [id#76L AS region_id#78L, cast(id#76L as string) AS region_name#79]
         +- Range (0, 10, step=1, splits=Some(1))
】--> new【
CreateDataSourceTableAsSelectCommand `order_all`, Overwrite, [order_id, user_id, region_id, order_num, user_name, region_name]
+- Project [order_id#60L, user_id#61L, region_id#62L, order_num#63, user_name#72, region_name#79]
   +- Join Inner, ((region_id#62L = region_id#78L) AND (region_id#71L = region_id#78L))
      :- Project [order_id#60L, user_id#61L, region_id#62L, order_num#63, region_id#71L, user_name#72]
      :  +- Join Inner, ((user_id#61L = user_id#70L) AND (region_id#62L = region_id#71L))
      :     :- Filter (isnotnull(user_id#61L) AND isnotnull(region_id#62L))
      :     :  +- Project [id#58L AS order_id#60L, (id#58L % 1000) AS user_id#61L, (id#58L % 10) AS region_id#62L, cast(id#58L as string) AS order_num#63]
      :     :     +- Range (0, 10000, step=1, splits=Some(10))
      :     +- Filter isnotnull(region_id#71L)
      :        +- Project [id#68L AS user_id#70L, (id#68L % 10) AS region_id#71L, cast(id#68L as string) AS user_name#72]
      :           +- Range (0, 1000, step=1, splits=Some(5))
      +- Project [id#76L AS region_id#78L, cast(id#76L as string) AS region_name#79]
         +- Range (0, 10, step=1, splits=Some(1))
】


SparkOptimizer(org.apache.spark.sql.internal.BaseSessionStateBuilder$$anon$2@445265b0) org.apache.spark.sql.catalyst.optimizer.PushDownPredicates --> old【
CreateDataSourceTableAsSelectCommand `order_all`, Overwrite, [order_id, user_id, region_id, order_num, user_name, region_name]
+- Project [order_id#60L, user_id#61L, region_id#62L, order_num#63, user_name#72, region_name#79]
   +- Join Inner, ((region_id#62L = region_id#78L) AND (region_id#71L = region_id#78L))
      :- Project [order_id#60L, user_id#61L, region_id#62L, order_num#63, region_id#71L, user_name#72]
      :  +- Join Inner, ((user_id#61L = user_id#70L) AND (region_id#62L = region_id#71L))
      :     :- Filter (isnotnull(user_id#61L) AND isnotnull(region_id#62L))
      :     :  +- Project [id#58L AS order_id#60L, (id#58L % 1000) AS user_id#61L, (id#58L % 10) AS region_id#62L, cast(id#58L as string) AS order_num#63]
      :     :     +- Range (0, 10000, step=1, splits=Some(10))
      :     +- Filter isnotnull(region_id#71L)
      :        +- Project [id#68L AS user_id#70L, (id#68L % 10) AS region_id#71L, cast(id#68L as string) AS user_name#72]
      :           +- Range (0, 1000, step=1, splits=Some(5))
      +- Project [id#76L AS region_id#78L, cast(id#76L as string) AS region_name#79]
         +- Range (0, 10, step=1, splits=Some(1))
】--> new【
CreateDataSourceTableAsSelectCommand `order_all`, Overwrite, [order_id, user_id, region_id, order_num, user_name, region_name]
+- Project [order_id#60L, user_id#61L, region_id#62L, order_num#63, user_name#72, region_name#79]
   +- Join Inner, ((region_id#62L = region_id#78L) AND (region_id#71L = region_id#78L))
      :- Project [order_id#60L, user_id#61L, region_id#62L, order_num#63, region_id#71L, user_name#72]
      :  +- Join Inner, ((user_id#61L = user_id#70L) AND (region_id#62L = region_id#71L))
      :     :- Project [id#58L AS order_id#60L, (id#58L % 1000) AS user_id#61L, (id#58L % 10) AS region_id#62L, cast(id#58L as string) AS order_num#63]
      :     :  +- Filter (isnotnull((id#58L % 1000)) AND isnotnull((id#58L % 10)))
      :     :     +- Range (0, 10000, step=1, splits=Some(10))
      :     +- Project [id#68L AS user_id#70L, (id#68L % 10) AS region_id#71L, cast(id#68L as string) AS user_name#72]
      :        +- Filter isnotnull((id#68L % 10))
      :           +- Range (0, 1000, step=1, splits=Some(5))
      +- Project [id#76L AS region_id#78L, cast(id#76L as string) AS region_name#79]
         +- Range (0, 10, step=1, splits=Some(1))
】


21:32:52.784 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.plans.logical.statsEstimation.FilterEstimation - [CBO] Unsupported filter condition: isnotnull((id#58L % 1000))
21:32:52.785 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.plans.logical.statsEstimation.FilterEstimation - [CBO] Unsupported filter condition: isnotnull((id#58L % 10))
21:32:52.791 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.plans.logical.statsEstimation.FilterEstimation - [CBO] Unsupported filter condition: isnotnull((id#68L % 10))
21:32:52.809 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - Considering join on: Some((region_id#71L = region_id#78L))
21:32:52.811 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - leftKeys:List(region_id#71L) | rightKeys:List(region_id#78L)
21:32:52.816 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - Considering join on: Some((region_id#62L = region_id#78L))
21:32:52.816 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - leftKeys:List(region_id#62L) | rightKeys:List(region_id#78L)
21:32:52.817 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - Considering join on: Some(((user_id#61L = user_id#70L) AND (region_id#62L = region_id#71L)))
21:32:52.817 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - leftKeys:List(user_id#61L, region_id#62L) | rightKeys:List(user_id#70L, region_id#71L)
21:32:52.819 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.optimizer.JoinReorderDP - Join reordering finished. Duration: 23 ms, number of items: 3, number of plans in memo: 7
SparkOptimizer(org.apache.spark.sql.internal.BaseSessionStateBuilder$$anon$2@445265b0) org.apache.spark.sql.catalyst.optimizer.CostBasedJoinReorder --> old【
CreateDataSourceTableAsSelectCommand `order_all`, Overwrite, [order_id, user_id, region_id, order_num, user_name, region_name]
+- Project [order_id#60L, user_id#61L, region_id#62L, order_num#63, user_name#72, region_name#79]
   +- Join Inner, ((region_id#62L = region_id#78L) AND (region_id#71L = region_id#78L))
      :- Project [order_id#60L, user_id#61L, region_id#62L, order_num#63, region_id#71L, user_name#72]
      :  +- Join Inner, ((user_id#61L = user_id#70L) AND (region_id#62L = region_id#71L))
      :     :- Project [id#58L AS order_id#60L, (id#58L % 1000) AS user_id#61L, (id#58L % 10) AS region_id#62L, cast(id#58L as string) AS order_num#63]
      :     :  +- Filter (isnotnull((id#58L % 1000)) AND isnotnull((id#58L % 10)))
      :     :     +- Range (0, 10000, step=1, splits=Some(10))
      :     +- Project [id#68L AS user_id#70L, (id#68L % 10) AS region_id#71L, cast(id#68L as string) AS user_name#72]
      :        +- Filter isnotnull((id#68L % 10))
      :           +- Range (0, 1000, step=1, splits=Some(5))
      +- Project [id#76L AS region_id#78L, cast(id#76L as string) AS region_name#79]
         +- Range (0, 10, step=1, splits=Some(1))
】--> new【
CreateDataSourceTableAsSelectCommand `order_all`, Overwrite, [order_id, user_id, region_id, order_num, user_name, region_name]
+- Project [order_id#60L, user_id#61L, region_id#62L, order_num#63, user_name#72, region_name#79]
   +- Join Inner, (((user_id#61L = user_id#70L) AND (region_id#62L = region_id#71L)) AND (region_id#62L = region_id#78L))
      :- Join Inner, (region_id#71L = region_id#78L)
      :  :- Project [id#68L AS user_id#70L, (id#68L % 10) AS region_id#71L, cast(id#68L as string) AS user_name#72]
      :  :  +- Filter isnotnull((id#68L % 10))
      :  :     +- Range (0, 1000, step=1, splits=Some(5))
      :  +- Project [id#76L AS region_id#78L, cast(id#76L as string) AS region_name#79]
      :     +- Range (0, 10, step=1, splits=Some(1))
      +- Project [id#58L AS order_id#60L, (id#58L % 1000) AS user_id#61L, (id#58L % 10) AS region_id#62L, cast(id#58L as string) AS order_num#63]
         +- Filter (isnotnull((id#58L % 1000)) AND isnotnull((id#58L % 10)))
            +- Range (0, 10000, step=1, splits=Some(10))
】


21:32:52.824 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - Considering join on: Some((((user_id#61L = user_id#70L) AND (region_id#62L = region_id#71L)) AND (region_id#62L = region_id#78L)))
21:32:52.824 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - leftKeys:List(user_id#70L, region_id#71L, region_id#78L) | rightKeys:List(user_id#61L, region_id#62L, region_id#62L)
21:32:52.824 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - Considering join on: Some((region_id#71L = region_id#78L))
21:32:52.824 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - leftKeys:List(region_id#71L) | rightKeys:List(region_id#78L)
21:32:52.825 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - Considering join on: Some((region_id#71L = region_id#78L))
21:32:52.825 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - leftKeys:List(region_id#71L) | rightKeys:List(region_id#78L)
21:32:52.826 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - Considering join on: Some((((user_id#61L = user_id#70L) AND (region_id#62L = region_id#71L)) AND (region_id#62L = region_id#78L)))
21:32:52.826 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - leftKeys:List(user_id#70L, region_id#71L, region_id#78L) | rightKeys:List(user_id#61L, region_id#62L, region_id#62L)
RuleExecutor.execute[SparkOptimizer(org.apache.spark.sql.internal.BaseSessionStateBuilder$$anon$2@445265b0)] final result ==> old【
CreateDataSourceTableAsSelectCommand `order_all`, Overwrite, [order_id, user_id, region_id, order_num, user_name, region_name]
+- Project [order_id#60L, user_id#61L, region_id#62L, order_num#63, user_name#72, region_name#79]
   +- Join Inner, ((region_id#62L = region_id#78L) AND (region_id#71L = region_id#78L))
      :- Join Inner, ((user_id#61L = user_id#70L) AND (region_id#62L = region_id#71L))
      :  :- SubqueryAlias o
      :  :  +- SubqueryAlias order
      :  :     +- View (`order`, [order_id#60L,user_id#61L,region_id#62L,order_num#63])
      :  :        +- Project [id#58L AS order_id#60L, (id#58L % cast(1000 as bigint)) AS user_id#61L, (id#58L % cast(10 as bigint)) AS region_id#62L, cast(id#58L as string) AS order_num#63]
      :  :           +- Range (0, 10000, step=1, splits=Some(10))
      :  +- SubqueryAlias u
      :     +- SubqueryAlias user
      :        +- View (`user`, [user_id#70L,region_id#71L,user_name#72])
      :           +- Project [id#68L AS user_id#70L, (id#68L % cast(10 as bigint)) AS region_id#71L, cast(id#68L as string) AS user_name#72]
      :              +- Range (0, 1000, step=1, splits=Some(5))
      +- SubqueryAlias r
         +- SubqueryAlias region
            +- View (`region`, [region_id#78L,region_name#79])
               +- Project [id#76L AS region_id#78L, cast(id#76L as string) AS region_name#79]
                  +- Range (0, 10, step=1, splits=Some(1))
】==> new【
CreateDataSourceTableAsSelectCommand `order_all`, Overwrite, [order_id, user_id, region_id, order_num, user_name, region_name]
+- Project [order_id#60L, user_id#61L, region_id#62L, order_num#63, user_name#72, region_name#79]
   +- Join Inner, (((user_id#61L = user_id#70L) AND (region_id#62L = region_id#71L)) AND (region_id#62L = region_id#78L))
      :- Join Inner, (region_id#71L = region_id#78L)
      :  :- Project [id#68L AS user_id#70L, (id#68L % 10) AS region_id#71L, cast(id#68L as string) AS user_name#72]
      :  :  +- Filter isnotnull((id#68L % 10))
      :  :     +- Range (0, 1000, step=1, splits=Some(5))
      :  +- Project [id#76L AS region_id#78L, cast(id#76L as string) AS region_name#79]
      :     +- Range (0, 10, step=1, splits=Some(1))
      +- Project [id#58L AS order_id#60L, (id#58L % 1000) AS user_id#61L, (id#58L % 10) AS region_id#62L, cast(id#58L as string) AS order_num#63]
         +- Filter (isnotnull((id#58L % 1000)) AND isnotnull((id#58L % 10)))
            +- Range (0, 10000, step=1, splits=Some(10))
】


optimizedPlan ====>
CreateDataSourceTableAsSelectCommand `order_all`, Overwrite, [order_id, user_id, region_id, order_num, user_name, region_name]
+- Project [order_id#60L, user_id#61L, region_id#62L, order_num#63, user_name#72, region_name#79]
   +- Join Inner, (((user_id#61L = user_id#70L) AND (region_id#62L = region_id#71L)) AND (region_id#62L = region_id#78L))
      :- Join Inner, (region_id#71L = region_id#78L)
      :  :- Project [id#68L AS user_id#70L, (id#68L % 10) AS region_id#71L, cast(id#68L as string) AS user_name#72]
      :  :  +- Filter isnotnull((id#68L % 10))
      :  :     +- Range (0, 1000, step=1, splits=Some(5))
      :  +- Project [id#76L AS region_id#78L, cast(id#76L as string) AS region_name#79]
      :     +- Range (0, 10, step=1, splits=Some(1))
      +- Project [id#58L AS order_id#60L, (id#58L % 1000) AS user_id#61L, (id#58L % 10) AS region_id#62L, cast(id#58L as string) AS order_num#63]
         +- Filter (isnotnull((id#58L % 1000)) AND isnotnull((id#58L % 10)))
            +- Range (0, 10000, step=1, splits=Some(10))

SparkPlanner(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2@321bf4b1) - org.apache.spark.sql.execution.SparkStrategies$SpecialLimits$ -->【
ReturnAnswer
+- CreateDataSourceTableAsSelectCommand `order_all`, Overwrite, [order_id, user_id, region_id, order_num, user_name, region_name]
   +- Project [order_id#60L, user_id#61L, region_id#62L, order_num#63, user_name#72, region_name#79]
      +- Join Inner, (((user_id#61L = user_id#70L) AND (region_id#62L = region_id#71L)) AND (region_id#62L = region_id#78L))
         :- Join Inner, (region_id#71L = region_id#78L)
         :  :- Project [id#68L AS user_id#70L, (id#68L % 10) AS region_id#71L, cast(id#68L as string) AS user_name#72]
         :  :  +- Filter isnotnull((id#68L % 10))
         :  :     +- Range (0, 1000, step=1, splits=Some(5))
         :  +- Project [id#76L AS region_id#78L, cast(id#76L as string) AS region_name#79]
         :     +- Range (0, 10, step=1, splits=Some(1))
         +- Project [id#58L AS order_id#60L, (id#58L % 1000) AS user_id#61L, (id#58L % 10) AS region_id#62L, cast(id#58L as string) AS order_num#63]
            +- Filter (isnotnull((id#58L % 1000)) AND isnotnull((id#58L % 10)))
               +- Range (0, 10000, step=1, splits=Some(10))
】 --> 【
PlanLater CreateDataSourceTableAsSelectCommand `order_all`, Overwrite, [order_id, user_id, region_id, order_num, user_name, region_name]
】

SparkPlanner(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2@321bf4b1) - org.apache.spark.sql.execution.SparkStrategies$BasicOperators$ -->【
CreateDataSourceTableAsSelectCommand `order_all`, Overwrite, [order_id, user_id, region_id, order_num, user_name, region_name]
+- Project [order_id#60L, user_id#61L, region_id#62L, order_num#63, user_name#72, region_name#79]
   +- Join Inner, (((user_id#61L = user_id#70L) AND (region_id#62L = region_id#71L)) AND (region_id#62L = region_id#78L))
      :- Join Inner, (region_id#71L = region_id#78L)
      :  :- Project [id#68L AS user_id#70L, (id#68L % 10) AS region_id#71L, cast(id#68L as string) AS user_name#72]
      :  :  +- Filter isnotnull((id#68L % 10))
      :  :     +- Range (0, 1000, step=1, splits=Some(5))
      :  +- Project [id#76L AS region_id#78L, cast(id#76L as string) AS region_name#79]
      :     +- Range (0, 10, step=1, splits=Some(1))
      +- Project [id#58L AS order_id#60L, (id#58L % 1000) AS user_id#61L, (id#58L % 10) AS region_id#62L, cast(id#58L as string) AS order_num#63]
         +- Filter (isnotnull((id#58L % 1000)) AND isnotnull((id#58L % 10)))
            +- Range (0, 10000, step=1, splits=Some(10))
】 --> 【
Execute CreateDataSourceTableAsSelectCommand `order_all`, Overwrite, [order_id, user_id, region_id, order_num, user_name, region_name]
+- PlanLater Project [order_id#60L, user_id#61L, region_id#62L, order_num#63, user_name#72, region_name#79]
】

SparkPlanner(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2@321bf4b1) - org.apache.spark.sql.execution.SparkStrategies$BasicOperators$ -->【
Project [order_id#60L, user_id#61L, region_id#62L, order_num#63, user_name#72, region_name#79]
+- Join Inner, (((user_id#61L = user_id#70L) AND (region_id#62L = region_id#71L)) AND (region_id#62L = region_id#78L))
   :- Join Inner, (region_id#71L = region_id#78L)
   :  :- Project [id#68L AS user_id#70L, (id#68L % 10) AS region_id#71L, cast(id#68L as string) AS user_name#72]
   :  :  +- Filter isnotnull((id#68L % 10))
   :  :     +- Range (0, 1000, step=1, splits=Some(5))
   :  +- Project [id#76L AS region_id#78L, cast(id#76L as string) AS region_name#79]
   :     +- Range (0, 10, step=1, splits=Some(1))
   +- Project [id#58L AS order_id#60L, (id#58L % 1000) AS user_id#61L, (id#58L % 10) AS region_id#62L, cast(id#58L as string) AS order_num#63]
      +- Filter (isnotnull((id#58L % 1000)) AND isnotnull((id#58L % 10)))
         +- Range (0, 10000, step=1, splits=Some(10))
】 --> 【
Project [order_id#60L, user_id#61L, region_id#62L, order_num#63, user_name#72, region_name#79]
+- PlanLater Join Inner, (((user_id#61L = user_id#70L) AND (region_id#62L = region_id#71L)) AND (region_id#62L = region_id#78L))
】

21:32:52.854 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - Considering join on: Some((((user_id#61L = user_id#70L) AND (region_id#62L = region_id#71L)) AND (region_id#62L = region_id#78L)))
21:32:52.854 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - leftKeys:List(user_id#70L, region_id#71L, region_id#78L) | rightKeys:List(user_id#61L, region_id#62L, region_id#62L)
21:32:52.856 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - Considering join on: Some((((user_id#61L = user_id#70L) AND (region_id#62L = region_id#71L)) AND (region_id#62L = region_id#78L)))
21:32:52.857 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - leftKeys:List(user_id#70L, region_id#71L, region_id#78L) | rightKeys:List(user_id#61L, region_id#62L, region_id#62L)
21:32:52.857 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.plans.logical.statsEstimation.FilterEstimation - [CBO] Unsupported filter condition: isnotnull((id#68L % 10))
21:32:52.857 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - Considering join on: Some((region_id#71L = region_id#78L))
21:32:52.857 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - leftKeys:List(region_id#71L) | rightKeys:List(region_id#78L)
21:32:52.858 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.plans.logical.statsEstimation.FilterEstimation - [CBO] Unsupported filter condition: isnotnull((id#58L % 1000))
21:32:52.858 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.plans.logical.statsEstimation.FilterEstimation - [CBO] Unsupported filter condition: isnotnull((id#58L % 10))
SparkPlanner(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2@321bf4b1) - org.apache.spark.sql.execution.SparkStrategies$JoinSelection$ -->【
Join Inner, (((user_id#61L = user_id#70L) AND (region_id#62L = region_id#71L)) AND (region_id#62L = region_id#78L))
:- Join Inner, (region_id#71L = region_id#78L)
:  :- Project [id#68L AS user_id#70L, (id#68L % 10) AS region_id#71L, cast(id#68L as string) AS user_name#72]
:  :  +- Filter isnotnull((id#68L % 10))
:  :     +- Range (0, 1000, step=1, splits=Some(5))
:  +- Project [id#76L AS region_id#78L, cast(id#76L as string) AS region_name#79]
:     +- Range (0, 10, step=1, splits=Some(1))
+- Project [id#58L AS order_id#60L, (id#58L % 1000) AS user_id#61L, (id#58L % 10) AS region_id#62L, cast(id#58L as string) AS order_num#63]
   +- Filter (isnotnull((id#58L % 1000)) AND isnotnull((id#58L % 10)))
      +- Range (0, 10000, step=1, splits=Some(10))
】 --> 【
SortMergeJoin [user_id#70L, region_id#71L, region_id#78L], [user_id#61L, region_id#62L, region_id#62L], Inner
:- PlanLater Join Inner, (region_id#71L = region_id#78L)
+- PlanLater Project [id#58L AS order_id#60L, (id#58L % 1000) AS user_id#61L, (id#58L % 10) AS region_id#62L, cast(id#58L as string) AS order_num#63]
】

21:32:52.872 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - Considering join on: Some((region_id#71L = region_id#78L))
21:32:52.872 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - leftKeys:List(region_id#71L) | rightKeys:List(region_id#78L)
21:32:52.872 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - Considering join on: Some((region_id#71L = region_id#78L))
21:32:52.872 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - leftKeys:List(region_id#71L) | rightKeys:List(region_id#78L)
SparkPlanner(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2@321bf4b1) - org.apache.spark.sql.execution.SparkStrategies$JoinSelection$ -->【
Join Inner, (region_id#71L = region_id#78L)
:- Project [id#68L AS user_id#70L, (id#68L % 10) AS region_id#71L, cast(id#68L as string) AS user_name#72]
:  +- Filter isnotnull((id#68L % 10))
:     +- Range (0, 1000, step=1, splits=Some(5))
+- Project [id#76L AS region_id#78L, cast(id#76L as string) AS region_name#79]
   +- Range (0, 10, step=1, splits=Some(1))
】 --> 【
SortMergeJoin [region_id#71L], [region_id#78L], Inner
:- PlanLater Project [id#68L AS user_id#70L, (id#68L % 10) AS region_id#71L, cast(id#68L as string) AS user_name#72]
+- PlanLater Project [id#76L AS region_id#78L, cast(id#76L as string) AS region_name#79]
】

SparkPlanner(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2@321bf4b1) - org.apache.spark.sql.execution.SparkStrategies$BasicOperators$ -->【
Project [id#68L AS user_id#70L, (id#68L % 10) AS region_id#71L, cast(id#68L as string) AS user_name#72]
+- Filter isnotnull((id#68L % 10))
   +- Range (0, 1000, step=1, splits=Some(5))
】 --> 【
Project [id#68L AS user_id#70L, (id#68L % 10) AS region_id#71L, cast(id#68L as string) AS user_name#72]
+- PlanLater Filter isnotnull((id#68L % 10))
】

SparkPlanner(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2@321bf4b1) - org.apache.spark.sql.execution.SparkStrategies$BasicOperators$ -->【
Filter isnotnull((id#68L % 10))
+- Range (0, 1000, step=1, splits=Some(5))
】 --> 【
Filter isnotnull((id#68L % 10))
+- PlanLater Range (0, 1000, step=1, splits=Some(5))
】

SparkPlanner(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2@321bf4b1) - org.apache.spark.sql.execution.SparkStrategies$BasicOperators$ -->【
Range (0, 1000, step=1, splits=Some(5))
】 --> 【
Range (0, 1000, step=1, splits=5)
】

SparkPlanner(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2@321bf4b1) - org.apache.spark.sql.execution.SparkStrategies$BasicOperators$ -->【
Project [id#76L AS region_id#78L, cast(id#76L as string) AS region_name#79]
+- Range (0, 10, step=1, splits=Some(1))
】 --> 【
Project [id#76L AS region_id#78L, cast(id#76L as string) AS region_name#79]
+- PlanLater Range (0, 10, step=1, splits=Some(1))
】

SparkPlanner(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2@321bf4b1) - org.apache.spark.sql.execution.SparkStrategies$BasicOperators$ -->【
Range (0, 10, step=1, splits=Some(1))
】 --> 【
Range (0, 10, step=1, splits=1)
】

SparkPlanner(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2@321bf4b1) - org.apache.spark.sql.execution.SparkStrategies$BasicOperators$ -->【
Project [id#58L AS order_id#60L, (id#58L % 1000) AS user_id#61L, (id#58L % 10) AS region_id#62L, cast(id#58L as string) AS order_num#63]
+- Filter (isnotnull((id#58L % 1000)) AND isnotnull((id#58L % 10)))
   +- Range (0, 10000, step=1, splits=Some(10))
】 --> 【
Project [id#58L AS order_id#60L, (id#58L % 1000) AS user_id#61L, (id#58L % 10) AS region_id#62L, cast(id#58L as string) AS order_num#63]
+- PlanLater Filter (isnotnull((id#58L % 1000)) AND isnotnull((id#58L % 10)))
】

SparkPlanner(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2@321bf4b1) - org.apache.spark.sql.execution.SparkStrategies$BasicOperators$ -->【
Filter (isnotnull((id#58L % 1000)) AND isnotnull((id#58L % 10)))
+- Range (0, 10000, step=1, splits=Some(10))
】 --> 【
Filter (isnotnull((id#58L % 1000)) AND isnotnull((id#58L % 10)))
+- PlanLater Range (0, 10000, step=1, splits=Some(10))
】

SparkPlanner(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2@321bf4b1) - org.apache.spark.sql.execution.SparkStrategies$BasicOperators$ -->【
Range (0, 10000, step=1, splits=Some(10))
】 --> 【
Range (0, 10000, step=1, splits=10)
】

sparkPlan ====>
Execute CreateDataSourceTableAsSelectCommand `order_all`, Overwrite, [order_id, user_id, region_id, order_num, user_name, region_name]
+- Project [order_id#60L, user_id#61L, region_id#62L, order_num#63, user_name#72, region_name#79]
   +- SortMergeJoin [user_id#70L, region_id#71L, region_id#78L], [user_id#61L, region_id#62L, region_id#62L], Inner
      :- SortMergeJoin [region_id#71L], [region_id#78L], Inner
      :  :- Project [id#68L AS user_id#70L, (id#68L % 10) AS region_id#71L, cast(id#68L as string) AS user_name#72]
      :  :  +- Filter isnotnull((id#68L % 10))
      :  :     +- Range (0, 1000, step=1, splits=5)
      :  +- Project [id#76L AS region_id#78L, cast(id#76L as string) AS region_name#79]
      :     +- Range (0, 10, step=1, splits=1)
      +- Project [id#58L AS order_id#60L, (id#58L % 1000) AS user_id#61L, (id#58L % 10) AS region_id#62L, cast(id#58L as string) AS order_num#63]
         +- Filter (isnotnull((id#58L % 1000)) AND isnotnull((id#58L % 10)))
            +- Range (0, 10000, step=1, splits=10)

21:32:52.897 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.execution.adaptive.InsertAdaptiveSparkPlan - Adaptive execution enabled for plan:
Project [order_id#60L, user_id#61L, region_id#62L, order_num#63, user_name#72, region_name#79]
+- SortMergeJoin [user_id#70L, region_id#71L, region_id#78L], [user_id#61L, region_id#62L, region_id#62L], Inner
   :- SortMergeJoin [region_id#71L], [region_id#78L], Inner
   :  :- Project [id#68L AS user_id#70L, (id#68L % 10) AS region_id#71L, cast(id#68L as string) AS user_name#72]
   :  :  +- Filter isnotnull((id#68L % 10))
   :  :     +- Range (0, 1000, step=1, splits=5)
   :  +- Project [id#76L AS region_id#78L, cast(id#76L as string) AS region_name#79]
   :     +- Range (0, 10, step=1, splits=1)
   +- Project [id#58L AS order_id#60L, (id#58L % 1000) AS user_id#61L, (id#58L % 10) AS region_id#62L, cast(id#58L as string) AS order_num#63]
      +- Filter (isnotnull((id#58L % 1000)) AND isnotnull((id#58L % 10)))
         +- Range (0, 10000, step=1, splits=10)
 ==>
Project [order_id#60L, user_id#61L, region_id#62L, order_num#63, user_name#72, region_name#79]
+- SortMergeJoin [user_id#70L, region_id#71L, region_id#78L], [user_id#61L, region_id#62L, region_id#62L], Inner
   :- SortMergeJoin [region_id#71L], [region_id#78L], Inner
   :  :- Project [id#68L AS user_id#70L, (id#68L % 10) AS region_id#71L, cast(id#68L as string) AS user_name#72]
   :  :  +- Filter isnotnull((id#68L % 10))
   :  :     +- Range (0, 1000, step=1, splits=5)
   :  +- Project [id#76L AS region_id#78L, cast(id#76L as string) AS region_name#79]
   :     +- Range (0, 10, step=1, splits=1)
   +- Project [id#58L AS order_id#60L, (id#58L % 1000) AS user_id#61L, (id#58L % 10) AS region_id#62L, cast(id#58L as string) AS order_num#63]
      +- Filter (isnotnull((id#58L % 1000)) AND isnotnull((id#58L % 10)))
         +- Range (0, 10000, step=1, splits=10)

org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec$ - applyPhysicalRules - org.apache.spark.sql.execution.exchange.EnsureRequirements -->【
Project [order_id#60L, user_id#61L, region_id#62L, order_num#63, user_name#72, region_name#79]
+- SortMergeJoin [user_id#70L, region_id#71L, region_id#78L], [user_id#61L, region_id#62L, region_id#62L], Inner
   :- SortMergeJoin [region_id#71L], [region_id#78L], Inner
   :  :- Project [id#68L AS user_id#70L, (id#68L % 10) AS region_id#71L, cast(id#68L as string) AS user_name#72]
   :  :  +- Filter isnotnull((id#68L % 10))
   :  :     +- Range (0, 1000, step=1, splits=5)
   :  +- Project [id#76L AS region_id#78L, cast(id#76L as string) AS region_name#79]
   :     +- Range (0, 10, step=1, splits=1)
   +- Project [id#58L AS order_id#60L, (id#58L % 1000) AS user_id#61L, (id#58L % 10) AS region_id#62L, cast(id#58L as string) AS order_num#63]
      +- Filter (isnotnull((id#58L % 1000)) AND isnotnull((id#58L % 10)))
         +- Range (0, 10000, step=1, splits=10)
】AQE --> 【
Project [order_id#60L, user_id#61L, region_id#62L, order_num#63, user_name#72, region_name#79]
+- SortMergeJoin [user_id#70L, region_id#71L, region_id#78L], [user_id#61L, region_id#62L, region_id#62L], Inner
   :- Sort [user_id#70L ASC NULLS FIRST, region_id#71L ASC NULLS FIRST, region_id#78L ASC NULLS FIRST], false, 0
   :  +- Exchange hashpartitioning(user_id#70L, region_id#71L, region_id#71L, 8), ENSURE_REQUIREMENTS, [plan_id=90]
   :     +- SortMergeJoin [region_id#71L], [region_id#78L], Inner
   :        :- Sort [region_id#71L ASC NULLS FIRST], false, 0
   :        :  +- Exchange hashpartitioning(region_id#71L, 8), ENSURE_REQUIREMENTS, [plan_id=82]
   :        :     +- Project [id#68L AS user_id#70L, (id#68L % 10) AS region_id#71L, cast(id#68L as string) AS user_name#72]
   :        :        +- Filter isnotnull((id#68L % 10))
   :        :           +- Range (0, 1000, step=1, splits=5)
   :        +- Sort [region_id#78L ASC NULLS FIRST], false, 0
   :           +- Exchange hashpartitioning(region_id#78L, 8), ENSURE_REQUIREMENTS, [plan_id=83]
   :              +- Project [id#76L AS region_id#78L, cast(id#76L as string) AS region_name#79]
   :                 +- Range (0, 10, step=1, splits=1)
   +- Sort [user_id#61L ASC NULLS FIRST, region_id#62L ASC NULLS FIRST, region_id#62L ASC NULLS FIRST], false, 0
      +- Exchange hashpartitioning(user_id#61L, region_id#62L, region_id#62L, 8), ENSURE_REQUIREMENTS, [plan_id=89]
         +- Project [id#58L AS order_id#60L, (id#58L % 1000) AS user_id#61L, (id#58L % 10) AS region_id#62L, cast(id#58L as string) AS order_num#63]
            +- Filter (isnotnull((id#58L % 1000)) AND isnotnull((id#58L % 10)))
               +- Range (0, 10000, step=1, splits=10)
】

org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec$ final result ==>【
Project [order_id#60L, user_id#61L, region_id#62L, order_num#63, user_name#72, region_name#79]
+- SortMergeJoin [user_id#70L, region_id#71L, region_id#78L], [user_id#61L, region_id#62L, region_id#62L], Inner
   :- SortMergeJoin [region_id#71L], [region_id#78L], Inner
   :  :- Project [id#68L AS user_id#70L, (id#68L % 10) AS region_id#71L, cast(id#68L as string) AS user_name#72]
   :  :  +- Filter isnotnull((id#68L % 10))
   :  :     +- Range (0, 1000, step=1, splits=5)
   :  +- Project [id#76L AS region_id#78L, cast(id#76L as string) AS region_name#79]
   :     +- Range (0, 10, step=1, splits=1)
   +- Project [id#58L AS order_id#60L, (id#58L % 1000) AS user_id#61L, (id#58L % 10) AS region_id#62L, cast(id#58L as string) AS order_num#63]
      +- Filter (isnotnull((id#58L % 1000)) AND isnotnull((id#58L % 10)))
         +- Range (0, 10000, step=1, splits=10)
】AQE ==> 【
Project [order_id#60L, user_id#61L, region_id#62L, order_num#63, user_name#72, region_name#79]
+- SortMergeJoin [user_id#70L, region_id#71L, region_id#78L], [user_id#61L, region_id#62L, region_id#62L], Inner
   :- Sort [user_id#70L ASC NULLS FIRST, region_id#71L ASC NULLS FIRST, region_id#78L ASC NULLS FIRST], false, 0
   :  +- Exchange hashpartitioning(user_id#70L, region_id#71L, region_id#71L, 8), ENSURE_REQUIREMENTS, [plan_id=90]
   :     +- SortMergeJoin [region_id#71L], [region_id#78L], Inner
   :        :- Sort [region_id#71L ASC NULLS FIRST], false, 0
   :        :  +- Exchange hashpartitioning(region_id#71L, 8), ENSURE_REQUIREMENTS, [plan_id=82]
   :        :     +- Project [id#68L AS user_id#70L, (id#68L % 10) AS region_id#71L, cast(id#68L as string) AS user_name#72]
   :        :        +- Filter isnotnull((id#68L % 10))
   :        :           +- Range (0, 1000, step=1, splits=5)
   :        +- Sort [region_id#78L ASC NULLS FIRST], false, 0
   :           +- Exchange hashpartitioning(region_id#78L, 8), ENSURE_REQUIREMENTS, [plan_id=83]
   :              +- Project [id#76L AS region_id#78L, cast(id#76L as string) AS region_name#79]
   :                 +- Range (0, 10, step=1, splits=1)
   +- Sort [user_id#61L ASC NULLS FIRST, region_id#62L ASC NULLS FIRST, region_id#62L ASC NULLS FIRST], false, 0
      +- Exchange hashpartitioning(user_id#61L, region_id#62L, region_id#62L, 8), ENSURE_REQUIREMENTS, [plan_id=89]
         +- Project [id#58L AS order_id#60L, (id#58L % 1000) AS user_id#61L, (id#58L % 10) AS region_id#62L, cast(id#58L as string) AS order_num#63]
            +- Filter (isnotnull((id#58L % 1000)) AND isnotnull((id#58L % 10)))
               +- Range (0, 10000, step=1, splits=10)
】

org.apache.spark.sql.execution.QueryExecution$ - org.apache.spark.sql.execution.adaptive.InsertAdaptiveSparkPlan -->【
Execute CreateDataSourceTableAsSelectCommand `order_all`, Overwrite, [order_id, user_id, region_id, order_num, user_name, region_name]
+- Project [order_id#60L, user_id#61L, region_id#62L, order_num#63, user_name#72, region_name#79]
   +- SortMergeJoin [user_id#70L, region_id#71L, region_id#78L], [user_id#61L, region_id#62L, region_id#62L], Inner
      :- SortMergeJoin [region_id#71L], [region_id#78L], Inner
      :  :- Project [id#68L AS user_id#70L, (id#68L % 10) AS region_id#71L, cast(id#68L as string) AS user_name#72]
      :  :  +- Filter isnotnull((id#68L % 10))
      :  :     +- Range (0, 1000, step=1, splits=5)
      :  +- Project [id#76L AS region_id#78L, cast(id#76L as string) AS region_name#79]
      :     +- Range (0, 10, step=1, splits=1)
      +- Project [id#58L AS order_id#60L, (id#58L % 1000) AS user_id#61L, (id#58L % 10) AS region_id#62L, cast(id#58L as string) AS order_num#63]
         +- Filter (isnotnull((id#58L % 1000)) AND isnotnull((id#58L % 10)))
            +- Range (0, 10000, step=1, splits=10)
】 --> 【
Execute CreateDataSourceTableAsSelectCommand `order_all`, Overwrite, [order_id, user_id, region_id, order_num, user_name, region_name]
+- AdaptiveSparkPlan isFinalPlan=false
   +- Project [order_id#60L, user_id#61L, region_id#62L, order_num#63, user_name#72, region_name#79]
      +- SortMergeJoin [user_id#70L, region_id#71L, region_id#78L], [user_id#61L, region_id#62L, region_id#62L], Inner
         :- Sort [user_id#70L ASC NULLS FIRST, region_id#71L ASC NULLS FIRST, region_id#78L ASC NULLS FIRST], false, 0
         :  +- Exchange hashpartitioning(user_id#70L, region_id#71L, region_id#71L, 8), ENSURE_REQUIREMENTS, [plan_id=90]
         :     +- SortMergeJoin [region_id#71L], [region_id#78L], Inner
         :        :- Sort [region_id#71L ASC NULLS FIRST], false, 0
         :        :  +- Exchange hashpartitioning(region_id#71L, 8), ENSURE_REQUIREMENTS, [plan_id=82]
         :        :     +- Project [id#68L AS user_id#70L, (id#68L % 10) AS region_id#71L, cast(id#68L as string) AS user_name#72]
         :        :        +- Filter isnotnull((id#68L % 10))
         :        :           +- Range (0, 1000, step=1, splits=5)
         :        +- Sort [region_id#78L ASC NULLS FIRST], false, 0
         :           +- Exchange hashpartitioning(region_id#78L, 8), ENSURE_REQUIREMENTS, [plan_id=83]
         :              +- Project [id#76L AS region_id#78L, cast(id#76L as string) AS region_name#79]
         :                 +- Range (0, 10, step=1, splits=1)
         +- Sort [user_id#61L ASC NULLS FIRST, region_id#62L ASC NULLS FIRST, region_id#62L ASC NULLS FIRST], false, 0
            +- Exchange hashpartitioning(user_id#61L, region_id#62L, region_id#62L, 8), ENSURE_REQUIREMENTS, [plan_id=89]
               +- Project [id#58L AS order_id#60L, (id#58L % 1000) AS user_id#61L, (id#58L % 10) AS region_id#62L, cast(id#58L as string) AS order_num#63]
                  +- Filter (isnotnull((id#58L % 1000)) AND isnotnull((id#58L % 10)))
                     +- Range (0, 10000, step=1, splits=10)
】

org.apache.spark.sql.execution.QueryExecution$ prepareForExecution final result ==>【
Execute CreateDataSourceTableAsSelectCommand `order_all`, Overwrite, [order_id, user_id, region_id, order_num, user_name, region_name]
+- Project [order_id#60L, user_id#61L, region_id#62L, order_num#63, user_name#72, region_name#79]
   +- SortMergeJoin [user_id#70L, region_id#71L, region_id#78L], [user_id#61L, region_id#62L, region_id#62L], Inner
      :- SortMergeJoin [region_id#71L], [region_id#78L], Inner
      :  :- Project [id#68L AS user_id#70L, (id#68L % 10) AS region_id#71L, cast(id#68L as string) AS user_name#72]
      :  :  +- Filter isnotnull((id#68L % 10))
      :  :     +- Range (0, 1000, step=1, splits=5)
      :  +- Project [id#76L AS region_id#78L, cast(id#76L as string) AS region_name#79]
      :     +- Range (0, 10, step=1, splits=1)
      +- Project [id#58L AS order_id#60L, (id#58L % 1000) AS user_id#61L, (id#58L % 10) AS region_id#62L, cast(id#58L as string) AS order_num#63]
         +- Filter (isnotnull((id#58L % 1000)) AND isnotnull((id#58L % 10)))
            +- Range (0, 10000, step=1, splits=10)
】 ==> 【
Execute CreateDataSourceTableAsSelectCommand `order_all`, Overwrite, [order_id, user_id, region_id, order_num, user_name, region_name]
+- AdaptiveSparkPlan isFinalPlan=false
   +- Project [order_id#60L, user_id#61L, region_id#62L, order_num#63, user_name#72, region_name#79]
      +- SortMergeJoin [user_id#70L, region_id#71L, region_id#78L], [user_id#61L, region_id#62L, region_id#62L], Inner
         :- Sort [user_id#70L ASC NULLS FIRST, region_id#71L ASC NULLS FIRST, region_id#78L ASC NULLS FIRST], false, 0
         :  +- Exchange hashpartitioning(user_id#70L, region_id#71L, region_id#71L, 8), ENSURE_REQUIREMENTS, [plan_id=90]
         :     +- SortMergeJoin [region_id#71L], [region_id#78L], Inner
         :        :- Sort [region_id#71L ASC NULLS FIRST], false, 0
         :        :  +- Exchange hashpartitioning(region_id#71L, 8), ENSURE_REQUIREMENTS, [plan_id=82]
         :        :     +- Project [id#68L AS user_id#70L, (id#68L % 10) AS region_id#71L, cast(id#68L as string) AS user_name#72]
         :        :        +- Filter isnotnull((id#68L % 10))
         :        :           +- Range (0, 1000, step=1, splits=5)
         :        +- Sort [region_id#78L ASC NULLS FIRST], false, 0
         :           +- Exchange hashpartitioning(region_id#78L, 8), ENSURE_REQUIREMENTS, [plan_id=83]
         :              +- Project [id#76L AS region_id#78L, cast(id#76L as string) AS region_name#79]
         :                 +- Range (0, 10, step=1, splits=1)
         +- Sort [user_id#61L ASC NULLS FIRST, region_id#62L ASC NULLS FIRST, region_id#62L ASC NULLS FIRST], false, 0
            +- Exchange hashpartitioning(user_id#61L, region_id#62L, region_id#62L, 8), ENSURE_REQUIREMENTS, [plan_id=89]
               +- Project [id#58L AS order_id#60L, (id#58L % 1000) AS user_id#61L, (id#58L % 10) AS region_id#62L, cast(id#58L as string) AS order_num#63]
                  +- Filter (isnotnull((id#58L % 1000)) AND isnotnull((id#58L % 10)))
                     +- Range (0, 10000, step=1, splits=10)
】

executedPlan ====>
Execute CreateDataSourceTableAsSelectCommand `order_all`, Overwrite, [order_id, user_id, region_id, order_num, user_name, region_name]
+- AdaptiveSparkPlan isFinalPlan=false
   +- Project [order_id#60L, user_id#61L, region_id#62L, order_num#63, user_name#72, region_name#79]
      +- SortMergeJoin [user_id#70L, region_id#71L, region_id#78L], [user_id#61L, region_id#62L, region_id#62L], Inner
         :- Sort [user_id#70L ASC NULLS FIRST, region_id#71L ASC NULLS FIRST, region_id#78L ASC NULLS FIRST], false, 0
         :  +- Exchange hashpartitioning(user_id#70L, region_id#71L, region_id#71L, 8), ENSURE_REQUIREMENTS, [plan_id=90]
         :     +- SortMergeJoin [region_id#71L], [region_id#78L], Inner
         :        :- Sort [region_id#71L ASC NULLS FIRST], false, 0
         :        :  +- Exchange hashpartitioning(region_id#71L, 8), ENSURE_REQUIREMENTS, [plan_id=82]
         :        :     +- Project [id#68L AS user_id#70L, (id#68L % 10) AS region_id#71L, cast(id#68L as string) AS user_name#72]
         :        :        +- Filter isnotnull((id#68L % 10))
         :        :           +- Range (0, 1000, step=1, splits=5)
         :        +- Sort [region_id#78L ASC NULLS FIRST], false, 0
         :           +- Exchange hashpartitioning(region_id#78L, 8), ENSURE_REQUIREMENTS, [plan_id=83]
         :              +- Project [id#76L AS region_id#78L, cast(id#76L as string) AS region_name#79]
         :                 +- Range (0, 10, step=1, splits=1)
         +- Sort [user_id#61L ASC NULLS FIRST, region_id#62L ASC NULLS FIRST, region_id#62L ASC NULLS FIRST], false, 0
            +- Exchange hashpartitioning(user_id#61L, region_id#62L, region_id#62L, 8), ENSURE_REQUIREMENTS, [plan_id=89]
               +- Project [id#58L AS order_id#60L, (id#58L % 1000) AS user_id#61L, (id#58L % 10) AS region_id#62L, cast(id#58L as string) AS order_num#63]
                  +- Filter (isnotnull((id#58L % 1000)) AND isnotnull((id#58L % 10)))
                     +- Range (0, 10000, step=1, splits=10)

21:32:53.058 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.internal.io.FileCommitProtocol - Creating committer org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol; job 3c078869-a689-431d-9543-6f900ece7ba0; output=file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-warehouse/order_all; dynamic=false
21:32:53.064 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.internal.io.FileCommitProtocol - Using (String, String, Boolean) constructor
21:32:53.103 [ScalaTest-run-running-PhysicalPlanSpec] INFO  org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat - Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21:32:53.134 [ScalaTest-run-running-PhysicalPlanSpec] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21:32:53.134 [ScalaTest-run-running-PhysicalPlanSpec] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21:32:53.363 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec - withFinalPlanUpdate.doExecute ==> before get final plan:
Project [order_id#60L, user_id#61L, region_id#62L, order_num#63, user_name#72, region_name#79]
+- SortMergeJoin [user_id#70L, region_id#71L, region_id#78L], [user_id#61L, region_id#62L, region_id#62L], Inner
   :- Sort [user_id#70L ASC NULLS FIRST, region_id#71L ASC NULLS FIRST, region_id#78L ASC NULLS FIRST], false, 0
   :  +- Exchange hashpartitioning(user_id#70L, region_id#71L, region_id#71L, 8), ENSURE_REQUIREMENTS, [plan_id=90]
   :     +- SortMergeJoin [region_id#71L], [region_id#78L], Inner
   :        :- Sort [region_id#71L ASC NULLS FIRST], false, 0
   :        :  +- Exchange hashpartitioning(region_id#71L, 8), ENSURE_REQUIREMENTS, [plan_id=82]
   :        :     +- Project [id#68L AS user_id#70L, (id#68L % 10) AS region_id#71L, cast(id#68L as string) AS user_name#72]
   :        :        +- Filter isnotnull((id#68L % 10))
   :        :           +- Range (0, 1000, step=1, splits=5)
   :        +- Sort [region_id#78L ASC NULLS FIRST], false, 0
   :           +- Exchange hashpartitioning(region_id#78L, 8), ENSURE_REQUIREMENTS, [plan_id=83]
   :              +- Project [id#76L AS region_id#78L, cast(id#76L as string) AS region_name#79]
   :                 +- Range (0, 10, step=1, splits=1)
   +- Sort [user_id#61L ASC NULLS FIRST, region_id#62L ASC NULLS FIRST, region_id#62L ASC NULLS FIRST], false, 0
      +- Exchange hashpartitioning(user_id#61L, region_id#62L, region_id#62L, 8), ENSURE_REQUIREMENTS, [plan_id=89]
         +- Project [id#58L AS order_id#60L, (id#58L % 1000) AS user_id#61L, (id#58L % 10) AS region_id#62L, cast(id#58L as string) AS order_num#63]
            +- Filter (isnotnull((id#58L % 1000)) AND isnotnull((id#58L % 10)))
               +- Range (0, 10000, step=1, splits=10)

org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec$ - applyPhysicalRules - org.apache.spark.sql.execution.CollapseCodegenStages -->【
Exchange hashpartitioning(region_id#71L, 8), ENSURE_REQUIREMENTS, [plan_id=82]
+- Project [id#68L AS user_id#70L, (id#68L % 10) AS region_id#71L, cast(id#68L as string) AS user_name#72]
   +- Filter isnotnull((id#68L % 10))
      +- Range (0, 1000, step=1, splits=5)
】AQE --> 【
Exchange hashpartitioning(region_id#71L, 8), ENSURE_REQUIREMENTS, [plan_id=117]
+- *(1) Project [id#68L AS user_id#70L, (id#68L % 10) AS region_id#71L, cast(id#68L as string) AS user_name#72]
   +- *(1) Filter isnotnull((id#68L % 10))
      +- *(1) Range (0, 1000, step=1, splits=5)
】

org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec$ final result ==>【
Exchange hashpartitioning(region_id#71L, 8), ENSURE_REQUIREMENTS, [plan_id=82]
+- Project [id#68L AS user_id#70L, (id#68L % 10) AS region_id#71L, cast(id#68L as string) AS user_name#72]
   +- Filter isnotnull((id#68L % 10))
      +- Range (0, 1000, step=1, splits=5)
】AQE ==> 【
Exchange hashpartitioning(region_id#71L, 8), ENSURE_REQUIREMENTS, [plan_id=117]
+- *(1) Project [id#68L AS user_id#70L, (id#68L % 10) AS region_id#71L, cast(id#68L as string) AS user_name#72]
   +- *(1) Filter isnotnull((id#68L % 10))
      +- *(1) Range (0, 1000, step=1, splits=5)
】

org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec$ - applyPhysicalRules - org.apache.spark.sql.execution.CollapseCodegenStages -->【
Exchange hashpartitioning(region_id#78L, 8), ENSURE_REQUIREMENTS, [plan_id=83]
+- Project [id#76L AS region_id#78L, cast(id#76L as string) AS region_name#79]
   +- Range (0, 10, step=1, splits=1)
】AQE --> 【
Exchange hashpartitioning(region_id#78L, 8), ENSURE_REQUIREMENTS, [plan_id=124]
+- *(2) Project [id#76L AS region_id#78L, cast(id#76L as string) AS region_name#79]
   +- *(2) Range (0, 10, step=1, splits=1)
】

org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec$ final result ==>【
Exchange hashpartitioning(region_id#78L, 8), ENSURE_REQUIREMENTS, [plan_id=83]
+- Project [id#76L AS region_id#78L, cast(id#76L as string) AS region_name#79]
   +- Range (0, 10, step=1, splits=1)
】AQE ==> 【
Exchange hashpartitioning(region_id#78L, 8), ENSURE_REQUIREMENTS, [plan_id=124]
+- *(2) Project [id#76L AS region_id#78L, cast(id#76L as string) AS region_name#79]
   +- *(2) Range (0, 10, step=1, splits=1)
】

org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec$ - applyPhysicalRules - org.apache.spark.sql.execution.CollapseCodegenStages -->【
Exchange hashpartitioning(user_id#61L, region_id#62L, region_id#62L, 8), ENSURE_REQUIREMENTS, [plan_id=89]
+- Project [id#58L AS order_id#60L, (id#58L % 1000) AS user_id#61L, (id#58L % 10) AS region_id#62L, cast(id#58L as string) AS order_num#63]
   +- Filter (isnotnull((id#58L % 1000)) AND isnotnull((id#58L % 10)))
      +- Range (0, 10000, step=1, splits=10)
】AQE --> 【
Exchange hashpartitioning(user_id#61L, region_id#62L, region_id#62L, 8), ENSURE_REQUIREMENTS, [plan_id=141]
+- *(3) Project [id#58L AS order_id#60L, (id#58L % 1000) AS user_id#61L, (id#58L % 10) AS region_id#62L, cast(id#58L as string) AS order_num#63]
   +- *(3) Filter (isnotnull((id#58L % 1000)) AND isnotnull((id#58L % 10)))
      +- *(3) Range (0, 10000, step=1, splits=10)
】

org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec$ final result ==>【
Exchange hashpartitioning(user_id#61L, region_id#62L, region_id#62L, 8), ENSURE_REQUIREMENTS, [plan_id=89]
+- Project [id#58L AS order_id#60L, (id#58L % 1000) AS user_id#61L, (id#58L % 10) AS region_id#62L, cast(id#58L as string) AS order_num#63]
   +- Filter (isnotnull((id#58L % 1000)) AND isnotnull((id#58L % 10)))
      +- Range (0, 10000, step=1, splits=10)
】AQE ==> 【
Exchange hashpartitioning(user_id#61L, region_id#62L, region_id#62L, 8), ENSURE_REQUIREMENTS, [plan_id=141]
+- *(3) Project [id#58L AS order_id#60L, (id#58L % 1000) AS user_id#61L, (id#58L % 10) AS region_id#62L, cast(id#58L as string) AS order_num#63]
   +- *(3) Filter (isnotnull((id#58L % 1000)) AND isnotnull((id#58L % 10)))
      +- *(3) Range (0, 10000, step=1, splits=10)
】

21:32:53.406 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec - Materialize query stage ShuffleQueryStageExec: 0, query stage plan:
Exchange hashpartitioning(region_id#71L, 8), ENSURE_REQUIREMENTS, [plan_id=117]
+- *(1) Project [id#68L AS user_id#70L, (id#68L % 10) AS region_id#71L, cast(id#68L as string) AS user_name#72]
   +- *(1) Filter isnotnull((id#68L % 10))
      +- *(1) Range (0, 1000, step=1, splits=5)

21:32:53.790 [ScalaTest-run-running-PhysicalPlanSpec] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 246.052522 ms
21:32:53.823 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.util.ClosureCleaner - Cleaning indylambda closure: $anonfun$inputRDDs$1$adapted
21:32:53.837 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.util.ClosureCleaner -  +++ indylambda closure ($anonfun$inputRDDs$1$adapted) is now cleaned +++
21:32:53.849 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.util.ClosureCleaner - Cleaning indylambda closure: $anonfun$doExecute$4$adapted
21:32:53.850 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.util.ClosureCleaner -  +++ indylambda closure ($anonfun$doExecute$4$adapted) is now cleaned +++
21:32:53.907 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.scheduler.DAGScheduler - eagerlyComputePartitionsForRddAndAncestors for RDD 3 took 0.000764 seconds
21:32:53.913 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - Merging stage rdd profiles: Set()
21:32:53.914 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec - Materialize query stage ShuffleQueryStageExec: 1, query stage plan:
Exchange hashpartitioning(region_id#78L, 8), ENSURE_REQUIREMENTS, [plan_id=124]
+- *(2) Project [id#76L AS region_id#78L, cast(id#76L as string) AS region_name#79]
   +- *(2) Range (0, 10, step=1, splits=1)

21:32:53.926 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Registering RDD 3 (saveAsTable at PhysicalPlanSpec.scala:338) as input to shuffle 0
21:32:53.933 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got map stage job 0 (saveAsTable at PhysicalPlanSpec.scala:338) with 5 output partitions
21:32:53.933 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ShuffleMapStage 0 (saveAsTable at PhysicalPlanSpec.scala:338)
21:32:53.934 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
21:32:53.935 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
21:32:53.937 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - submitStage(ShuffleMapStage 0 (name=saveAsTable at PhysicalPlanSpec.scala:338;jobs=0))
21:32:53.938 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - missing: List()
21:32:53.939 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at saveAsTable at PhysicalPlanSpec.scala:338), which has no missing parents
21:32:53.939 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - submitMissingTasks(ShuffleMapStage 0)
21:32:53.940 [ScalaTest-run-running-PhysicalPlanSpec] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 22.849695 ms
21:32:53.941 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.util.ClosureCleaner - Cleaning indylambda closure: $anonfun$inputRDDs$1$adapted
21:32:53.943 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.util.ClosureCleaner -  +++ indylambda closure ($anonfun$inputRDDs$1$adapted) is now cleaned +++
21:32:53.944 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.util.ClosureCleaner - Cleaning indylambda closure: $anonfun$doExecute$4$adapted
21:32:53.945 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.util.ClosureCleaner -  +++ indylambda closure ($anonfun$doExecute$4$adapted) is now cleaned +++
21:32:53.948 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.scheduler.DAGScheduler - eagerlyComputePartitionsForRddAndAncestors for RDD 7 took 0.000073 seconds
21:32:53.949 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec - Materialize query stage ShuffleQueryStageExec: 2, query stage plan:
Exchange hashpartitioning(user_id#61L, region_id#62L, region_id#62L, 8), ENSURE_REQUIREMENTS, [plan_id=141]
+- *(3) Project [id#58L AS order_id#60L, (id#58L % 1000) AS user_id#61L, (id#58L % 10) AS region_id#62L, cast(id#58L as string) AS order_num#63]
   +- *(3) Filter (isnotnull((id#58L % 1000)) AND isnotnull((id#58L % 10)))
      +- *(3) Range (0, 10000, step=1, splits=10)

21:32:53.980 [ScalaTest-run-running-PhysicalPlanSpec] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 25.168933 ms
21:32:53.981 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.util.ClosureCleaner - Cleaning indylambda closure: $anonfun$inputRDDs$1$adapted
21:32:53.982 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.util.ClosureCleaner -  +++ indylambda closure ($anonfun$inputRDDs$1$adapted) is now cleaned +++
21:32:53.983 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.util.ClosureCleaner - Cleaning indylambda closure: $anonfun$doExecute$4$adapted
21:32:53.984 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.util.ClosureCleaner -  +++ indylambda closure ($anonfun$doExecute$4$adapted) is now cleaned +++
21:32:53.987 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.scheduler.DAGScheduler - eagerlyComputePartitionsForRddAndAncestors for RDD 11 took 0.000047 seconds
21:32:54.077 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 14.9 KiB, free 4.1 GiB)
21:32:54.079 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager - Put block broadcast_0 locally took 20 ms
21:32:54.081 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager - Putting block broadcast_0 without replication took 21 ms
21:32:54.403 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 7.2 KiB, free 4.1 GiB)
21:32:54.404 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_0_piece0 for BlockManagerId(driver, 192.168.31.69, 63737, None)
21:32:54.405 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on 192.168.31.69:63737 (size: 7.2 KiB, free: 4.1 GiB)
21:32:54.408 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_0_piece0
21:32:54.408 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager - Told master about block broadcast_0_piece0
21:32:54.409 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager - Put block broadcast_0_piece0 locally took 7 ms
21:32:54.409 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager - Putting block broadcast_0_piece0 without replication took 7 ms
21:32:54.409 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 0 from broadcast at DAGScheduler.scala:1513
21:32:54.423 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 5 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at saveAsTable at PhysicalPlanSpec.scala:338) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4))
21:32:54.424 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 5 tasks resource profile 0
21:32:54.448 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.TaskSetManager - Epoch for TaskSet 0.0: 0
21:32:54.451 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.TaskSetManager - Adding pending tasks took 2 ms
21:32:54.452 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.TaskSetManager - Valid locality levels for TaskSet 0.0: NO_PREF, ANY
21:32:54.457 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - Merging stage rdd profiles: Set()
21:32:54.457 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Registering RDD 7 (saveAsTable at PhysicalPlanSpec.scala:338) as input to shuffle 1
21:32:54.457 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got map stage job 1 (saveAsTable at PhysicalPlanSpec.scala:338) with 1 output partitions
21:32:54.457 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ShuffleMapStage 1 (saveAsTable at PhysicalPlanSpec.scala:338)
21:32:54.457 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
21:32:54.457 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
21:32:54.457 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - submitStage(ShuffleMapStage 1 (name=saveAsTable at PhysicalPlanSpec.scala:338;jobs=1))
21:32:54.458 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - missing: List()
21:32:54.458 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 1 (MapPartitionsRDD[7] at saveAsTable at PhysicalPlanSpec.scala:338), which has no missing parents
21:32:54.458 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - submitMissingTasks(ShuffleMapStage 1)
21:32:54.462 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 14.1 KiB, free 4.1 GiB)
21:32:54.462 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager - Put block broadcast_1 locally took 1 ms
21:32:54.462 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager - Putting block broadcast_1 without replication took 1 ms
21:32:54.464 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.8 KiB, free 4.1 GiB)
21:32:54.464 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_1_piece0 for BlockManagerId(driver, 192.168.31.69, 63737, None)
21:32:54.464 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on 192.168.31.69:63737 (size: 6.8 KiB, free: 4.1 GiB)
21:32:54.464 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_1_piece0
21:32:54.465 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager - Told master about block broadcast_1_piece0
21:32:54.465 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager - Put block broadcast_1_piece0 locally took 1 ms
21:32:54.465 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager - Putting block broadcast_1_piece0 without replication took 1 ms
21:32:54.465 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1513
21:32:54.466 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[7] at saveAsTable at PhysicalPlanSpec.scala:338) (first 15 tasks are for partitions Vector(0))
21:32:54.466 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks resource profile 0
21:32:54.468 [dispatcher-event-loop-0] DEBUG org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0.0, runningTasks: 0
21:32:54.468 [dispatcher-event-loop-0] DEBUG org.apache.spark.scheduler.TaskSetManager - Valid locality levels for TaskSet 0.0: NO_PREF, ANY
21:32:54.493 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0) (192.168.31.69, executor driver, partition 0, PROCESS_LOCAL, 4567 bytes) taskResourceAssignments Map()
21:32:54.497 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.TaskSetManager - Epoch for TaskSet 1.0: 0
21:32:54.497 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.TaskSetManager - Adding pending tasks took 0 ms
21:32:54.497 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.TaskSetManager - Valid locality levels for TaskSet 1.0: NO_PREF, ANY
21:32:54.498 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - Merging stage rdd profiles: Set()
21:32:54.498 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Registering RDD 11 (saveAsTable at PhysicalPlanSpec.scala:338) as input to shuffle 2
21:32:54.498 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got map stage job 2 (saveAsTable at PhysicalPlanSpec.scala:338) with 10 output partitions
21:32:54.498 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ShuffleMapStage 2 (saveAsTable at PhysicalPlanSpec.scala:338)
21:32:54.498 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
21:32:54.498 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
21:32:54.499 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - submitStage(ShuffleMapStage 2 (name=saveAsTable at PhysicalPlanSpec.scala:338;jobs=2))
21:32:54.499 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - missing: List()
21:32:54.499 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 2 (MapPartitionsRDD[11] at saveAsTable at PhysicalPlanSpec.scala:338), which has no missing parents
21:32:54.499 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - submitMissingTasks(ShuffleMapStage 2)
21:32:54.505 [dispatcher-event-loop-0] DEBUG org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0.0, runningTasks: 1
21:32:54.505 [dispatcher-event-loop-0] DEBUG org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_1.0, runningTasks: 0
21:32:54.505 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 15.7 KiB, free 4.1 GiB)
21:32:54.506 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager - Put block broadcast_2 locally took 1 ms
21:32:54.506 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager - Putting block broadcast_2 without replication took 1 ms
21:32:54.507 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 7.4 KiB, free 4.1 GiB)
21:32:54.507 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_2_piece0 for BlockManagerId(driver, 192.168.31.69, 63737, None)
21:32:54.508 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on 192.168.31.69:63737 (size: 7.4 KiB, free: 4.1 GiB)
21:32:54.508 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_2_piece0
21:32:54.508 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager - Told master about block broadcast_2_piece0
21:32:54.508 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager - Put block broadcast_2_piece0 locally took 1 ms
21:32:54.508 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager - Putting block broadcast_2_piece0 without replication took 1 ms
21:32:54.508 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1513
21:32:54.508 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 10 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[11] at saveAsTable at PhysicalPlanSpec.scala:338) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9))
21:32:54.509 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 2.0 with 10 tasks resource profile 0
21:32:54.509 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.TaskSetManager - Epoch for TaskSet 2.0: 0
21:32:54.509 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.TaskSetManager - Adding pending tasks took 0 ms
21:32:54.509 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.TaskSetManager - Valid locality levels for TaskSet 2.0: NO_PREF, ANY
21:32:54.509 [dispatcher-event-loop-1] DEBUG org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0.0, runningTasks: 1
21:32:54.509 [dispatcher-event-loop-1] DEBUG org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_1.0, runningTasks: 0
21:32:54.509 [dispatcher-event-loop-1] DEBUG org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_2.0, runningTasks: 0
21:32:54.511 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
21:32:54.519 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] DEBUG org.apache.spark.executor.ExecutorMetricsPoller - stageTCMP: (0, 0) -> 1
21:32:54.540 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] DEBUG org.apache.spark.storage.BlockManager - Getting local block broadcast_0
21:32:54.542 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] DEBUG org.apache.spark.storage.BlockManager - Level for block broadcast_0 is StorageLevel(disk, memory, deserialized, 1 replicas)
21:32:54.771 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 16.699419 ms
21:32:54.822 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] DEBUG org.apache.spark.shuffle.sort.io.LocalDiskShuffleMapOutputWriter - Writing shuffle index file for mapId 0 with length 8
21:32:54.827 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] DEBUG org.apache.spark.shuffle.IndexShuffleBlockResolver - Shuffle index for mapId 0: [340,335,0,652,888,897,0,0]
21:32:54.844 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 1847 bytes result sent to driver
21:32:54.845 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] DEBUG org.apache.spark.executor.ExecutorMetricsPoller - stageTCMP: (0, 0) -> 0
21:32:54.847 [dispatcher-event-loop-1] DEBUG org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0.0, runningTasks: 0
21:32:54.847 [dispatcher-event-loop-1] DEBUG org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_1.0, runningTasks: 0
21:32:54.847 [dispatcher-event-loop-1] DEBUG org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_2.0, runningTasks: 0
21:32:54.847 [dispatcher-event-loop-1] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 0.0 (TID 1) (192.168.31.69, executor driver, partition 1, PROCESS_LOCAL, 4567 bytes) taskResourceAssignments Map()
21:32:54.848 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1)] INFO  org.apache.spark.executor.Executor - Running task 1.0 in stage 0.0 (TID 1)
21:32:54.848 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1)] DEBUG org.apache.spark.executor.ExecutorMetricsPoller - stageTCMP: (0, 0) -> 1
21:32:54.851 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 378 ms on 192.168.31.69 (executor driver) (1/5)
21:32:54.861 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1)] DEBUG org.apache.spark.shuffle.sort.io.LocalDiskShuffleMapOutputWriter - Writing shuffle index file for mapId 1 with length 8
21:32:54.862 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - ShuffleMapTask finished on driver
21:32:54.862 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1)] DEBUG org.apache.spark.shuffle.IndexShuffleBlockResolver - Shuffle index for mapId 1: [256,256,0,681,746,841,0,0]
21:32:54.863 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1)] INFO  org.apache.spark.executor.Executor - Finished task 1.0 in stage 0.0 (TID 1). 1804 bytes result sent to driver
21:32:54.864 [Executor task launch worker for task 1.0 in stage 0.0 (TID 1)] DEBUG org.apache.spark.executor.ExecutorMetricsPoller - stageTCMP: (0, 0) -> 0
21:32:54.864 [dispatcher-event-loop-1] DEBUG org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0.0, runningTasks: 0
21:32:54.864 [dispatcher-event-loop-1] DEBUG org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_1.0, runningTasks: 0
21:32:54.864 [dispatcher-event-loop-1] DEBUG org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_2.0, runningTasks: 0
21:32:54.864 [dispatcher-event-loop-1] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 0.0 (TID 2) (192.168.31.69, executor driver, partition 2, PROCESS_LOCAL, 4567 bytes) taskResourceAssignments Map()
21:32:54.865 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 0.0 (TID 1) in 18 ms on 192.168.31.69 (executor driver) (2/5)
21:32:54.865 [Executor task launch worker for task 2.0 in stage 0.0 (TID 2)] INFO  org.apache.spark.executor.Executor - Running task 2.0 in stage 0.0 (TID 2)
21:32:54.866 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - ShuffleMapTask finished on driver
21:32:54.866 [Executor task launch worker for task 2.0 in stage 0.0 (TID 2)] DEBUG org.apache.spark.executor.ExecutorMetricsPoller - stageTCMP: (0, 0) -> 1
21:32:54.879 [Executor task launch worker for task 2.0 in stage 0.0 (TID 2)] DEBUG org.apache.spark.shuffle.sort.io.LocalDiskShuffleMapOutputWriter - Writing shuffle index file for mapId 2 with length 8
21:32:54.880 [Executor task launch worker for task 2.0 in stage 0.0 (TID 2)] DEBUG org.apache.spark.shuffle.IndexShuffleBlockResolver - Shuffle index for mapId 2: [298,257,0,686,748,832,0,0]
21:32:54.881 [Executor task launch worker for task 2.0 in stage 0.0 (TID 2)] INFO  org.apache.spark.executor.Executor - Finished task 2.0 in stage 0.0 (TID 2). 1804 bytes result sent to driver
21:32:54.881 [Executor task launch worker for task 2.0 in stage 0.0 (TID 2)] DEBUG org.apache.spark.executor.ExecutorMetricsPoller - stageTCMP: (0, 0) -> 0
21:32:54.882 [dispatcher-event-loop-1] DEBUG org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0.0, runningTasks: 0
21:32:54.882 [dispatcher-event-loop-1] DEBUG org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_1.0, runningTasks: 0
21:32:54.882 [dispatcher-event-loop-1] DEBUG org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_2.0, runningTasks: 0
21:32:54.882 [dispatcher-event-loop-1] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 0.0 (TID 3) (192.168.31.69, executor driver, partition 3, PROCESS_LOCAL, 4567 bytes) taskResourceAssignments Map()
21:32:54.882 [Executor task launch worker for task 3.0 in stage 0.0 (TID 3)] INFO  org.apache.spark.executor.Executor - Running task 3.0 in stage 0.0 (TID 3)
21:32:54.882 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 0.0 (TID 2) in 18 ms on 192.168.31.69 (executor driver) (3/5)
21:32:54.883 [Executor task launch worker for task 3.0 in stage 0.0 (TID 3)] DEBUG org.apache.spark.executor.ExecutorMetricsPoller - stageTCMP: (0, 0) -> 1
21:32:54.883 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - ShuffleMapTask finished on driver
21:32:54.893 [Executor task launch worker for task 3.0 in stage 0.0 (TID 3)] DEBUG org.apache.spark.shuffle.sort.io.LocalDiskShuffleMapOutputWriter - Writing shuffle index file for mapId 3 with length 8
21:32:54.894 [Executor task launch worker for task 3.0 in stage 0.0 (TID 3)] DEBUG org.apache.spark.shuffle.IndexShuffleBlockResolver - Shuffle index for mapId 3: [335,257,0,678,752,815,0,0]
21:32:54.895 [Executor task launch worker for task 3.0 in stage 0.0 (TID 3)] INFO  org.apache.spark.executor.Executor - Finished task 3.0 in stage 0.0 (TID 3). 1804 bytes result sent to driver
21:32:54.895 [Executor task launch worker for task 3.0 in stage 0.0 (TID 3)] DEBUG org.apache.spark.executor.ExecutorMetricsPoller - stageTCMP: (0, 0) -> 0
21:32:54.911 [dispatcher-event-loop-1] DEBUG org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0.0, runningTasks: 0
21:32:54.911 [dispatcher-event-loop-1] DEBUG org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_1.0, runningTasks: 0
21:32:54.912 [dispatcher-event-loop-1] DEBUG org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_2.0, runningTasks: 0
21:32:54.912 [dispatcher-event-loop-1] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 0.0 (TID 4) (192.168.31.69, executor driver, partition 4, PROCESS_LOCAL, 4567 bytes) taskResourceAssignments Map()
21:32:54.913 [Executor task launch worker for task 4.0 in stage 0.0 (TID 4)] INFO  org.apache.spark.executor.Executor - Running task 4.0 in stage 0.0 (TID 4)
21:32:54.936 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 0.0 (TID 3) in 54 ms on 192.168.31.69 (executor driver) (4/5)
21:32:54.937 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - ShuffleMapTask finished on driver
21:32:54.987 [Executor task launch worker for task 4.0 in stage 0.0 (TID 4)] DEBUG org.apache.spark.executor.ExecutorMetricsPoller - stageTCMP: (0, 0) -> 1
21:32:54.995 [Executor task launch worker for task 4.0 in stage 0.0 (TID 4)] DEBUG org.apache.spark.shuffle.sort.io.LocalDiskShuffleMapOutputWriter - Writing shuffle index file for mapId 4 with length 8
21:32:54.997 [Executor task launch worker for task 4.0 in stage 0.0 (TID 4)] DEBUG org.apache.spark.shuffle.IndexShuffleBlockResolver - Shuffle index for mapId 4: [255,255,0,582,756,883,0,0]
21:32:54.997 [Executor task launch worker for task 4.0 in stage 0.0 (TID 4)] INFO  org.apache.spark.executor.Executor - Finished task 4.0 in stage 0.0 (TID 4). 1804 bytes result sent to driver
21:32:54.997 [Executor task launch worker for task 4.0 in stage 0.0 (TID 4)] DEBUG org.apache.spark.executor.ExecutorMetricsPoller - stageTCMP: (0, 0) -> 0
21:32:54.998 [dispatcher-event-loop-1] DEBUG org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0.0, runningTasks: 0
21:32:54.998 [dispatcher-event-loop-1] DEBUG org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_1.0, runningTasks: 0
21:32:54.998 [dispatcher-event-loop-1] DEBUG org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_2.0, runningTasks: 0
21:32:54.998 [dispatcher-event-loop-1] DEBUG org.apache.spark.scheduler.TaskSetManager - No tasks for locality level NO_PREF, so moving to locality level ANY
21:32:55.000 [dispatcher-event-loop-1] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 5) (192.168.31.69, executor driver, partition 0, PROCESS_LOCAL, 4567 bytes) taskResourceAssignments Map()
21:32:55.000 [Executor task launch worker for task 0.0 in stage 1.0 (TID 5)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 5)
21:32:55.000 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 0.0 (TID 4) in 88 ms on 192.168.31.69 (executor driver) (5/5)
21:32:55.001 [Executor task launch worker for task 0.0 in stage 1.0 (TID 5)] DEBUG org.apache.spark.executor.ExecutorMetricsPoller - stageTCMP: (1, 0) -> 1
21:32:55.001 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - ShuffleMapTask finished on driver
21:32:55.001 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool
21:32:55.002 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 0 (saveAsTable at PhysicalPlanSpec.scala:338) finished in 1.047 s
21:32:55.002 [Executor task launch worker for task 0.0 in stage 1.0 (TID 5)] DEBUG org.apache.spark.storage.BlockManager - Getting local block broadcast_1
21:32:55.002 [Executor task launch worker for task 0.0 in stage 1.0 (TID 5)] DEBUG org.apache.spark.storage.BlockManager - Level for block broadcast_1 is StorageLevel(disk, memory, deserialized, 1 replicas)
21:32:55.002 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
21:32:55.003 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - running: Set(ShuffleMapStage 1, ShuffleMapStage 2)
21:32:55.003 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - waiting: Set()
21:32:55.003 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - failed: Set()
21:32:55.005 [dag-scheduler-event-loop] DEBUG org.apache.spark.MapOutputTrackerMaster - Increasing epoch to 1
21:32:55.015 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - After removal of stage 0, remaining stages = 2
21:32:55.027 [Executor task launch worker for task 0.0 in stage 1.0 (TID 5)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 18.129594 ms
21:32:55.030 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - Considering join on: Some((((user_id#61L = user_id#70L) AND (region_id#62L = region_id#71L)) AND (region_id#62L = region_id#78L)))
21:32:55.031 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - leftKeys:List(user_id#70L, region_id#71L, region_id#78L) | rightKeys:List(user_id#61L, region_id#62L, region_id#62L)
21:32:55.033 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - Considering join on: Some((region_id#71L = region_id#78L))
21:32:55.033 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - leftKeys:List(region_id#71L) | rightKeys:List(region_id#78L)
21:32:55.033 [Executor task launch worker for task 0.0 in stage 1.0 (TID 5)] DEBUG org.apache.spark.shuffle.sort.io.LocalDiskShuffleMapOutputWriter - Writing shuffle index file for mapId 5 with length 8
21:32:55.034 [Executor task launch worker for task 0.0 in stage 1.0 (TID 5)] DEBUG org.apache.spark.shuffle.IndexShuffleBlockResolver - Shuffle index for mapId 5: [72,72,0,85,102,103,0,0]
SparkPlanner(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2@321bf4b1) - org.apache.spark.sql.execution.SparkStrategies$SpecialLimits$ -->【
ReturnAnswer
+- Project [order_id#60L, user_id#61L, region_id#62L, order_num#63, user_name#72, region_name#79]
   +- Join Inner, (((user_id#61L = user_id#70L) AND (region_id#62L = region_id#71L)) AND (region_id#62L = region_id#78L))
      :- Join Inner, (region_id#71L = region_id#78L)
      :  :- LogicalQueryStage Project [id#68L AS user_id#70L, (id#68L % 10) AS region_id#71L, cast(id#68L as string) AS user_name#72], ShuffleQueryStage 0
      :  +- LogicalQueryStage Project [id#76L AS region_id#78L, cast(id#76L as string) AS region_name#79], ShuffleQueryStage 1
      +- LogicalQueryStage Project [id#58L AS order_id#60L, (id#58L % 1000) AS user_id#61L, (id#58L % 10) AS region_id#62L, cast(id#58L as string) AS order_num#63], ShuffleQueryStage 2
】 --> 【
PlanLater Project [order_id#60L, user_id#61L, region_id#62L, order_num#63, user_name#72, region_name#79]
】

21:32:55.035 [Executor task launch worker for task 0.0 in stage 1.0 (TID 5)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 5). 1748 bytes result sent to driver
21:32:55.035 [Executor task launch worker for task 0.0 in stage 1.0 (TID 5)] DEBUG org.apache.spark.executor.ExecutorMetricsPoller - stageTCMP: (1, 0) -> 0
21:32:55.035 [dispatcher-event-loop-1] DEBUG org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_1.0, runningTasks: 0
21:32:55.035 [dispatcher-event-loop-1] DEBUG org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_2.0, runningTasks: 0
21:32:55.036 [dispatcher-event-loop-1] DEBUG org.apache.spark.scheduler.TaskSetManager - No tasks for locality level NO_PREF, so moving to locality level ANY
21:32:55.036 [dispatcher-event-loop-1] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 2.0 (TID 6) (192.168.31.69, executor driver, partition 0, PROCESS_LOCAL, 4567 bytes) taskResourceAssignments Map()
SparkPlanner(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2@321bf4b1) - org.apache.spark.sql.execution.SparkStrategies$BasicOperators$ -->【
Project [order_id#60L, user_id#61L, region_id#62L, order_num#63, user_name#72, region_name#79]
+- Join Inner, (((user_id#61L = user_id#70L) AND (region_id#62L = region_id#71L)) AND (region_id#62L = region_id#78L))
   :- Join Inner, (region_id#71L = region_id#78L)
   :  :- LogicalQueryStage Project [id#68L AS user_id#70L, (id#68L % 10) AS region_id#71L, cast(id#68L as string) AS user_name#72], ShuffleQueryStage 0
   :  +- LogicalQueryStage Project [id#76L AS region_id#78L, cast(id#76L as string) AS region_name#79], ShuffleQueryStage 1
   +- LogicalQueryStage Project [id#58L AS order_id#60L, (id#58L % 1000) AS user_id#61L, (id#58L % 10) AS region_id#62L, cast(id#58L as string) AS order_num#63], ShuffleQueryStage 2
】 --> 【
Project [order_id#60L, user_id#61L, region_id#62L, order_num#63, user_name#72, region_name#79]
+- PlanLater Join Inner, (((user_id#61L = user_id#70L) AND (region_id#62L = region_id#71L)) AND (region_id#62L = region_id#78L))
】

21:32:55.037 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 5) in 37 ms on 192.168.31.69 (executor driver) (1/1)
21:32:55.037 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - Considering join on: Some((((user_id#61L = user_id#70L) AND (region_id#62L = region_id#71L)) AND (region_id#62L = region_id#78L)))
21:32:55.037 [Executor task launch worker for task 0.0 in stage 2.0 (TID 6)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 2.0 (TID 6)
21:32:55.037 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool
21:32:55.037 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - leftKeys:List(user_id#70L, region_id#71L, region_id#78L) | rightKeys:List(user_id#61L, region_id#62L, region_id#62L)
21:32:55.038 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - ShuffleMapTask finished on driver
21:32:55.038 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - Considering join on: Some((((user_id#61L = user_id#70L) AND (region_id#62L = region_id#71L)) AND (region_id#62L = region_id#78L)))
21:32:55.038 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - leftKeys:List(user_id#70L, region_id#71L, region_id#78L) | rightKeys:List(user_id#61L, region_id#62L, region_id#62L)
21:32:55.038 [Executor task launch worker for task 0.0 in stage 2.0 (TID 6)] DEBUG org.apache.spark.executor.ExecutorMetricsPoller - stageTCMP: (2, 0) -> 1
21:32:55.038 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 1 (saveAsTable at PhysicalPlanSpec.scala:338) finished in 0.580 s
21:32:55.038 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
21:32:55.038 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - running: Set(ShuffleMapStage 2)
21:32:55.038 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - waiting: Set()
21:32:55.038 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - failed: Set()
21:32:55.038 [dag-scheduler-event-loop] DEBUG org.apache.spark.MapOutputTrackerMaster - Increasing epoch to 2
21:32:55.039 [Executor task launch worker for task 0.0 in stage 2.0 (TID 6)] DEBUG org.apache.spark.storage.BlockManager - Getting local block broadcast_2
21:32:55.039 [Executor task launch worker for task 0.0 in stage 2.0 (TID 6)] DEBUG org.apache.spark.storage.BlockManager - Level for block broadcast_2 is StorageLevel(disk, memory, deserialized, 1 replicas)
21:32:55.039 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - After removal of stage 1, remaining stages = 1
21:32:55.043 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.execution.adaptive.LogicalQueryStage - Physical stats available as Statistics(sizeInBytes=39.1 KiB, rowCount=1.00E+3) for plan: ShuffleQueryStage 0
+- Exchange hashpartitioning(region_id#71L, 8), ENSURE_REQUIREMENTS, [plan_id=117]
   +- *(1) Project [id#68L AS user_id#70L, (id#68L % 10) AS region_id#71L, cast(id#68L as string) AS user_name#72]
      +- *(1) Filter isnotnull((id#68L % 10))
         +- *(1) Range (0, 1000, step=1, splits=5)

21:32:55.044 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.execution.adaptive.LogicalQueryStage - Physical stats not available for plan: ShuffleQueryStage 1
+- Exchange hashpartitioning(region_id#78L, 8), ENSURE_REQUIREMENTS, [plan_id=124]
   +- *(2) Project [id#76L AS region_id#78L, cast(id#76L as string) AS region_name#79]
      +- *(2) Range (0, 10, step=1, splits=1)

21:32:55.044 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - Considering join on: Some((region_id#71L = region_id#78L))
21:32:55.044 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - leftKeys:List(region_id#71L) | rightKeys:List(region_id#78L)
21:32:55.045 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.execution.adaptive.LogicalQueryStage - Physical stats not available for plan: ShuffleQueryStage 2
+- Exchange hashpartitioning(user_id#61L, region_id#62L, region_id#62L, 8), ENSURE_REQUIREMENTS, [plan_id=141]
   +- *(3) Project [id#58L AS order_id#60L, (id#58L % 1000) AS user_id#61L, (id#58L % 10) AS region_id#62L, cast(id#58L as string) AS order_num#63]
      +- *(3) Filter (isnotnull((id#58L % 1000)) AND isnotnull((id#58L % 10)))
         +- *(3) Range (0, 10000, step=1, splits=10)

SparkPlanner(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2@321bf4b1) - org.apache.spark.sql.execution.SparkStrategies$JoinSelection$ -->【
Join Inner, (((user_id#61L = user_id#70L) AND (region_id#62L = region_id#71L)) AND (region_id#62L = region_id#78L))
:- Join Inner, (region_id#71L = region_id#78L)
:  :- LogicalQueryStage Project [id#68L AS user_id#70L, (id#68L % 10) AS region_id#71L, cast(id#68L as string) AS user_name#72], ShuffleQueryStage 0
:  +- LogicalQueryStage Project [id#76L AS region_id#78L, cast(id#76L as string) AS region_name#79], ShuffleQueryStage 1
+- LogicalQueryStage Project [id#58L AS order_id#60L, (id#58L % 1000) AS user_id#61L, (id#58L % 10) AS region_id#62L, cast(id#58L as string) AS order_num#63], ShuffleQueryStage 2
】 --> 【
SortMergeJoin [user_id#70L, region_id#71L, region_id#78L], [user_id#61L, region_id#62L, region_id#62L], Inner
:- PlanLater Join Inner, (region_id#71L = region_id#78L)
+- PlanLater LogicalQueryStage Project [id#58L AS order_id#60L, (id#58L % 1000) AS user_id#61L, (id#58L % 10) AS region_id#62L, cast(id#58L as string) AS order_num#63], ShuffleQueryStage 2
】

21:32:55.047 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - Considering join on: Some((region_id#71L = region_id#78L))
21:32:55.047 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - leftKeys:List(region_id#71L) | rightKeys:List(region_id#78L)
21:32:55.047 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - Considering join on: Some((region_id#71L = region_id#78L))
21:32:55.047 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - leftKeys:List(region_id#71L) | rightKeys:List(region_id#78L)
SparkPlanner(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2@321bf4b1) - org.apache.spark.sql.execution.SparkStrategies$JoinSelection$ -->【
Join Inner, (region_id#71L = region_id#78L)
:- LogicalQueryStage Project [id#68L AS user_id#70L, (id#68L % 10) AS region_id#71L, cast(id#68L as string) AS user_name#72], ShuffleQueryStage 0
+- LogicalQueryStage Project [id#76L AS region_id#78L, cast(id#76L as string) AS region_name#79], ShuffleQueryStage 1
】 --> 【
SortMergeJoin [region_id#71L], [region_id#78L], Inner
:- PlanLater LogicalQueryStage Project [id#68L AS user_id#70L, (id#68L % 10) AS region_id#71L, cast(id#68L as string) AS user_name#72], ShuffleQueryStage 0
+- PlanLater LogicalQueryStage Project [id#76L AS region_id#78L, cast(id#76L as string) AS region_name#79], ShuffleQueryStage 1
】

SparkPlanner(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2@321bf4b1) - org.apache.spark.sql.execution.adaptive.LogicalQueryStageStrategy$ -->【
LogicalQueryStage Project [id#68L AS user_id#70L, (id#68L % 10) AS region_id#71L, cast(id#68L as string) AS user_name#72], ShuffleQueryStage 0
】 --> 【
ShuffleQueryStage 0
+- Exchange hashpartitioning(region_id#71L, 8), ENSURE_REQUIREMENTS, [plan_id=117]
   +- *(1) Project [id#68L AS user_id#70L, (id#68L % 10) AS region_id#71L, cast(id#68L as string) AS user_name#72]
      +- *(1) Filter isnotnull((id#68L % 10))
         +- *(1) Range (0, 1000, step=1, splits=5)
】

SparkPlanner(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2@321bf4b1) - org.apache.spark.sql.execution.adaptive.LogicalQueryStageStrategy$ -->【
LogicalQueryStage Project [id#76L AS region_id#78L, cast(id#76L as string) AS region_name#79], ShuffleQueryStage 1
】 --> 【
ShuffleQueryStage 1
+- Exchange hashpartitioning(region_id#78L, 8), ENSURE_REQUIREMENTS, [plan_id=124]
   +- *(2) Project [id#76L AS region_id#78L, cast(id#76L as string) AS region_name#79]
      +- *(2) Range (0, 10, step=1, splits=1)
】

SparkPlanner(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2@321bf4b1) - org.apache.spark.sql.execution.adaptive.LogicalQueryStageStrategy$ -->【
LogicalQueryStage Project [id#58L AS order_id#60L, (id#58L % 1000) AS user_id#61L, (id#58L % 10) AS region_id#62L, cast(id#58L as string) AS order_num#63], ShuffleQueryStage 2
】 --> 【
ShuffleQueryStage 2
+- Exchange hashpartitioning(user_id#61L, region_id#62L, region_id#62L, 8), ENSURE_REQUIREMENTS, [plan_id=141]
   +- *(3) Project [id#58L AS order_id#60L, (id#58L % 1000) AS user_id#61L, (id#58L % 10) AS region_id#62L, cast(id#58L as string) AS order_num#63]
      +- *(3) Filter (isnotnull((id#58L % 1000)) AND isnotnull((id#58L % 10)))
         +- *(3) Range (0, 10000, step=1, splits=10)
】

org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec$ - applyPhysicalRules - org.apache.spark.sql.execution.exchange.EnsureRequirements -->【
Project [order_id#60L, user_id#61L, region_id#62L, order_num#63, user_name#72, region_name#79]
+- SortMergeJoin [user_id#70L, region_id#71L, region_id#78L], [user_id#61L, region_id#62L, region_id#62L], Inner
   :- SortMergeJoin [region_id#71L], [region_id#78L], Inner
   :  :- ShuffleQueryStage 0
   :  :  +- Exchange hashpartitioning(region_id#71L, 8), ENSURE_REQUIREMENTS, [plan_id=117]
   :  :     +- *(1) Project [id#68L AS user_id#70L, (id#68L % 10) AS region_id#71L, cast(id#68L as string) AS user_name#72]
   :  :        +- *(1) Filter isnotnull((id#68L % 10))
   :  :           +- *(1) Range (0, 1000, step=1, splits=5)
   :  +- ShuffleQueryStage 1
   :     +- Exchange hashpartitioning(region_id#78L, 8), ENSURE_REQUIREMENTS, [plan_id=124]
   :        +- *(2) Project [id#76L AS region_id#78L, cast(id#76L as string) AS region_name#79]
   :           +- *(2) Range (0, 10, step=1, splits=1)
   +- ShuffleQueryStage 2
      +- Exchange hashpartitioning(user_id#61L, region_id#62L, region_id#62L, 8), ENSURE_REQUIREMENTS, [plan_id=141]
         +- *(3) Project [id#58L AS order_id#60L, (id#58L % 1000) AS user_id#61L, (id#58L % 10) AS region_id#62L, cast(id#58L as string) AS order_num#63]
            +- *(3) Filter (isnotnull((id#58L % 1000)) AND isnotnull((id#58L % 10)))
               +- *(3) Range (0, 10000, step=1, splits=10)
】AQE --> 【
Project [order_id#60L, user_id#61L, region_id#62L, order_num#63, user_name#72, region_name#79]
+- SortMergeJoin [user_id#70L, region_id#71L, region_id#78L], [user_id#61L, region_id#62L, region_id#62L], Inner
   :- Sort [user_id#70L ASC NULLS FIRST, region_id#71L ASC NULLS FIRST, region_id#78L ASC NULLS FIRST], false, 0
   :  +- Exchange hashpartitioning(user_id#70L, region_id#71L, region_id#71L, 8), ENSURE_REQUIREMENTS, [plan_id=169]
   :     +- SortMergeJoin [region_id#71L], [region_id#78L], Inner
   :        :- Sort [region_id#71L ASC NULLS FIRST], false, 0
   :        :  +- ShuffleQueryStage 0
   :        :     +- Exchange hashpartitioning(region_id#71L, 8), ENSURE_REQUIREMENTS, [plan_id=117]
   :        :        +- *(1) Project [id#68L AS user_id#70L, (id#68L % 10) AS region_id#71L, cast(id#68L as string) AS user_name#72]
   :        :           +- *(1) Filter isnotnull((id#68L % 10))
   :        :              +- *(1) Range (0, 1000, step=1, splits=5)
   :        +- Sort [region_id#78L ASC NULLS FIRST], false, 0
   :           +- ShuffleQueryStage 1
   :              +- Exchange hashpartitioning(region_id#78L, 8), ENSURE_REQUIREMENTS, [plan_id=124]
   :                 +- *(2) Project [id#76L AS region_id#78L, cast(id#76L as string) AS region_name#79]
   :                    +- *(2) Range (0, 10, step=1, splits=1)
   +- Sort [user_id#61L ASC NULLS FIRST, region_id#62L ASC NULLS FIRST, region_id#62L ASC NULLS FIRST], false, 0
      +- ShuffleQueryStage 2
         +- Exchange hashpartitioning(user_id#61L, region_id#62L, region_id#62L, 8), ENSURE_REQUIREMENTS, [plan_id=141]
            +- *(3) Project [id#58L AS order_id#60L, (id#58L % 1000) AS user_id#61L, (id#58L % 10) AS region_id#62L, cast(id#58L as string) AS order_num#63]
               +- *(3) Filter (isnotnull((id#58L % 1000)) AND isnotnull((id#58L % 10)))
                  +- *(3) Range (0, 10000, step=1, splits=10)
】

org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec$ final result ==>【
Project [order_id#60L, user_id#61L, region_id#62L, order_num#63, user_name#72, region_name#79]
+- SortMergeJoin [user_id#70L, region_id#71L, region_id#78L], [user_id#61L, region_id#62L, region_id#62L], Inner
   :- SortMergeJoin [region_id#71L], [region_id#78L], Inner
   :  :- ShuffleQueryStage 0
   :  :  +- Exchange hashpartitioning(region_id#71L, 8), ENSURE_REQUIREMENTS, [plan_id=117]
   :  :     +- *(1) Project [id#68L AS user_id#70L, (id#68L % 10) AS region_id#71L, cast(id#68L as string) AS user_name#72]
   :  :        +- *(1) Filter isnotnull((id#68L % 10))
   :  :           +- *(1) Range (0, 1000, step=1, splits=5)
   :  +- ShuffleQueryStage 1
   :     +- Exchange hashpartitioning(region_id#78L, 8), ENSURE_REQUIREMENTS, [plan_id=124]
   :        +- *(2) Project [id#76L AS region_id#78L, cast(id#76L as string) AS region_name#79]
   :           +- *(2) Range (0, 10, step=1, splits=1)
   +- ShuffleQueryStage 2
      +- Exchange hashpartitioning(user_id#61L, region_id#62L, region_id#62L, 8), ENSURE_REQUIREMENTS, [plan_id=141]
         +- *(3) Project [id#58L AS order_id#60L, (id#58L % 1000) AS user_id#61L, (id#58L % 10) AS region_id#62L, cast(id#58L as string) AS order_num#63]
            +- *(3) Filter (isnotnull((id#58L % 1000)) AND isnotnull((id#58L % 10)))
               +- *(3) Range (0, 10000, step=1, splits=10)
】AQE ==> 【
Project [order_id#60L, user_id#61L, region_id#62L, order_num#63, user_name#72, region_name#79]
+- SortMergeJoin [user_id#70L, region_id#71L, region_id#78L], [user_id#61L, region_id#62L, region_id#62L], Inner
   :- Sort [user_id#70L ASC NULLS FIRST, region_id#71L ASC NULLS FIRST, region_id#78L ASC NULLS FIRST], false, 0
   :  +- Exchange hashpartitioning(user_id#70L, region_id#71L, region_id#71L, 8), ENSURE_REQUIREMENTS, [plan_id=169]
   :     +- SortMergeJoin [region_id#71L], [region_id#78L], Inner
   :        :- Sort [region_id#71L ASC NULLS FIRST], false, 0
   :        :  +- ShuffleQueryStage 0
   :        :     +- Exchange hashpartitioning(region_id#71L, 8), ENSURE_REQUIREMENTS, [plan_id=117]
   :        :        +- *(1) Project [id#68L AS user_id#70L, (id#68L % 10) AS region_id#71L, cast(id#68L as string) AS user_name#72]
   :        :           +- *(1) Filter isnotnull((id#68L % 10))
   :        :              +- *(1) Range (0, 1000, step=1, splits=5)
   :        +- Sort [region_id#78L ASC NULLS FIRST], false, 0
   :           +- ShuffleQueryStage 1
   :              +- Exchange hashpartitioning(region_id#78L, 8), ENSURE_REQUIREMENTS, [plan_id=124]
   :                 +- *(2) Project [id#76L AS region_id#78L, cast(id#76L as string) AS region_name#79]
   :                    +- *(2) Range (0, 10, step=1, splits=1)
   +- Sort [user_id#61L ASC NULLS FIRST, region_id#62L ASC NULLS FIRST, region_id#62L ASC NULLS FIRST], false, 0
      +- ShuffleQueryStage 2
         +- Exchange hashpartitioning(user_id#61L, region_id#62L, region_id#62L, 8), ENSURE_REQUIREMENTS, [plan_id=141]
            +- *(3) Project [id#58L AS order_id#60L, (id#58L % 1000) AS user_id#61L, (id#58L % 10) AS region_id#62L, cast(id#58L as string) AS order_num#63]
               +- *(3) Filter (isnotnull((id#58L % 1000)) AND isnotnull((id#58L % 10)))
                  +- *(3) Range (0, 10000, step=1, splits=10)
】

21:32:55.065 [Executor task launch worker for task 0.0 in stage 2.0 (TID 6)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 18.597743 ms
21:32:55.067 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - Considering join on: Some((((user_id#61L = user_id#70L) AND (region_id#62L = region_id#71L)) AND (region_id#62L = region_id#78L)))
21:32:55.067 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - leftKeys:List(user_id#70L, region_id#71L, region_id#78L) | rightKeys:List(user_id#61L, region_id#62L, region_id#62L)
21:32:55.067 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - Considering join on: Some((region_id#71L = region_id#78L))
21:32:55.067 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - leftKeys:List(region_id#71L) | rightKeys:List(region_id#78L)
SparkPlanner(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2@321bf4b1) - org.apache.spark.sql.execution.SparkStrategies$SpecialLimits$ -->【
ReturnAnswer
+- Project [order_id#60L, user_id#61L, region_id#62L, order_num#63, user_name#72, region_name#79]
   +- Join Inner, (((user_id#61L = user_id#70L) AND (region_id#62L = region_id#71L)) AND (region_id#62L = region_id#78L))
      :- Join Inner, (region_id#71L = region_id#78L)
      :  :- LogicalQueryStage Project [id#68L AS user_id#70L, (id#68L % 10) AS region_id#71L, cast(id#68L as string) AS user_name#72], ShuffleQueryStage 0
      :  +- LogicalQueryStage Project [id#76L AS region_id#78L, cast(id#76L as string) AS region_name#79], ShuffleQueryStage 1
      +- LogicalQueryStage Project [id#58L AS order_id#60L, (id#58L % 1000) AS user_id#61L, (id#58L % 10) AS region_id#62L, cast(id#58L as string) AS order_num#63], ShuffleQueryStage 2
】 --> 【
PlanLater Project [order_id#60L, user_id#61L, region_id#62L, order_num#63, user_name#72, region_name#79]
】

SparkPlanner(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2@321bf4b1) - org.apache.spark.sql.execution.SparkStrategies$BasicOperators$ -->【
Project [order_id#60L, user_id#61L, region_id#62L, order_num#63, user_name#72, region_name#79]
+- Join Inner, (((user_id#61L = user_id#70L) AND (region_id#62L = region_id#71L)) AND (region_id#62L = region_id#78L))
   :- Join Inner, (region_id#71L = region_id#78L)
   :  :- LogicalQueryStage Project [id#68L AS user_id#70L, (id#68L % 10) AS region_id#71L, cast(id#68L as string) AS user_name#72], ShuffleQueryStage 0
   :  +- LogicalQueryStage Project [id#76L AS region_id#78L, cast(id#76L as string) AS region_name#79], ShuffleQueryStage 1
   +- LogicalQueryStage Project [id#58L AS order_id#60L, (id#58L % 1000) AS user_id#61L, (id#58L % 10) AS region_id#62L, cast(id#58L as string) AS order_num#63], ShuffleQueryStage 2
】 --> 【
Project [order_id#60L, user_id#61L, region_id#62L, order_num#63, user_name#72, region_name#79]
+- PlanLater Join Inner, (((user_id#61L = user_id#70L) AND (region_id#62L = region_id#71L)) AND (region_id#62L = region_id#78L))
】

21:32:55.072 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - Considering join on: Some((((user_id#61L = user_id#70L) AND (region_id#62L = region_id#71L)) AND (region_id#62L = region_id#78L)))
21:32:55.072 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - leftKeys:List(user_id#70L, region_id#71L, region_id#78L) | rightKeys:List(user_id#61L, region_id#62L, region_id#62L)
21:32:55.072 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - Considering join on: Some((((user_id#61L = user_id#70L) AND (region_id#62L = region_id#71L)) AND (region_id#62L = region_id#78L)))
21:32:55.072 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - leftKeys:List(user_id#70L, region_id#71L, region_id#78L) | rightKeys:List(user_id#61L, region_id#62L, region_id#62L)
21:32:55.073 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.execution.adaptive.LogicalQueryStage - Physical stats available as Statistics(sizeInBytes=39.1 KiB, rowCount=1.00E+3) for plan: ShuffleQueryStage 0
+- Exchange hashpartitioning(region_id#71L, 8), ENSURE_REQUIREMENTS, [plan_id=117]
   +- *(1) Project [id#68L AS user_id#70L, (id#68L % 10) AS region_id#71L, cast(id#68L as string) AS user_name#72]
      +- *(1) Filter isnotnull((id#68L % 10))
         +- *(1) Range (0, 1000, step=1, splits=5)

21:32:55.073 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.execution.adaptive.LogicalQueryStage - Physical stats available as Statistics(sizeInBytes=320.0 B, rowCount=10) for plan: ShuffleQueryStage 1
+- Exchange hashpartitioning(region_id#78L, 8), ENSURE_REQUIREMENTS, [plan_id=124]
   +- *(2) Project [id#76L AS region_id#78L, cast(id#76L as string) AS region_name#79]
      +- *(2) Range (0, 10, step=1, splits=1)

21:32:55.073 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - Considering join on: Some((region_id#71L = region_id#78L))
21:32:55.073 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - leftKeys:List(region_id#71L) | rightKeys:List(region_id#78L)
21:32:55.074 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.execution.adaptive.LogicalQueryStage - Physical stats not available for plan: ShuffleQueryStage 2
+- Exchange hashpartitioning(user_id#61L, region_id#62L, region_id#62L, 8), ENSURE_REQUIREMENTS, [plan_id=141]
   +- *(3) Project [id#58L AS order_id#60L, (id#58L % 1000) AS user_id#61L, (id#58L % 10) AS region_id#62L, cast(id#58L as string) AS order_num#63]
      +- *(3) Filter (isnotnull((id#58L % 1000)) AND isnotnull((id#58L % 10)))
         +- *(3) Range (0, 10000, step=1, splits=10)

SparkPlanner(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2@321bf4b1) - org.apache.spark.sql.execution.SparkStrategies$JoinSelection$ -->【
Join Inner, (((user_id#61L = user_id#70L) AND (region_id#62L = region_id#71L)) AND (region_id#62L = region_id#78L))
:- Join Inner, (region_id#71L = region_id#78L)
:  :- LogicalQueryStage Project [id#68L AS user_id#70L, (id#68L % 10) AS region_id#71L, cast(id#68L as string) AS user_name#72], ShuffleQueryStage 0
:  +- LogicalQueryStage Project [id#76L AS region_id#78L, cast(id#76L as string) AS region_name#79], ShuffleQueryStage 1
+- LogicalQueryStage Project [id#58L AS order_id#60L, (id#58L % 1000) AS user_id#61L, (id#58L % 10) AS region_id#62L, cast(id#58L as string) AS order_num#63], ShuffleQueryStage 2
】 --> 【
SortMergeJoin [user_id#70L, region_id#71L, region_id#78L], [user_id#61L, region_id#62L, region_id#62L], Inner
:- PlanLater Join Inner, (region_id#71L = region_id#78L)
+- PlanLater LogicalQueryStage Project [id#58L AS order_id#60L, (id#58L % 1000) AS user_id#61L, (id#58L % 10) AS region_id#62L, cast(id#58L as string) AS order_num#63], ShuffleQueryStage 2
】

21:32:55.075 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - Considering join on: Some((region_id#71L = region_id#78L))
21:32:55.075 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - leftKeys:List(region_id#71L) | rightKeys:List(region_id#78L)
21:32:55.076 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - Considering join on: Some((region_id#71L = region_id#78L))
21:32:55.076 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - leftKeys:List(region_id#71L) | rightKeys:List(region_id#78L)
SparkPlanner(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2@321bf4b1) - org.apache.spark.sql.execution.SparkStrategies$JoinSelection$ -->【
Join Inner, (region_id#71L = region_id#78L)
:- LogicalQueryStage Project [id#68L AS user_id#70L, (id#68L % 10) AS region_id#71L, cast(id#68L as string) AS user_name#72], ShuffleQueryStage 0
+- LogicalQueryStage Project [id#76L AS region_id#78L, cast(id#76L as string) AS region_name#79], ShuffleQueryStage 1
】 --> 【
SortMergeJoin [region_id#71L], [region_id#78L], Inner
:- PlanLater LogicalQueryStage Project [id#68L AS user_id#70L, (id#68L % 10) AS region_id#71L, cast(id#68L as string) AS user_name#72], ShuffleQueryStage 0
+- PlanLater LogicalQueryStage Project [id#76L AS region_id#78L, cast(id#76L as string) AS region_name#79], ShuffleQueryStage 1
】

SparkPlanner(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2@321bf4b1) - org.apache.spark.sql.execution.adaptive.LogicalQueryStageStrategy$ -->【
LogicalQueryStage Project [id#68L AS user_id#70L, (id#68L % 10) AS region_id#71L, cast(id#68L as string) AS user_name#72], ShuffleQueryStage 0
】 --> 【
ShuffleQueryStage 0
+- Exchange hashpartitioning(region_id#71L, 8), ENSURE_REQUIREMENTS, [plan_id=117]
   +- *(1) Project [id#68L AS user_id#70L, (id#68L % 10) AS region_id#71L, cast(id#68L as string) AS user_name#72]
      +- *(1) Filter isnotnull((id#68L % 10))
         +- *(1) Range (0, 1000, step=1, splits=5)
】

21:32:55.078 [Executor task launch worker for task 0.0 in stage 2.0 (TID 6)] DEBUG org.apache.spark.shuffle.sort.io.LocalDiskShuffleMapOutputWriter - Writing shuffle index file for mapId 6 with length 8
SparkPlanner(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2@321bf4b1) - org.apache.spark.sql.execution.adaptive.LogicalQueryStageStrategy$ -->【
LogicalQueryStage Project [id#76L AS region_id#78L, cast(id#76L as string) AS region_name#79], ShuffleQueryStage 1
】 --> 【
ShuffleQueryStage 1
+- Exchange hashpartitioning(region_id#78L, 8), ENSURE_REQUIREMENTS, [plan_id=124]
   +- *(2) Project [id#76L AS region_id#78L, cast(id#76L as string) AS region_name#79]
      +- *(2) Range (0, 10, step=1, splits=1)
】

SparkPlanner(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2@321bf4b1) - org.apache.spark.sql.execution.adaptive.LogicalQueryStageStrategy$ -->【
LogicalQueryStage Project [id#58L AS order_id#60L, (id#58L % 1000) AS user_id#61L, (id#58L % 10) AS region_id#62L, cast(id#58L as string) AS order_num#63], ShuffleQueryStage 2
】 --> 【
ShuffleQueryStage 2
+- Exchange hashpartitioning(user_id#61L, region_id#62L, region_id#62L, 8), ENSURE_REQUIREMENTS, [plan_id=141]
   +- *(3) Project [id#58L AS order_id#60L, (id#58L % 1000) AS user_id#61L, (id#58L % 10) AS region_id#62L, cast(id#58L as string) AS order_num#63]
      +- *(3) Filter (isnotnull((id#58L % 1000)) AND isnotnull((id#58L % 10)))
         +- *(3) Range (0, 10000, step=1, splits=10)
】

21:32:55.080 [Executor task launch worker for task 0.0 in stage 2.0 (TID 6)] DEBUG org.apache.spark.shuffle.IndexShuffleBlockResolver - Shuffle index for mapId 6: [2189,2208,2420,2245,1814,2067,2249,2345]
21:32:55.081 [Executor task launch worker for task 0.0 in stage 2.0 (TID 6)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 2.0 (TID 6). 1804 bytes result sent to driver
21:32:55.081 [Executor task launch worker for task 0.0 in stage 2.0 (TID 6)] DEBUG org.apache.spark.executor.ExecutorMetricsPoller - stageTCMP: (2, 0) -> 0
21:32:55.081 [dispatcher-event-loop-1] DEBUG org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_2.0, runningTasks: 0
21:32:55.082 [dispatcher-event-loop-1] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 2.0 (TID 7) (192.168.31.69, executor driver, partition 1, PROCESS_LOCAL, 4567 bytes) taskResourceAssignments Map()
21:32:55.082 [Executor task launch worker for task 1.0 in stage 2.0 (TID 7)] INFO  org.apache.spark.executor.Executor - Running task 1.0 in stage 2.0 (TID 7)
21:32:55.082 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 2.0 (TID 6) in 46 ms on 192.168.31.69 (executor driver) (1/10)
21:32:55.082 [Executor task launch worker for task 1.0 in stage 2.0 (TID 7)] DEBUG org.apache.spark.executor.ExecutorMetricsPoller - stageTCMP: (2, 0) -> 1
21:32:55.082 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - ShuffleMapTask finished on driver
org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec$ - applyPhysicalRules - org.apache.spark.sql.execution.exchange.EnsureRequirements -->【
Project [order_id#60L, user_id#61L, region_id#62L, order_num#63, user_name#72, region_name#79]
+- SortMergeJoin [user_id#70L, region_id#71L, region_id#78L], [user_id#61L, region_id#62L, region_id#62L], Inner
   :- SortMergeJoin [region_id#71L], [region_id#78L], Inner
   :  :- ShuffleQueryStage 0
   :  :  +- Exchange hashpartitioning(region_id#71L, 8), ENSURE_REQUIREMENTS, [plan_id=117]
   :  :     +- *(1) Project [id#68L AS user_id#70L, (id#68L % 10) AS region_id#71L, cast(id#68L as string) AS user_name#72]
   :  :        +- *(1) Filter isnotnull((id#68L % 10))
   :  :           +- *(1) Range (0, 1000, step=1, splits=5)
   :  +- ShuffleQueryStage 1
   :     +- Exchange hashpartitioning(region_id#78L, 8), ENSURE_REQUIREMENTS, [plan_id=124]
   :        +- *(2) Project [id#76L AS region_id#78L, cast(id#76L as string) AS region_name#79]
   :           +- *(2) Range (0, 10, step=1, splits=1)
   +- ShuffleQueryStage 2
      +- Exchange hashpartitioning(user_id#61L, region_id#62L, region_id#62L, 8), ENSURE_REQUIREMENTS, [plan_id=141]
         +- *(3) Project [id#58L AS order_id#60L, (id#58L % 1000) AS user_id#61L, (id#58L % 10) AS region_id#62L, cast(id#58L as string) AS order_num#63]
            +- *(3) Filter (isnotnull((id#58L % 1000)) AND isnotnull((id#58L % 10)))
               +- *(3) Range (0, 10000, step=1, splits=10)
】AQE --> 【
Project [order_id#60L, user_id#61L, region_id#62L, order_num#63, user_name#72, region_name#79]
+- SortMergeJoin [user_id#70L, region_id#71L, region_id#78L], [user_id#61L, region_id#62L, region_id#62L], Inner
   :- Sort [user_id#70L ASC NULLS FIRST, region_id#71L ASC NULLS FIRST, region_id#78L ASC NULLS FIRST], false, 0
   :  +- Exchange hashpartitioning(user_id#70L, region_id#71L, region_id#71L, 8), ENSURE_REQUIREMENTS, [plan_id=202]
   :     +- SortMergeJoin [region_id#71L], [region_id#78L], Inner
   :        :- Sort [region_id#71L ASC NULLS FIRST], false, 0
   :        :  +- ShuffleQueryStage 0
   :        :     +- Exchange hashpartitioning(region_id#71L, 8), ENSURE_REQUIREMENTS, [plan_id=117]
   :        :        +- *(1) Project [id#68L AS user_id#70L, (id#68L % 10) AS region_id#71L, cast(id#68L as string) AS user_name#72]
   :        :           +- *(1) Filter isnotnull((id#68L % 10))
   :        :              +- *(1) Range (0, 1000, step=1, splits=5)
   :        +- Sort [region_id#78L ASC NULLS FIRST], false, 0
   :           +- ShuffleQueryStage 1
   :              +- Exchange hashpartitioning(region_id#78L, 8), ENSURE_REQUIREMENTS, [plan_id=124]
   :                 +- *(2) Project [id#76L AS region_id#78L, cast(id#76L as string) AS region_name#79]
   :                    +- *(2) Range (0, 10, step=1, splits=1)
   +- Sort [user_id#61L ASC NULLS FIRST, region_id#62L ASC NULLS FIRST, region_id#62L ASC NULLS FIRST], false, 0
      +- ShuffleQueryStage 2
         +- Exchange hashpartitioning(user_id#61L, region_id#62L, region_id#62L, 8), ENSURE_REQUIREMENTS, [plan_id=141]
            +- *(3) Project [id#58L AS order_id#60L, (id#58L % 1000) AS user_id#61L, (id#58L % 10) AS region_id#62L, cast(id#58L as string) AS order_num#63]
               +- *(3) Filter (isnotnull((id#58L % 1000)) AND isnotnull((id#58L % 10)))
                  +- *(3) Range (0, 10000, step=1, splits=10)
】

21:32:55.087 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.execution.adaptive.OptimizeSkewedJoin -
Optimizing skewed join.
Left side partitions size info:
median size: 1532, max size: 4514, min size: 0, avg size: 1885
Right side partitions size info:
median size: 72, max size: 106, min size: 0, avg size: 55

21:32:55.095 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.execution.adaptive.OptimizeSkewedJoin - number of skewed partitions: left 0, right 0
21:32:55.096 [Executor task launch worker for task 1.0 in stage 2.0 (TID 7)] DEBUG org.apache.spark.shuffle.sort.io.LocalDiskShuffleMapOutputWriter - Writing shuffle index file for mapId 7 with length 8
21:32:55.097 [Executor task launch worker for task 1.0 in stage 2.0 (TID 7)] DEBUG org.apache.spark.shuffle.IndexShuffleBlockResolver - Shuffle index for mapId 7: [2280,2250,2609,2384,1905,2149,2330,2554]
21:32:55.098 [Executor task launch worker for task 1.0 in stage 2.0 (TID 7)] INFO  org.apache.spark.executor.Executor - Finished task 1.0 in stage 2.0 (TID 7). 1804 bytes result sent to driver
org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec$ final result ==>【
Project [order_id#60L, user_id#61L, region_id#62L, order_num#63, user_name#72, region_name#79]
+- SortMergeJoin [user_id#70L, region_id#71L, region_id#78L], [user_id#61L, region_id#62L, region_id#62L], Inner
   :- SortMergeJoin [region_id#71L], [region_id#78L], Inner
   :  :- ShuffleQueryStage 0
   :  :  +- Exchange hashpartitioning(region_id#71L, 8), ENSURE_REQUIREMENTS, [plan_id=117]
   :  :     +- *(1) Project [id#68L AS user_id#70L, (id#68L % 10) AS region_id#71L, cast(id#68L as string) AS user_name#72]
   :  :        +- *(1) Filter isnotnull((id#68L % 10))
   :  :           +- *(1) Range (0, 1000, step=1, splits=5)
   :  +- ShuffleQueryStage 1
   :     +- Exchange hashpartitioning(region_id#78L, 8), ENSURE_REQUIREMENTS, [plan_id=124]
   :        +- *(2) Project [id#76L AS region_id#78L, cast(id#76L as string) AS region_name#79]
   :           +- *(2) Range (0, 10, step=1, splits=1)
   +- ShuffleQueryStage 2
      +- Exchange hashpartitioning(user_id#61L, region_id#62L, region_id#62L, 8), ENSURE_REQUIREMENTS, [plan_id=141]
         +- *(3) Project [id#58L AS order_id#60L, (id#58L % 1000) AS user_id#61L, (id#58L % 10) AS region_id#62L, cast(id#58L as string) AS order_num#63]
            +- *(3) Filter (isnotnull((id#58L % 1000)) AND isnotnull((id#58L % 10)))
               +- *(3) Range (0, 10000, step=1, splits=10)
】AQE ==> 【
Project [order_id#60L, user_id#61L, region_id#62L, order_num#63, user_name#72, region_name#79]
+- SortMergeJoin [user_id#70L, region_id#71L, region_id#78L], [user_id#61L, region_id#62L, region_id#62L], Inner
   :- Sort [user_id#70L ASC NULLS FIRST, region_id#71L ASC NULLS FIRST, region_id#78L ASC NULLS FIRST], false, 0
   :  +- Exchange hashpartitioning(user_id#70L, region_id#71L, region_id#71L, 8), ENSURE_REQUIREMENTS, [plan_id=202]
   :     +- SortMergeJoin [region_id#71L], [region_id#78L], Inner
   :        :- Sort [region_id#71L ASC NULLS FIRST], false, 0
   :        :  +- ShuffleQueryStage 0
   :        :     +- Exchange hashpartitioning(region_id#71L, 8), ENSURE_REQUIREMENTS, [plan_id=117]
   :        :        +- *(1) Project [id#68L AS user_id#70L, (id#68L % 10) AS region_id#71L, cast(id#68L as string) AS user_name#72]
   :        :           +- *(1) Filter isnotnull((id#68L % 10))
   :        :              +- *(1) Range (0, 1000, step=1, splits=5)
   :        +- Sort [region_id#78L ASC NULLS FIRST], false, 0
   :           +- ShuffleQueryStage 1
   :              +- Exchange hashpartitioning(region_id#78L, 8), ENSURE_REQUIREMENTS, [plan_id=124]
   :                 +- *(2) Project [id#76L AS region_id#78L, cast(id#76L as string) AS region_name#79]
   :                    +- *(2) Range (0, 10, step=1, splits=1)
   +- Sort [user_id#61L ASC NULLS FIRST, region_id#62L ASC NULLS FIRST, region_id#62L ASC NULLS FIRST], false, 0
      +- ShuffleQueryStage 2
         +- Exchange hashpartitioning(user_id#61L, region_id#62L, region_id#62L, 8), ENSURE_REQUIREMENTS, [plan_id=141]
            +- *(3) Project [id#58L AS order_id#60L, (id#58L % 1000) AS user_id#61L, (id#58L % 10) AS region_id#62L, cast(id#58L as string) AS order_num#63]
               +- *(3) Filter (isnotnull((id#58L % 1000)) AND isnotnull((id#58L % 10)))
                  +- *(3) Range (0, 10000, step=1, splits=10)
】

21:32:55.098 [Executor task launch worker for task 1.0 in stage 2.0 (TID 7)] DEBUG org.apache.spark.executor.ExecutorMetricsPoller - stageTCMP: (2, 0) -> 0
21:32:55.098 [dispatcher-event-loop-1] DEBUG org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_2.0, runningTasks: 0
21:32:55.098 [dispatcher-event-loop-1] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 2.0 (TID 8) (192.168.31.69, executor driver, partition 2, PROCESS_LOCAL, 4567 bytes) taskResourceAssignments Map()
21:32:55.098 [Executor task launch worker for task 2.0 in stage 2.0 (TID 8)] INFO  org.apache.spark.executor.Executor - Running task 2.0 in stage 2.0 (TID 8)
21:32:55.098 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 2.0 (TID 7) in 17 ms on 192.168.31.69 (executor driver) (2/10)
21:32:55.099 [Executor task launch worker for task 2.0 in stage 2.0 (TID 8)] DEBUG org.apache.spark.executor.ExecutorMetricsPoller - stageTCMP: (2, 0) -> 1
21:32:55.099 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - ShuffleMapTask finished on driver
21:32:55.106 [ScalaTest-run-running-PhysicalPlanSpec] INFO  org.apache.spark.sql.execution.adaptive.ShufflePartitionsUtil - For shuffle(0, 1), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
21:32:55.109 [Executor task launch worker for task 2.0 in stage 2.0 (TID 8)] DEBUG org.apache.spark.shuffle.sort.io.LocalDiskShuffleMapOutputWriter - Writing shuffle index file for mapId 8 with length 8
21:32:55.110 [Executor task launch worker for task 2.0 in stage 2.0 (TID 8)] DEBUG org.apache.spark.shuffle.IndexShuffleBlockResolver - Shuffle index for mapId 8: [2204,2214,2522,2284,1874,2077,2287,2510]
21:32:55.111 [Executor task launch worker for task 2.0 in stage 2.0 (TID 8)] INFO  org.apache.spark.executor.Executor - Finished task 2.0 in stage 2.0 (TID 8). 1804 bytes result sent to driver
21:32:55.111 [Executor task launch worker for task 2.0 in stage 2.0 (TID 8)] DEBUG org.apache.spark.executor.ExecutorMetricsPoller - stageTCMP: (2, 0) -> 0
21:32:55.111 [dispatcher-event-loop-1] DEBUG org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_2.0, runningTasks: 0
21:32:55.111 [dispatcher-event-loop-1] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 2.0 (TID 9) (192.168.31.69, executor driver, partition 3, PROCESS_LOCAL, 4567 bytes) taskResourceAssignments Map()
21:32:55.112 [Executor task launch worker for task 3.0 in stage 2.0 (TID 9)] INFO  org.apache.spark.executor.Executor - Running task 3.0 in stage 2.0 (TID 9)
21:32:55.112 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 2.0 (TID 8) in 14 ms on 192.168.31.69 (executor driver) (3/10)
21:32:55.112 [Executor task launch worker for task 3.0 in stage 2.0 (TID 9)] DEBUG org.apache.spark.executor.ExecutorMetricsPoller - stageTCMP: (2, 0) -> 1
21:32:55.112 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - ShuffleMapTask finished on driver
optimizeQueryStage - org.apache.spark.sql.execution.adaptive.CoalesceShufflePartitions -> old【
SortMergeJoin [region_id#71L], [region_id#78L], Inner
:- Sort [region_id#71L ASC NULLS FIRST], false, 0
:  +- ShuffleQueryStage 0
:     +- Exchange hashpartitioning(region_id#71L, 8), ENSURE_REQUIREMENTS, [plan_id=117]
:        +- *(1) Project [id#68L AS user_id#70L, (id#68L % 10) AS region_id#71L, cast(id#68L as string) AS user_name#72]
:           +- *(1) Filter isnotnull((id#68L % 10))
:              +- *(1) Range (0, 1000, step=1, splits=5)
+- Sort [region_id#78L ASC NULLS FIRST], false, 0
   +- ShuffleQueryStage 1
      +- Exchange hashpartitioning(region_id#78L, 8), ENSURE_REQUIREMENTS, [plan_id=124]
         +- *(2) Project [id#76L AS region_id#78L, cast(id#76L as string) AS region_name#79]
            +- *(2) Range (0, 10, step=1, splits=1)
】AQE --> new【
SortMergeJoin [region_id#71L], [region_id#78L], Inner
:- Sort [region_id#71L ASC NULLS FIRST], false, 0
:  +- AQEShuffleRead coalesced
:     +- ShuffleQueryStage 0
:        +- Exchange hashpartitioning(region_id#71L, 8), ENSURE_REQUIREMENTS, [plan_id=117]
:           +- *(1) Project [id#68L AS user_id#70L, (id#68L % 10) AS region_id#71L, cast(id#68L as string) AS user_name#72]
:              +- *(1) Filter isnotnull((id#68L % 10))
:                 +- *(1) Range (0, 1000, step=1, splits=5)
+- Sort [region_id#78L ASC NULLS FIRST], false, 0
   +- AQEShuffleRead coalesced
      +- ShuffleQueryStage 1
         +- Exchange hashpartitioning(region_id#78L, 8), ENSURE_REQUIREMENTS, [plan_id=124]
            +- *(2) Project [id#76L AS region_id#78L, cast(id#76L as string) AS region_name#79]
               +- *(2) Range (0, 10, step=1, splits=1)
】


org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec$ - applyPhysicalRules - org.apache.spark.sql.execution.CollapseCodegenStages -->【
Exchange hashpartitioning(user_id#70L, region_id#71L, region_id#71L, 8), ENSURE_REQUIREMENTS, [plan_id=212]
+- SortMergeJoin [region_id#71L], [region_id#78L], Inner
   :- Sort [region_id#71L ASC NULLS FIRST], false, 0
   :  +- AQEShuffleRead coalesced
   :     +- ShuffleQueryStage 0
   :        +- Exchange hashpartitioning(region_id#71L, 8), ENSURE_REQUIREMENTS, [plan_id=117]
   :           +- *(1) Project [id#68L AS user_id#70L, (id#68L % 10) AS region_id#71L, cast(id#68L as string) AS user_name#72]
   :              +- *(1) Filter isnotnull((id#68L % 10))
   :                 +- *(1) Range (0, 1000, step=1, splits=5)
   +- Sort [region_id#78L ASC NULLS FIRST], false, 0
      +- AQEShuffleRead coalesced
         +- ShuffleQueryStage 1
            +- Exchange hashpartitioning(region_id#78L, 8), ENSURE_REQUIREMENTS, [plan_id=124]
               +- *(2) Project [id#76L AS region_id#78L, cast(id#76L as string) AS region_name#79]
                  +- *(2) Range (0, 10, step=1, splits=1)
】AQE --> 【
Exchange hashpartitioning(user_id#70L, region_id#71L, region_id#71L, 8), ENSURE_REQUIREMENTS, [plan_id=223]
+- *(6) SortMergeJoin [region_id#71L], [region_id#78L], Inner
   :- *(4) Sort [region_id#71L ASC NULLS FIRST], false, 0
   :  +- AQEShuffleRead coalesced
   :     +- ShuffleQueryStage 0
   :        +- Exchange hashpartitioning(region_id#71L, 8), ENSURE_REQUIREMENTS, [plan_id=117]
   :           +- *(1) Project [id#68L AS user_id#70L, (id#68L % 10) AS region_id#71L, cast(id#68L as string) AS user_name#72]
   :              +- *(1) Filter isnotnull((id#68L % 10))
   :                 +- *(1) Range (0, 1000, step=1, splits=5)
   +- *(5) Sort [region_id#78L ASC NULLS FIRST], false, 0
      +- AQEShuffleRead coalesced
         +- ShuffleQueryStage 1
            +- Exchange hashpartitioning(region_id#78L, 8), ENSURE_REQUIREMENTS, [plan_id=124]
               +- *(2) Project [id#76L AS region_id#78L, cast(id#76L as string) AS region_name#79]
                  +- *(2) Range (0, 10, step=1, splits=1)
】

org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec$ final result ==>【
Exchange hashpartitioning(user_id#70L, region_id#71L, region_id#71L, 8), ENSURE_REQUIREMENTS, [plan_id=212]
+- SortMergeJoin [region_id#71L], [region_id#78L], Inner
   :- Sort [region_id#71L ASC NULLS FIRST], false, 0
   :  +- AQEShuffleRead coalesced
   :     +- ShuffleQueryStage 0
   :        +- Exchange hashpartitioning(region_id#71L, 8), ENSURE_REQUIREMENTS, [plan_id=117]
   :           +- *(1) Project [id#68L AS user_id#70L, (id#68L % 10) AS region_id#71L, cast(id#68L as string) AS user_name#72]
   :              +- *(1) Filter isnotnull((id#68L % 10))
   :                 +- *(1) Range (0, 1000, step=1, splits=5)
   +- Sort [region_id#78L ASC NULLS FIRST], false, 0
      +- AQEShuffleRead coalesced
         +- ShuffleQueryStage 1
            +- Exchange hashpartitioning(region_id#78L, 8), ENSURE_REQUIREMENTS, [plan_id=124]
               +- *(2) Project [id#76L AS region_id#78L, cast(id#76L as string) AS region_name#79]
                  +- *(2) Range (0, 10, step=1, splits=1)
】AQE ==> 【
Exchange hashpartitioning(user_id#70L, region_id#71L, region_id#71L, 8), ENSURE_REQUIREMENTS, [plan_id=223]
+- *(6) SortMergeJoin [region_id#71L], [region_id#78L], Inner
   :- *(4) Sort [region_id#71L ASC NULLS FIRST], false, 0
   :  +- AQEShuffleRead coalesced
   :     +- ShuffleQueryStage 0
   :        +- Exchange hashpartitioning(region_id#71L, 8), ENSURE_REQUIREMENTS, [plan_id=117]
   :           +- *(1) Project [id#68L AS user_id#70L, (id#68L % 10) AS region_id#71L, cast(id#68L as string) AS user_name#72]
   :              +- *(1) Filter isnotnull((id#68L % 10))
   :                 +- *(1) Range (0, 1000, step=1, splits=5)
   +- *(5) Sort [region_id#78L ASC NULLS FIRST], false, 0
      +- AQEShuffleRead coalesced
         +- ShuffleQueryStage 1
            +- Exchange hashpartitioning(region_id#78L, 8), ENSURE_REQUIREMENTS, [plan_id=124]
               +- *(2) Project [id#76L AS region_id#78L, cast(id#76L as string) AS region_name#79]
                  +- *(2) Range (0, 10, step=1, splits=1)
】

21:32:55.122 [Executor task launch worker for task 3.0 in stage 2.0 (TID 9)] DEBUG org.apache.spark.shuffle.sort.io.LocalDiskShuffleMapOutputWriter - Writing shuffle index file for mapId 9 with length 8
21:32:55.124 [Executor task launch worker for task 3.0 in stage 2.0 (TID 9)] DEBUG org.apache.spark.shuffle.IndexShuffleBlockResolver - Shuffle index for mapId 9: [2189,2199,2534,2284,1852,2065,2262,2493]
21:32:55.125 [Executor task launch worker for task 3.0 in stage 2.0 (TID 9)] INFO  org.apache.spark.executor.Executor - Finished task 3.0 in stage 2.0 (TID 9). 1804 bytes result sent to driver
21:32:55.125 [Executor task launch worker for task 3.0 in stage 2.0 (TID 9)] DEBUG org.apache.spark.executor.ExecutorMetricsPoller - stageTCMP: (2, 0) -> 0
21:32:55.126 [dispatcher-event-loop-1] DEBUG org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_2.0, runningTasks: 0
21:32:55.127 [dispatcher-event-loop-1] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 2.0 (TID 10) (192.168.31.69, executor driver, partition 4, PROCESS_LOCAL, 4567 bytes) taskResourceAssignments Map()
21:32:55.127 [Executor task launch worker for task 4.0 in stage 2.0 (TID 10)] INFO  org.apache.spark.executor.Executor - Running task 4.0 in stage 2.0 (TID 10)
21:32:55.127 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 2.0 (TID 9) in 16 ms on 192.168.31.69 (executor driver) (4/10)
21:32:55.127 [Executor task launch worker for task 4.0 in stage 2.0 (TID 10)] DEBUG org.apache.spark.executor.ExecutorMetricsPoller - stageTCMP: (2, 0) -> 1
21:32:55.128 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - ShuffleMapTask finished on driver
21:32:55.130 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec - Materialize query stage ShuffleQueryStageExec: 3, query stage plan:
Exchange hashpartitioning(user_id#70L, region_id#71L, region_id#71L, 8), ENSURE_REQUIREMENTS, [plan_id=223]
+- *(6) SortMergeJoin [region_id#71L], [region_id#78L], Inner
   :- *(4) Sort [region_id#71L ASC NULLS FIRST], false, 0
   :  +- AQEShuffleRead coalesced
   :     +- ShuffleQueryStage 0
   :        +- Exchange hashpartitioning(region_id#71L, 8), ENSURE_REQUIREMENTS, [plan_id=117]
   :           +- *(1) Project [id#68L AS user_id#70L, (id#68L % 10) AS region_id#71L, cast(id#68L as string) AS user_name#72]
   :              +- *(1) Filter isnotnull((id#68L % 10))
   :                 +- *(1) Range (0, 1000, step=1, splits=5)
   +- *(5) Sort [region_id#78L ASC NULLS FIRST], false, 0
      +- AQEShuffleRead coalesced
         +- ShuffleQueryStage 1
            +- Exchange hashpartitioning(region_id#78L, 8), ENSURE_REQUIREMENTS, [plan_id=124]
               +- *(2) Project [id#76L AS region_id#78L, cast(id#76L as string) AS region_name#79]
                  +- *(2) Range (0, 10, step=1, splits=1)

21:32:55.140 [Executor task launch worker for task 4.0 in stage 2.0 (TID 10)] DEBUG org.apache.spark.shuffle.sort.io.LocalDiskShuffleMapOutputWriter - Writing shuffle index file for mapId 10 with length 8
21:32:55.141 [Executor task launch worker for task 4.0 in stage 2.0 (TID 10)] DEBUG org.apache.spark.shuffle.IndexShuffleBlockResolver - Shuffle index for mapId 10: [2189,2212,2509,2281,1849,2064,2265,2490]
21:32:55.141 [Executor task launch worker for task 4.0 in stage 2.0 (TID 10)] INFO  org.apache.spark.executor.Executor - Finished task 4.0 in stage 2.0 (TID 10). 1804 bytes result sent to driver
21:32:55.141 [Executor task launch worker for task 4.0 in stage 2.0 (TID 10)] DEBUG org.apache.spark.executor.ExecutorMetricsPoller - stageTCMP: (2, 0) -> 0
21:32:55.142 [dispatcher-event-loop-1] DEBUG org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_2.0, runningTasks: 0
21:32:55.142 [dispatcher-event-loop-1] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 2.0 (TID 11) (192.168.31.69, executor driver, partition 5, PROCESS_LOCAL, 4567 bytes) taskResourceAssignments Map()
21:32:55.142 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 2.0 (TID 10) in 16 ms on 192.168.31.69 (executor driver) (5/10)
21:32:55.142 [Executor task launch worker for task 5.0 in stage 2.0 (TID 11)] INFO  org.apache.spark.executor.Executor - Running task 5.0 in stage 2.0 (TID 11)
21:32:55.143 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - ShuffleMapTask finished on driver
21:32:55.143 [Executor task launch worker for task 5.0 in stage 2.0 (TID 11)] DEBUG org.apache.spark.executor.ExecutorMetricsPoller - stageTCMP: (2, 0) -> 1
21:32:55.155 [Executor task launch worker for task 5.0 in stage 2.0 (TID 11)] DEBUG org.apache.spark.shuffle.sort.io.LocalDiskShuffleMapOutputWriter - Writing shuffle index file for mapId 11 with length 8
21:32:55.156 [Executor task launch worker for task 5.0 in stage 2.0 (TID 11)] DEBUG org.apache.spark.shuffle.IndexShuffleBlockResolver - Shuffle index for mapId 11: [2186,2209,2534,2286,1856,2065,2266,2491]
21:32:55.157 [Executor task launch worker for task 5.0 in stage 2.0 (TID 11)] INFO  org.apache.spark.executor.Executor - Finished task 5.0 in stage 2.0 (TID 11). 1804 bytes result sent to driver
21:32:55.157 [Executor task launch worker for task 5.0 in stage 2.0 (TID 11)] DEBUG org.apache.spark.executor.ExecutorMetricsPoller - stageTCMP: (2, 0) -> 0
21:32:55.157 [dispatcher-event-loop-1] DEBUG org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_2.0, runningTasks: 0
21:32:55.158 [dispatcher-event-loop-1] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 2.0 (TID 12) (192.168.31.69, executor driver, partition 6, PROCESS_LOCAL, 4567 bytes) taskResourceAssignments Map()
21:32:55.158 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 2.0 (TID 11) in 16 ms on 192.168.31.69 (executor driver) (6/10)
21:32:55.158 [Executor task launch worker for task 6.0 in stage 2.0 (TID 12)] INFO  org.apache.spark.executor.Executor - Running task 6.0 in stage 2.0 (TID 12)
21:32:55.158 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - ShuffleMapTask finished on driver
21:32:55.158 [Executor task launch worker for task 6.0 in stage 2.0 (TID 12)] DEBUG org.apache.spark.executor.ExecutorMetricsPoller - stageTCMP: (2, 0) -> 1
21:32:55.170 [Executor task launch worker for task 6.0 in stage 2.0 (TID 12)] DEBUG org.apache.spark.shuffle.sort.io.LocalDiskShuffleMapOutputWriter - Writing shuffle index file for mapId 12 with length 8
21:32:55.171 [Executor task launch worker for task 6.0 in stage 2.0 (TID 12)] DEBUG org.apache.spark.shuffle.IndexShuffleBlockResolver - Shuffle index for mapId 12: [2199,2198,2538,2286,1853,2066,2272,2486]
21:32:55.172 [Executor task launch worker for task 6.0 in stage 2.0 (TID 12)] INFO  org.apache.spark.executor.Executor - Finished task 6.0 in stage 2.0 (TID 12). 1804 bytes result sent to driver
21:32:55.172 [Executor task launch worker for task 6.0 in stage 2.0 (TID 12)] DEBUG org.apache.spark.executor.ExecutorMetricsPoller - stageTCMP: (2, 0) -> 0
21:32:55.172 [dispatcher-event-loop-1] DEBUG org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_2.0, runningTasks: 0
21:32:55.172 [dispatcher-event-loop-1] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 2.0 (TID 13) (192.168.31.69, executor driver, partition 7, PROCESS_LOCAL, 4567 bytes) taskResourceAssignments Map()
21:32:55.173 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 2.0 (TID 12) in 15 ms on 192.168.31.69 (executor driver) (7/10)
21:32:55.173 [Executor task launch worker for task 7.0 in stage 2.0 (TID 13)] INFO  org.apache.spark.executor.Executor - Running task 7.0 in stage 2.0 (TID 13)
21:32:55.173 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - ShuffleMapTask finished on driver
21:32:55.173 [Executor task launch worker for task 7.0 in stage 2.0 (TID 13)] DEBUG org.apache.spark.executor.ExecutorMetricsPoller - stageTCMP: (2, 0) -> 1
21:32:55.179 [ScalaTest-run-running-PhysicalPlanSpec] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 36.167617 ms
21:32:55.182 [Executor task launch worker for task 7.0 in stage 2.0 (TID 13)] DEBUG org.apache.spark.shuffle.sort.io.LocalDiskShuffleMapOutputWriter - Writing shuffle index file for mapId 13 with length 8
21:32:55.183 [Executor task launch worker for task 7.0 in stage 2.0 (TID 13)] DEBUG org.apache.spark.shuffle.IndexShuffleBlockResolver - Shuffle index for mapId 13: [2187,2200,2530,2279,1847,2064,2268,2492]
21:32:55.184 [Executor task launch worker for task 7.0 in stage 2.0 (TID 13)] INFO  org.apache.spark.executor.Executor - Finished task 7.0 in stage 2.0 (TID 13). 1804 bytes result sent to driver
21:32:55.184 [Executor task launch worker for task 7.0 in stage 2.0 (TID 13)] DEBUG org.apache.spark.executor.ExecutorMetricsPoller - stageTCMP: (2, 0) -> 0
21:32:55.185 [dispatcher-event-loop-1] DEBUG org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_2.0, runningTasks: 0
21:32:55.185 [dispatcher-event-loop-1] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 8.0 in stage 2.0 (TID 14) (192.168.31.69, executor driver, partition 8, PROCESS_LOCAL, 4567 bytes) taskResourceAssignments Map()
21:32:55.185 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 2.0 (TID 13) in 13 ms on 192.168.31.69 (executor driver) (8/10)
21:32:55.185 [Executor task launch worker for task 8.0 in stage 2.0 (TID 14)] INFO  org.apache.spark.executor.Executor - Running task 8.0 in stage 2.0 (TID 14)
21:32:55.186 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - ShuffleMapTask finished on driver
21:32:55.186 [Executor task launch worker for task 8.0 in stage 2.0 (TID 14)] DEBUG org.apache.spark.executor.ExecutorMetricsPoller - stageTCMP: (2, 0) -> 1
21:32:55.198 [Executor task launch worker for task 8.0 in stage 2.0 (TID 14)] DEBUG org.apache.spark.shuffle.sort.io.LocalDiskShuffleMapOutputWriter - Writing shuffle index file for mapId 14 with length 8
21:32:55.199 [Executor task launch worker for task 8.0 in stage 2.0 (TID 14)] DEBUG org.apache.spark.shuffle.IndexShuffleBlockResolver - Shuffle index for mapId 14: [2190,2200,2530,2278,1851,2068,2261,2493]
21:32:55.200 [Executor task launch worker for task 8.0 in stage 2.0 (TID 14)] INFO  org.apache.spark.executor.Executor - Finished task 8.0 in stage 2.0 (TID 14). 1804 bytes result sent to driver
21:32:55.200 [Executor task launch worker for task 8.0 in stage 2.0 (TID 14)] DEBUG org.apache.spark.executor.ExecutorMetricsPoller - stageTCMP: (2, 0) -> 0
21:32:55.200 [dispatcher-event-loop-1] DEBUG org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_2.0, runningTasks: 0
21:32:55.201 [dispatcher-event-loop-1] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 9.0 in stage 2.0 (TID 15) (192.168.31.69, executor driver, partition 9, PROCESS_LOCAL, 4567 bytes) taskResourceAssignments Map()
21:32:55.201 [Executor task launch worker for task 9.0 in stage 2.0 (TID 15)] INFO  org.apache.spark.executor.Executor - Running task 9.0 in stage 2.0 (TID 15)
21:32:55.201 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 8.0 in stage 2.0 (TID 14) in 16 ms on 192.168.31.69 (executor driver) (9/10)
21:32:55.202 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - ShuffleMapTask finished on driver
21:32:55.202 [Executor task launch worker for task 9.0 in stage 2.0 (TID 15)] DEBUG org.apache.spark.executor.ExecutorMetricsPoller - stageTCMP: (2, 0) -> 1
21:32:55.203 [ScalaTest-run-running-PhysicalPlanSpec] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 17.392284 ms
21:32:55.209 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.util.ClosureCleaner - Cleaning indylambda closure: $anonfun$doExecute$4$adapted
21:32:55.210 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.util.ClosureCleaner -  +++ indylambda closure ($anonfun$doExecute$4$adapted) is now cleaned +++
21:32:55.217 [Executor task launch worker for task 9.0 in stage 2.0 (TID 15)] DEBUG org.apache.spark.shuffle.sort.io.LocalDiskShuffleMapOutputWriter - Writing shuffle index file for mapId 15 with length 8
21:32:55.218 [Executor task launch worker for task 9.0 in stage 2.0 (TID 15)] DEBUG org.apache.spark.shuffle.IndexShuffleBlockResolver - Shuffle index for mapId 15: [2197,2200,2531,2284,1853,2072,2271,2479]
21:32:55.219 [Executor task launch worker for task 9.0 in stage 2.0 (TID 15)] INFO  org.apache.spark.executor.Executor - Finished task 9.0 in stage 2.0 (TID 15). 1804 bytes result sent to driver
21:32:55.219 [Executor task launch worker for task 9.0 in stage 2.0 (TID 15)] DEBUG org.apache.spark.executor.ExecutorMetricsPoller - stageTCMP: (2, 0) -> 0
21:32:55.219 [dispatcher-event-loop-1] DEBUG org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_2.0, runningTasks: 0
21:32:55.219 [dispatcher-event-loop-1] DEBUG org.apache.spark.scheduler.TaskSetManager - No tasks for locality level NO_PREF, so moving to locality level ANY
21:32:55.220 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 9.0 in stage 2.0 (TID 15) in 20 ms on 192.168.31.69 (executor driver) (10/10)
21:32:55.220 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 2.0, whose tasks have all completed, from pool
21:32:55.220 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - ShuffleMapTask finished on driver
21:32:55.221 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 2 (saveAsTable at PhysicalPlanSpec.scala:338) finished in 0.720 s
21:32:55.221 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
21:32:55.221 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - running: Set()
21:32:55.221 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - waiting: Set()
21:32:55.221 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - failed: Set()
21:32:55.221 [dag-scheduler-event-loop] DEBUG org.apache.spark.MapOutputTrackerMaster - Increasing epoch to 3
21:32:55.221 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - After removal of stage 2, remaining stages = 0
21:32:55.298 [ScalaTest-run-running-PhysicalPlanSpec] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 12.840399 ms
21:32:55.300 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.util.ClosureCleaner - Cleaning indylambda closure: $anonfun$doExecute$4$adapted
21:32:55.301 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.util.ClosureCleaner -  +++ indylambda closure ($anonfun$doExecute$4$adapted) is now cleaned +++
21:32:55.307 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.util.ClosureCleaner - Cleaning indylambda closure: $anonfun$doExecute$5
21:32:55.308 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.util.ClosureCleaner -  +++ indylambda closure ($anonfun$doExecute$5) is now cleaned +++
21:32:55.311 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.util.ClosureCleaner - Cleaning indylambda closure: $anonfun$doExecute$6$adapted
21:32:55.312 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.util.ClosureCleaner -  +++ indylambda closure ($anonfun$doExecute$6$adapted) is now cleaned +++
21:32:55.322 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.scheduler.DAGScheduler - eagerlyComputePartitionsForRddAndAncestors for RDD 18 took 0.000178 seconds
21:32:55.322 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - Merging stage rdd profiles: Set()
21:32:55.323 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - Merging stage rdd profiles: Set()
21:32:55.323 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - Merging stage rdd profiles: Set()
21:32:55.323 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - Considering join on: Some((((user_id#61L = user_id#70L) AND (region_id#62L = region_id#71L)) AND (region_id#62L = region_id#78L)))
21:32:55.323 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - leftKeys:List(user_id#70L, region_id#71L, region_id#78L) | rightKeys:List(user_id#61L, region_id#62L, region_id#62L)
21:32:55.323 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Registering RDD 18 (saveAsTable at PhysicalPlanSpec.scala:338) as input to shuffle 3
21:32:55.324 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got map stage job 3 (saveAsTable at PhysicalPlanSpec.scala:338) with 1 output partitions
21:32:55.324 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ShuffleMapStage 5 (saveAsTable at PhysicalPlanSpec.scala:338)
21:32:55.324 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 3, ShuffleMapStage 4)
21:32:55.324 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
21:32:55.324 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - submitStage(ShuffleMapStage 5 (name=saveAsTable at PhysicalPlanSpec.scala:338;jobs=3))
21:32:55.325 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - missing: List()
SparkPlanner(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2@321bf4b1) - org.apache.spark.sql.execution.SparkStrategies$SpecialLimits$ -->【
ReturnAnswer
+- Project [order_id#60L, user_id#61L, region_id#62L, order_num#63, user_name#72, region_name#79]
   +- Join Inner, (((user_id#61L = user_id#70L) AND (region_id#62L = region_id#71L)) AND (region_id#62L = region_id#78L))
      :- LogicalQueryStage Join Inner, (region_id#71L = region_id#78L), ShuffleQueryStage 3
      +- LogicalQueryStage Project [id#58L AS order_id#60L, (id#58L % 1000) AS user_id#61L, (id#58L % 10) AS region_id#62L, cast(id#58L as string) AS order_num#63], ShuffleQueryStage 2
】 --> 【
PlanLater Project [order_id#60L, user_id#61L, region_id#62L, order_num#63, user_name#72, region_name#79]
】

21:32:55.325 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 5 (MapPartitionsRDD[18] at saveAsTable at PhysicalPlanSpec.scala:338), which has no missing parents
21:32:55.325 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - submitMissingTasks(ShuffleMapStage 5)
SparkPlanner(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2@321bf4b1) - org.apache.spark.sql.execution.SparkStrategies$BasicOperators$ -->【
Project [order_id#60L, user_id#61L, region_id#62L, order_num#63, user_name#72, region_name#79]
+- Join Inner, (((user_id#61L = user_id#70L) AND (region_id#62L = region_id#71L)) AND (region_id#62L = region_id#78L))
   :- LogicalQueryStage Join Inner, (region_id#71L = region_id#78L), ShuffleQueryStage 3
   +- LogicalQueryStage Project [id#58L AS order_id#60L, (id#58L % 1000) AS user_id#61L, (id#58L % 10) AS region_id#62L, cast(id#58L as string) AS order_num#63], ShuffleQueryStage 2
】 --> 【
Project [order_id#60L, user_id#61L, region_id#62L, order_num#63, user_name#72, region_name#79]
+- PlanLater Join Inner, (((user_id#61L = user_id#70L) AND (region_id#62L = region_id#71L)) AND (region_id#62L = region_id#78L))
】

21:32:55.326 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - Considering join on: Some((((user_id#61L = user_id#70L) AND (region_id#62L = region_id#71L)) AND (region_id#62L = region_id#78L)))
21:32:55.326 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - leftKeys:List(user_id#70L, region_id#71L, region_id#78L) | rightKeys:List(user_id#61L, region_id#62L, region_id#62L)
21:32:55.326 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - Considering join on: Some((((user_id#61L = user_id#70L) AND (region_id#62L = region_id#71L)) AND (region_id#62L = region_id#78L)))
21:32:55.326 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - leftKeys:List(user_id#70L, region_id#71L, region_id#78L) | rightKeys:List(user_id#61L, region_id#62L, region_id#62L)
21:32:55.327 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.execution.adaptive.LogicalQueryStage - Physical stats not available for plan: ShuffleQueryStage 3
+- Exchange hashpartitioning(user_id#70L, region_id#71L, region_id#71L, 8), ENSURE_REQUIREMENTS, [plan_id=223]
   +- *(6) SortMergeJoin [region_id#71L], [region_id#78L], Inner
      :- *(4) Sort [region_id#71L ASC NULLS FIRST], false, 0
      :  +- AQEShuffleRead coalesced
      :     +- ShuffleQueryStage 0
      :        +- Exchange hashpartitioning(region_id#71L, 8), ENSURE_REQUIREMENTS, [plan_id=117]
      :           +- *(1) Project [id#68L AS user_id#70L, (id#68L % 10) AS region_id#71L, cast(id#68L as string) AS user_name#72]
      :              +- *(1) Filter isnotnull((id#68L % 10))
      :                 +- *(1) Range (0, 1000, step=1, splits=5)
      +- *(5) Sort [region_id#78L ASC NULLS FIRST], false, 0
         +- AQEShuffleRead coalesced
            +- ShuffleQueryStage 1
               +- Exchange hashpartitioning(region_id#78L, 8), ENSURE_REQUIREMENTS, [plan_id=124]
                  +- *(2) Project [id#76L AS region_id#78L, cast(id#76L as string) AS region_name#79]
                     +- *(2) Range (0, 10, step=1, splits=1)

21:32:55.327 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.execution.adaptive.LogicalQueryStage - Physical stats available as Statistics(sizeInBytes=468.8 KiB, rowCount=1.00E+4) for plan: ShuffleQueryStage 2
+- Exchange hashpartitioning(user_id#61L, region_id#62L, region_id#62L, 8), ENSURE_REQUIREMENTS, [plan_id=141]
   +- *(3) Project [id#58L AS order_id#60L, (id#58L % 1000) AS user_id#61L, (id#58L % 10) AS region_id#62L, cast(id#58L as string) AS order_num#63]
      +- *(3) Filter (isnotnull((id#58L % 1000)) AND isnotnull((id#58L % 10)))
         +- *(3) Range (0, 10000, step=1, splits=10)

SparkPlanner(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2@321bf4b1) - org.apache.spark.sql.execution.SparkStrategies$JoinSelection$ -->【
Join Inner, (((user_id#61L = user_id#70L) AND (region_id#62L = region_id#71L)) AND (region_id#62L = region_id#78L))
:- LogicalQueryStage Join Inner, (region_id#71L = region_id#78L), ShuffleQueryStage 3
+- LogicalQueryStage Project [id#58L AS order_id#60L, (id#58L % 1000) AS user_id#61L, (id#58L % 10) AS region_id#62L, cast(id#58L as string) AS order_num#63], ShuffleQueryStage 2
】 --> 【
SortMergeJoin [user_id#70L, region_id#71L, region_id#78L], [user_id#61L, region_id#62L, region_id#62L], Inner
:- PlanLater LogicalQueryStage Join Inner, (region_id#71L = region_id#78L), ShuffleQueryStage 3
+- PlanLater LogicalQueryStage Project [id#58L AS order_id#60L, (id#58L % 1000) AS user_id#61L, (id#58L % 10) AS region_id#62L, cast(id#58L as string) AS order_num#63], ShuffleQueryStage 2
】

SparkPlanner(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2@321bf4b1) - org.apache.spark.sql.execution.adaptive.LogicalQueryStageStrategy$ -->【
LogicalQueryStage Join Inner, (region_id#71L = region_id#78L), ShuffleQueryStage 3
】 --> 【
ShuffleQueryStage 3
+- Exchange hashpartitioning(user_id#70L, region_id#71L, region_id#71L, 8), ENSURE_REQUIREMENTS, [plan_id=223]
   +- *(6) SortMergeJoin [region_id#71L], [region_id#78L], Inner
      :- *(4) Sort [region_id#71L ASC NULLS FIRST], false, 0
      :  +- AQEShuffleRead coalesced
      :     +- ShuffleQueryStage 0
      :        +- Exchange hashpartitioning(region_id#71L, 8), ENSURE_REQUIREMENTS, [plan_id=117]
      :           +- *(1) Project [id#68L AS user_id#70L, (id#68L % 10) AS region_id#71L, cast(id#68L as string) AS user_name#72]
      :              +- *(1) Filter isnotnull((id#68L % 10))
      :                 +- *(1) Range (0, 1000, step=1, splits=5)
      +- *(5) Sort [region_id#78L ASC NULLS FIRST], false, 0
         +- AQEShuffleRead coalesced
            +- ShuffleQueryStage 1
               +- Exchange hashpartitioning(region_id#78L, 8), ENSURE_REQUIREMENTS, [plan_id=124]
                  +- *(2) Project [id#76L AS region_id#78L, cast(id#76L as string) AS region_name#79]
                     +- *(2) Range (0, 10, step=1, splits=1)
】

SparkPlanner(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2@321bf4b1) - org.apache.spark.sql.execution.adaptive.LogicalQueryStageStrategy$ -->【
LogicalQueryStage Project [id#58L AS order_id#60L, (id#58L % 1000) AS user_id#61L, (id#58L % 10) AS region_id#62L, cast(id#58L as string) AS order_num#63], ShuffleQueryStage 2
】 --> 【
ShuffleQueryStage 2
+- Exchange hashpartitioning(user_id#61L, region_id#62L, region_id#62L, 8), ENSURE_REQUIREMENTS, [plan_id=141]
   +- *(3) Project [id#58L AS order_id#60L, (id#58L % 1000) AS user_id#61L, (id#58L % 10) AS region_id#62L, cast(id#58L as string) AS order_num#63]
      +- *(3) Filter (isnotnull((id#58L % 1000)) AND isnotnull((id#58L % 10)))
         +- *(3) Range (0, 10000, step=1, splits=10)
】

org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec$ - applyPhysicalRules - org.apache.spark.sql.execution.exchange.EnsureRequirements -->【
Project [order_id#60L, user_id#61L, region_id#62L, order_num#63, user_name#72, region_name#79]
+- SortMergeJoin [user_id#70L, region_id#71L, region_id#78L], [user_id#61L, region_id#62L, region_id#62L], Inner
   :- ShuffleQueryStage 3
   :  +- Exchange hashpartitioning(user_id#70L, region_id#71L, region_id#71L, 8), ENSURE_REQUIREMENTS, [plan_id=223]
   :     +- *(6) SortMergeJoin [region_id#71L], [region_id#78L], Inner
   :        :- *(4) Sort [region_id#71L ASC NULLS FIRST], false, 0
   :        :  +- AQEShuffleRead coalesced
   :        :     +- ShuffleQueryStage 0
   :        :        +- Exchange hashpartitioning(region_id#71L, 8), ENSURE_REQUIREMENTS, [plan_id=117]
   :        :           +- *(1) Project [id#68L AS user_id#70L, (id#68L % 10) AS region_id#71L, cast(id#68L as string) AS user_name#72]
   :        :              +- *(1) Filter isnotnull((id#68L % 10))
   :        :                 +- *(1) Range (0, 1000, step=1, splits=5)
   :        +- *(5) Sort [region_id#78L ASC NULLS FIRST], false, 0
   :           +- AQEShuffleRead coalesced
   :              +- ShuffleQueryStage 1
   :                 +- Exchange hashpartitioning(region_id#78L, 8), ENSURE_REQUIREMENTS, [plan_id=124]
   :                    +- *(2) Project [id#76L AS region_id#78L, cast(id#76L as string) AS region_name#79]
   :                       +- *(2) Range (0, 10, step=1, splits=1)
   +- ShuffleQueryStage 2
      +- Exchange hashpartitioning(user_id#61L, region_id#62L, region_id#62L, 8), ENSURE_REQUIREMENTS, [plan_id=141]
         +- *(3) Project [id#58L AS order_id#60L, (id#58L % 1000) AS user_id#61L, (id#58L % 10) AS region_id#62L, cast(id#58L as string) AS order_num#63]
            +- *(3) Filter (isnotnull((id#58L % 1000)) AND isnotnull((id#58L % 10)))
               +- *(3) Range (0, 10000, step=1, splits=10)
】AQE --> 【
Project [order_id#60L, user_id#61L, region_id#62L, order_num#63, user_name#72, region_name#79]
+- SortMergeJoin [user_id#70L, region_id#71L, region_id#78L], [user_id#61L, region_id#62L, region_id#62L], Inner
   :- Sort [user_id#70L ASC NULLS FIRST, region_id#71L ASC NULLS FIRST, region_id#78L ASC NULLS FIRST], false, 0
   :  +- ShuffleQueryStage 3
   :     +- Exchange hashpartitioning(user_id#70L, region_id#71L, region_id#71L, 8), ENSURE_REQUIREMENTS, [plan_id=223]
   :        +- *(6) SortMergeJoin [region_id#71L], [region_id#78L], Inner
   :           :- *(4) Sort [region_id#71L ASC NULLS FIRST], false, 0
   :           :  +- AQEShuffleRead coalesced
   :           :     +- ShuffleQueryStage 0
   :           :        +- Exchange hashpartitioning(region_id#71L, 8), ENSURE_REQUIREMENTS, [plan_id=117]
   :           :           +- *(1) Project [id#68L AS user_id#70L, (id#68L % 10) AS region_id#71L, cast(id#68L as string) AS user_name#72]
   :           :              +- *(1) Filter isnotnull((id#68L % 10))
   :           :                 +- *(1) Range (0, 1000, step=1, splits=5)
   :           +- *(5) Sort [region_id#78L ASC NULLS FIRST], false, 0
   :              +- AQEShuffleRead coalesced
   :                 +- ShuffleQueryStage 1
   :                    +- Exchange hashpartitioning(region_id#78L, 8), ENSURE_REQUIREMENTS, [plan_id=124]
   :                       +- *(2) Project [id#76L AS region_id#78L, cast(id#76L as string) AS region_name#79]
   :                          +- *(2) Range (0, 10, step=1, splits=1)
   +- Sort [user_id#61L ASC NULLS FIRST, region_id#62L ASC NULLS FIRST, region_id#62L ASC NULLS FIRST], false, 0
      +- ShuffleQueryStage 2
         +- Exchange hashpartitioning(user_id#61L, region_id#62L, region_id#62L, 8), ENSURE_REQUIREMENTS, [plan_id=141]
            +- *(3) Project [id#58L AS order_id#60L, (id#58L % 1000) AS user_id#61L, (id#58L % 10) AS region_id#62L, cast(id#58L as string) AS order_num#63]
               +- *(3) Filter (isnotnull((id#58L % 1000)) AND isnotnull((id#58L % 10)))
                  +- *(3) Range (0, 10000, step=1, splits=10)
】

org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec$ final result ==>【
Project [order_id#60L, user_id#61L, region_id#62L, order_num#63, user_name#72, region_name#79]
+- SortMergeJoin [user_id#70L, region_id#71L, region_id#78L], [user_id#61L, region_id#62L, region_id#62L], Inner
   :- ShuffleQueryStage 3
   :  +- Exchange hashpartitioning(user_id#70L, region_id#71L, region_id#71L, 8), ENSURE_REQUIREMENTS, [plan_id=223]
   :     +- *(6) SortMergeJoin [region_id#71L], [region_id#78L], Inner
   :        :- *(4) Sort [region_id#71L ASC NULLS FIRST], false, 0
   :        :  +- AQEShuffleRead coalesced
   :        :     +- ShuffleQueryStage 0
   :        :        +- Exchange hashpartitioning(region_id#71L, 8), ENSURE_REQUIREMENTS, [plan_id=117]
   :        :           +- *(1) Project [id#68L AS user_id#70L, (id#68L % 10) AS region_id#71L, cast(id#68L as string) AS user_name#72]
   :        :              +- *(1) Filter isnotnull((id#68L % 10))
   :        :                 +- *(1) Range (0, 1000, step=1, splits=5)
   :        +- *(5) Sort [region_id#78L ASC NULLS FIRST], false, 0
   :           +- AQEShuffleRead coalesced
   :              +- ShuffleQueryStage 1
   :                 +- Exchange hashpartitioning(region_id#78L, 8), ENSURE_REQUIREMENTS, [plan_id=124]
   :                    +- *(2) Project [id#76L AS region_id#78L, cast(id#76L as string) AS region_name#79]
   :                       +- *(2) Range (0, 10, step=1, splits=1)
   +- ShuffleQueryStage 2
      +- Exchange hashpartitioning(user_id#61L, region_id#62L, region_id#62L, 8), ENSURE_REQUIREMENTS, [plan_id=141]
         +- *(3) Project [id#58L AS order_id#60L, (id#58L % 1000) AS user_id#61L, (id#58L % 10) AS region_id#62L, cast(id#58L as string) AS order_num#63]
            +- *(3) Filter (isnotnull((id#58L % 1000)) AND isnotnull((id#58L % 10)))
               +- *(3) Range (0, 10000, step=1, splits=10)
】AQE ==> 【
Project [order_id#60L, user_id#61L, region_id#62L, order_num#63, user_name#72, region_name#79]
+- SortMergeJoin [user_id#70L, region_id#71L, region_id#78L], [user_id#61L, region_id#62L, region_id#62L], Inner
   :- Sort [user_id#70L ASC NULLS FIRST, region_id#71L ASC NULLS FIRST, region_id#78L ASC NULLS FIRST], false, 0
   :  +- ShuffleQueryStage 3
   :     +- Exchange hashpartitioning(user_id#70L, region_id#71L, region_id#71L, 8), ENSURE_REQUIREMENTS, [plan_id=223]
   :        +- *(6) SortMergeJoin [region_id#71L], [region_id#78L], Inner
   :           :- *(4) Sort [region_id#71L ASC NULLS FIRST], false, 0
   :           :  +- AQEShuffleRead coalesced
   :           :     +- ShuffleQueryStage 0
   :           :        +- Exchange hashpartitioning(region_id#71L, 8), ENSURE_REQUIREMENTS, [plan_id=117]
   :           :           +- *(1) Project [id#68L AS user_id#70L, (id#68L % 10) AS region_id#71L, cast(id#68L as string) AS user_name#72]
   :           :              +- *(1) Filter isnotnull((id#68L % 10))
   :           :                 +- *(1) Range (0, 1000, step=1, splits=5)
   :           +- *(5) Sort [region_id#78L ASC NULLS FIRST], false, 0
   :              +- AQEShuffleRead coalesced
   :                 +- ShuffleQueryStage 1
   :                    +- Exchange hashpartitioning(region_id#78L, 8), ENSURE_REQUIREMENTS, [plan_id=124]
   :                       +- *(2) Project [id#76L AS region_id#78L, cast(id#76L as string) AS region_name#79]
   :                          +- *(2) Range (0, 10, step=1, splits=1)
   +- Sort [user_id#61L ASC NULLS FIRST, region_id#62L ASC NULLS FIRST, region_id#62L ASC NULLS FIRST], false, 0
      +- ShuffleQueryStage 2
         +- Exchange hashpartitioning(user_id#61L, region_id#62L, region_id#62L, 8), ENSURE_REQUIREMENTS, [plan_id=141]
            +- *(3) Project [id#58L AS order_id#60L, (id#58L % 1000) AS user_id#61L, (id#58L % 10) AS region_id#62L, cast(id#58L as string) AS order_num#63]
               +- *(3) Filter (isnotnull((id#58L % 1000)) AND isnotnull((id#58L % 10)))
                  +- *(3) Range (0, 10000, step=1, splits=10)
】

21:32:55.343 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_3 stored as values in memory (estimated size 38.0 KiB, free 4.1 GiB)
21:32:55.343 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager - Put block broadcast_3 locally took 0 ms
21:32:55.343 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager - Putting block broadcast_3 without replication took 0 ms
21:32:55.344 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_3_piece0 stored as bytes in memory (estimated size 17.2 KiB, free 4.1 GiB)
21:32:55.344 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_3_piece0 for BlockManagerId(driver, 192.168.31.69, 63737, None)
21:32:55.344 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_3_piece0 in memory on 192.168.31.69:63737 (size: 17.2 KiB, free: 4.1 GiB)
21:32:55.344 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_3_piece0
21:32:55.344 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager - Told master about block broadcast_3_piece0
21:32:55.344 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager - Put block broadcast_3_piece0 locally took 0 ms
21:32:55.344 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager - Putting block broadcast_3_piece0 without replication took 0 ms
21:32:55.344 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 3 from broadcast at DAGScheduler.scala:1513
21:32:55.345 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 5 (MapPartitionsRDD[18] at saveAsTable at PhysicalPlanSpec.scala:338) (first 15 tasks are for partitions Vector(0))
21:32:55.345 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 5.0 with 1 tasks resource profile 0
21:32:55.345 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.TaskSetManager - Epoch for TaskSet 5.0: 3
21:32:55.347 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.TaskSetManager - Adding pending tasks took 1 ms
21:32:55.348 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.TaskSetManager - Valid locality levels for TaskSet 5.0: NODE_LOCAL, ANY
21:32:55.348 [dispatcher-event-loop-0] DEBUG org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_5.0, runningTasks: 0
21:32:55.351 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 5.0 (TID 16) (192.168.31.69, executor driver, partition 0, NODE_LOCAL, 4724 bytes) taskResourceAssignments Map()
21:32:55.351 [Executor task launch worker for task 0.0 in stage 5.0 (TID 16)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 5.0 (TID 16)
21:32:55.352 [Executor task launch worker for task 0.0 in stage 5.0 (TID 16)] DEBUG org.apache.spark.executor.ExecutorMetricsPoller - stageTCMP: (5, 0) -> 1
21:32:55.352 [Executor task launch worker for task 0.0 in stage 5.0 (TID 16)] DEBUG org.apache.spark.storage.BlockManager - Getting local block broadcast_3
21:32:55.352 [Executor task launch worker for task 0.0 in stage 5.0 (TID 16)] DEBUG org.apache.spark.storage.BlockManager - Level for block broadcast_3 is StorageLevel(disk, memory, deserialized, 1 replicas)
21:32:55.361 [Executor task launch worker for task 0.0 in stage 5.0 (TID 16)] DEBUG org.apache.spark.MapOutputTrackerMaster - Fetching outputs for shuffle 0
21:32:55.366 [Executor task launch worker for task 0.0 in stage 5.0 (TID 16)] DEBUG org.apache.spark.MapOutputTrackerMaster - Convert map statuses for shuffle 0, mappers 0-5, partitions 0-8
21:32:55.389 [Executor task launch worker for task 0.0 in stage 5.0 (TID 16)] DEBUG org.apache.spark.storage.ShuffleBlockFetcherIterator - maxBytesInFlight: 50331648, targetRemoteRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
21:32:55.396 [Executor task launch worker for task 0.0 in stage 5.0 (TID 16)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 5 (14.7 KiB) non-empty blocks including 5 (14.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
21:32:55.397 [Executor task launch worker for task 0.0 in stage 5.0 (TID 16)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 13 ms
21:32:55.398 [Executor task launch worker for task 0.0 in stage 5.0 (TID 16)] DEBUG org.apache.spark.storage.ShuffleBlockFetcherIterator - Start fetching local blocks: (shuffle_0_0_0_6,0), (shuffle_0_1_0_6,1), (shuffle_0_2_0_6,2), (shuffle_0_3_0_6,3), (shuffle_0_4_0_6,4)
21:32:55.398 [Executor task launch worker for task 0.0 in stage 5.0 (TID 16)] DEBUG org.apache.spark.storage.BlockManager - Getting local shuffle block shuffle_0_0_0_6
21:32:55.405 [Executor task launch worker for task 0.0 in stage 5.0 (TID 16)] DEBUG org.apache.spark.storage.BlockManager - Getting local shuffle block shuffle_0_1_0_6
21:32:55.405 [Executor task launch worker for task 0.0 in stage 5.0 (TID 16)] DEBUG org.apache.spark.storage.BlockManager - Getting local shuffle block shuffle_0_2_0_6
21:32:55.406 [Executor task launch worker for task 0.0 in stage 5.0 (TID 16)] DEBUG org.apache.spark.storage.BlockManager - Getting local shuffle block shuffle_0_3_0_6
21:32:55.406 [Executor task launch worker for task 0.0 in stage 5.0 (TID 16)] DEBUG org.apache.spark.storage.BlockManager - Getting local shuffle block shuffle_0_4_0_6
21:32:55.406 [Executor task launch worker for task 0.0 in stage 5.0 (TID 16)] DEBUG org.apache.spark.storage.ShuffleBlockFetcherIterator - Got local blocks in 22 ms
21:32:55.441 [Executor task launch worker for task 0.0 in stage 5.0 (TID 16)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 24.257522 ms
21:32:55.472 [Executor task launch worker for task 0.0 in stage 5.0 (TID 16)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 8.668508 ms
21:32:55.490 [Executor task launch worker for task 0.0 in stage 5.0 (TID 16)] DEBUG org.apache.spark.memory.TaskMemoryManager - Task 16 acquired 64.0 KiB for org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@119513fc
21:32:55.494 [Executor task launch worker for task 0.0 in stage 5.0 (TID 16)] DEBUG org.apache.spark.MapOutputTrackerMaster - Fetching outputs for shuffle 1
21:32:55.494 [Executor task launch worker for task 0.0 in stage 5.0 (TID 16)] DEBUG org.apache.spark.MapOutputTrackerMaster - Convert map statuses for shuffle 1, mappers 0-1, partitions 0-8
21:32:55.494 [Executor task launch worker for task 0.0 in stage 5.0 (TID 16)] DEBUG org.apache.spark.storage.ShuffleBlockFetcherIterator - maxBytesInFlight: 50331648, targetRemoteRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
21:32:55.495 [Executor task launch worker for task 0.0 in stage 5.0 (TID 16)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 (444.0 B) non-empty blocks including 1 (444.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
21:32:55.495 [Executor task launch worker for task 0.0 in stage 5.0 (TID 16)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
21:32:55.495 [Executor task launch worker for task 0.0 in stage 5.0 (TID 16)] DEBUG org.apache.spark.storage.ShuffleBlockFetcherIterator - Start fetching local blocks: (shuffle_1_5_0_6,0)
21:32:55.495 [Executor task launch worker for task 0.0 in stage 5.0 (TID 16)] DEBUG org.apache.spark.storage.BlockManager - Getting local shuffle block shuffle_1_5_0_6
21:32:55.495 [Executor task launch worker for task 0.0 in stage 5.0 (TID 16)] DEBUG org.apache.spark.storage.ShuffleBlockFetcherIterator - Got local blocks in 1 ms
21:32:55.507 [Executor task launch worker for task 0.0 in stage 5.0 (TID 16)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 8.608675 ms
21:32:55.520 [Executor task launch worker for task 0.0 in stage 5.0 (TID 16)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 9.904273 ms
21:32:55.522 [Executor task launch worker for task 0.0 in stage 5.0 (TID 16)] DEBUG org.apache.spark.memory.TaskMemoryManager - Task 16 acquired 64.0 KiB for org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@33b887b2
21:32:55.545 [Executor task launch worker for task 0.0 in stage 5.0 (TID 16)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 14.499408 ms
21:32:55.561 [Executor task launch worker for task 0.0 in stage 5.0 (TID 16)] DEBUG org.apache.spark.memory.TaskMemoryManager - Task 16 acquired 64.0 MiB for org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@119513fc
21:32:55.583 [Executor task launch worker for task 0.0 in stage 5.0 (TID 16)] DEBUG org.apache.spark.memory.TaskMemoryManager - Task 16 acquired 64.0 MiB for org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@33b887b2
21:32:55.601 [Executor task launch worker for task 0.0 in stage 5.0 (TID 16)] DEBUG org.apache.spark.memory.TaskMemoryManager - Task 16 release 64.0 MiB from org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@33b887b2
21:32:55.602 [Executor task launch worker for task 0.0 in stage 5.0 (TID 16)] DEBUG org.apache.spark.memory.TaskMemoryManager - Task 16 release 64.0 KiB from org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@33b887b2
21:32:55.603 [Executor task launch worker for task 0.0 in stage 5.0 (TID 16)] DEBUG org.apache.spark.memory.TaskMemoryManager - Task 16 release 64.0 MiB from org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@119513fc
21:32:55.603 [Executor task launch worker for task 0.0 in stage 5.0 (TID 16)] DEBUG org.apache.spark.memory.TaskMemoryManager - Task 16 release 64.0 KiB from org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@119513fc
21:32:55.606 [Executor task launch worker for task 0.0 in stage 5.0 (TID 16)] DEBUG org.apache.spark.shuffle.sort.io.LocalDiskShuffleMapOutputWriter - Writing shuffle index file for mapId 16 with length 8
21:32:55.607 [Executor task launch worker for task 0.0 in stage 5.0 (TID 16)] DEBUG org.apache.spark.shuffle.IndexShuffleBlockResolver - Shuffle index for mapId 16: [2465,2479,2680,2467,2060,2393,2491,2656]
21:32:55.609 [Executor task launch worker for task 0.0 in stage 5.0 (TID 16)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 5.0 (TID 16). 4391 bytes result sent to driver
21:32:55.609 [Executor task launch worker for task 0.0 in stage 5.0 (TID 16)] DEBUG org.apache.spark.executor.ExecutorMetricsPoller - stageTCMP: (5, 0) -> 0
21:32:55.609 [dispatcher-event-loop-0] DEBUG org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_5.0, runningTasks: 0
21:32:55.609 [dispatcher-event-loop-0] DEBUG org.apache.spark.scheduler.TaskSetManager - No tasks for locality level NODE_LOCAL, so moving to locality level ANY
21:32:55.609 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 5.0 (TID 16) in 261 ms on 192.168.31.69 (executor driver) (1/1)
21:32:55.610 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 5.0, whose tasks have all completed, from pool
21:32:55.610 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - ShuffleMapTask finished on driver
21:32:55.611 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 5 (saveAsTable at PhysicalPlanSpec.scala:338) finished in 0.275 s
21:32:55.611 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
21:32:55.611 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - running: Set()
21:32:55.611 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - waiting: Set()
21:32:55.611 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - failed: Set()
21:32:55.611 [dag-scheduler-event-loop] DEBUG org.apache.spark.MapOutputTrackerMaster - Increasing epoch to 4
21:32:55.611 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - After removal of stage 5, remaining stages = 2
21:32:55.611 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - After removal of stage 4, remaining stages = 1
21:32:55.611 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - After removal of stage 3, remaining stages = 0
21:32:55.613 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - Considering join on: Some((((user_id#61L = user_id#70L) AND (region_id#62L = region_id#71L)) AND (region_id#62L = region_id#78L)))
21:32:55.613 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - leftKeys:List(user_id#70L, region_id#71L, region_id#78L) | rightKeys:List(user_id#61L, region_id#62L, region_id#62L)
SparkPlanner(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2@321bf4b1) - org.apache.spark.sql.execution.SparkStrategies$SpecialLimits$ -->【
ReturnAnswer
+- Project [order_id#60L, user_id#61L, region_id#62L, order_num#63, user_name#72, region_name#79]
   +- Join Inner, (((user_id#61L = user_id#70L) AND (region_id#62L = region_id#71L)) AND (region_id#62L = region_id#78L))
      :- LogicalQueryStage Join Inner, (region_id#71L = region_id#78L), ShuffleQueryStage 3
      +- LogicalQueryStage Project [id#58L AS order_id#60L, (id#58L % 1000) AS user_id#61L, (id#58L % 10) AS region_id#62L, cast(id#58L as string) AS order_num#63], ShuffleQueryStage 2
】 --> 【
PlanLater Project [order_id#60L, user_id#61L, region_id#62L, order_num#63, user_name#72, region_name#79]
】

SparkPlanner(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2@321bf4b1) - org.apache.spark.sql.execution.SparkStrategies$BasicOperators$ -->【
Project [order_id#60L, user_id#61L, region_id#62L, order_num#63, user_name#72, region_name#79]
+- Join Inner, (((user_id#61L = user_id#70L) AND (region_id#62L = region_id#71L)) AND (region_id#62L = region_id#78L))
   :- LogicalQueryStage Join Inner, (region_id#71L = region_id#78L), ShuffleQueryStage 3
   +- LogicalQueryStage Project [id#58L AS order_id#60L, (id#58L % 1000) AS user_id#61L, (id#58L % 10) AS region_id#62L, cast(id#58L as string) AS order_num#63], ShuffleQueryStage 2
】 --> 【
Project [order_id#60L, user_id#61L, region_id#62L, order_num#63, user_name#72, region_name#79]
+- PlanLater Join Inner, (((user_id#61L = user_id#70L) AND (region_id#62L = region_id#71L)) AND (region_id#62L = region_id#78L))
】

21:32:55.616 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - Considering join on: Some((((user_id#61L = user_id#70L) AND (region_id#62L = region_id#71L)) AND (region_id#62L = region_id#78L)))
21:32:55.616 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - leftKeys:List(user_id#70L, region_id#71L, region_id#78L) | rightKeys:List(user_id#61L, region_id#62L, region_id#62L)
21:32:55.617 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - Considering join on: Some((((user_id#61L = user_id#70L) AND (region_id#62L = region_id#71L)) AND (region_id#62L = region_id#78L)))
21:32:55.617 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - leftKeys:List(user_id#70L, region_id#71L, region_id#78L) | rightKeys:List(user_id#61L, region_id#62L, region_id#62L)
21:32:55.618 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.execution.adaptive.LogicalQueryStage - Physical stats available as Statistics(sizeInBytes=62.5 KiB, rowCount=1.00E+3) for plan: ShuffleQueryStage 3
+- Exchange hashpartitioning(user_id#70L, region_id#71L, region_id#71L, 8), ENSURE_REQUIREMENTS, [plan_id=223]
   +- *(6) SortMergeJoin [region_id#71L], [region_id#78L], Inner
      :- *(4) Sort [region_id#71L ASC NULLS FIRST], false, 0
      :  +- AQEShuffleRead coalesced
      :     +- ShuffleQueryStage 0
      :        +- Exchange hashpartitioning(region_id#71L, 8), ENSURE_REQUIREMENTS, [plan_id=117]
      :           +- *(1) Project [id#68L AS user_id#70L, (id#68L % 10) AS region_id#71L, cast(id#68L as string) AS user_name#72]
      :              +- *(1) Filter isnotnull((id#68L % 10))
      :                 +- *(1) Range (0, 1000, step=1, splits=5)
      +- *(5) Sort [region_id#78L ASC NULLS FIRST], false, 0
         +- AQEShuffleRead coalesced
            +- ShuffleQueryStage 1
               +- Exchange hashpartitioning(region_id#78L, 8), ENSURE_REQUIREMENTS, [plan_id=124]
                  +- *(2) Project [id#76L AS region_id#78L, cast(id#76L as string) AS region_name#79]
                     +- *(2) Range (0, 10, step=1, splits=1)

21:32:55.619 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.execution.adaptive.LogicalQueryStage - Physical stats available as Statistics(sizeInBytes=468.8 KiB, rowCount=1.00E+4) for plan: ShuffleQueryStage 2
+- Exchange hashpartitioning(user_id#61L, region_id#62L, region_id#62L, 8), ENSURE_REQUIREMENTS, [plan_id=141]
   +- *(3) Project [id#58L AS order_id#60L, (id#58L % 1000) AS user_id#61L, (id#58L % 10) AS region_id#62L, cast(id#58L as string) AS order_num#63]
      +- *(3) Filter (isnotnull((id#58L % 1000)) AND isnotnull((id#58L % 10)))
         +- *(3) Range (0, 10000, step=1, splits=10)

SparkPlanner(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2@321bf4b1) - org.apache.spark.sql.execution.SparkStrategies$JoinSelection$ -->【
Join Inner, (((user_id#61L = user_id#70L) AND (region_id#62L = region_id#71L)) AND (region_id#62L = region_id#78L))
:- LogicalQueryStage Join Inner, (region_id#71L = region_id#78L), ShuffleQueryStage 3
+- LogicalQueryStage Project [id#58L AS order_id#60L, (id#58L % 1000) AS user_id#61L, (id#58L % 10) AS region_id#62L, cast(id#58L as string) AS order_num#63], ShuffleQueryStage 2
】 --> 【
SortMergeJoin [user_id#70L, region_id#71L, region_id#78L], [user_id#61L, region_id#62L, region_id#62L], Inner
:- PlanLater LogicalQueryStage Join Inner, (region_id#71L = region_id#78L), ShuffleQueryStage 3
+- PlanLater LogicalQueryStage Project [id#58L AS order_id#60L, (id#58L % 1000) AS user_id#61L, (id#58L % 10) AS region_id#62L, cast(id#58L as string) AS order_num#63], ShuffleQueryStage 2
】

SparkPlanner(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2@321bf4b1) - org.apache.spark.sql.execution.adaptive.LogicalQueryStageStrategy$ -->【
LogicalQueryStage Join Inner, (region_id#71L = region_id#78L), ShuffleQueryStage 3
】 --> 【
ShuffleQueryStage 3
+- Exchange hashpartitioning(user_id#70L, region_id#71L, region_id#71L, 8), ENSURE_REQUIREMENTS, [plan_id=223]
   +- *(6) SortMergeJoin [region_id#71L], [region_id#78L], Inner
      :- *(4) Sort [region_id#71L ASC NULLS FIRST], false, 0
      :  +- AQEShuffleRead coalesced
      :     +- ShuffleQueryStage 0
      :        +- Exchange hashpartitioning(region_id#71L, 8), ENSURE_REQUIREMENTS, [plan_id=117]
      :           +- *(1) Project [id#68L AS user_id#70L, (id#68L % 10) AS region_id#71L, cast(id#68L as string) AS user_name#72]
      :              +- *(1) Filter isnotnull((id#68L % 10))
      :                 +- *(1) Range (0, 1000, step=1, splits=5)
      +- *(5) Sort [region_id#78L ASC NULLS FIRST], false, 0
         +- AQEShuffleRead coalesced
            +- ShuffleQueryStage 1
               +- Exchange hashpartitioning(region_id#78L, 8), ENSURE_REQUIREMENTS, [plan_id=124]
                  +- *(2) Project [id#76L AS region_id#78L, cast(id#76L as string) AS region_name#79]
                     +- *(2) Range (0, 10, step=1, splits=1)
】

SparkPlanner(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2@321bf4b1) - org.apache.spark.sql.execution.adaptive.LogicalQueryStageStrategy$ -->【
LogicalQueryStage Project [id#58L AS order_id#60L, (id#58L % 1000) AS user_id#61L, (id#58L % 10) AS region_id#62L, cast(id#58L as string) AS order_num#63], ShuffleQueryStage 2
】 --> 【
ShuffleQueryStage 2
+- Exchange hashpartitioning(user_id#61L, region_id#62L, region_id#62L, 8), ENSURE_REQUIREMENTS, [plan_id=141]
   +- *(3) Project [id#58L AS order_id#60L, (id#58L % 1000) AS user_id#61L, (id#58L % 10) AS region_id#62L, cast(id#58L as string) AS order_num#63]
      +- *(3) Filter (isnotnull((id#58L % 1000)) AND isnotnull((id#58L % 10)))
         +- *(3) Range (0, 10000, step=1, splits=10)
】

org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec$ - applyPhysicalRules - org.apache.spark.sql.execution.exchange.EnsureRequirements -->【
Project [order_id#60L, user_id#61L, region_id#62L, order_num#63, user_name#72, region_name#79]
+- SortMergeJoin [user_id#70L, region_id#71L, region_id#78L], [user_id#61L, region_id#62L, region_id#62L], Inner
   :- ShuffleQueryStage 3
   :  +- Exchange hashpartitioning(user_id#70L, region_id#71L, region_id#71L, 8), ENSURE_REQUIREMENTS, [plan_id=223]
   :     +- *(6) SortMergeJoin [region_id#71L], [region_id#78L], Inner
   :        :- *(4) Sort [region_id#71L ASC NULLS FIRST], false, 0
   :        :  +- AQEShuffleRead coalesced
   :        :     +- ShuffleQueryStage 0
   :        :        +- Exchange hashpartitioning(region_id#71L, 8), ENSURE_REQUIREMENTS, [plan_id=117]
   :        :           +- *(1) Project [id#68L AS user_id#70L, (id#68L % 10) AS region_id#71L, cast(id#68L as string) AS user_name#72]
   :        :              +- *(1) Filter isnotnull((id#68L % 10))
   :        :                 +- *(1) Range (0, 1000, step=1, splits=5)
   :        +- *(5) Sort [region_id#78L ASC NULLS FIRST], false, 0
   :           +- AQEShuffleRead coalesced
   :              +- ShuffleQueryStage 1
   :                 +- Exchange hashpartitioning(region_id#78L, 8), ENSURE_REQUIREMENTS, [plan_id=124]
   :                    +- *(2) Project [id#76L AS region_id#78L, cast(id#76L as string) AS region_name#79]
   :                       +- *(2) Range (0, 10, step=1, splits=1)
   +- ShuffleQueryStage 2
      +- Exchange hashpartitioning(user_id#61L, region_id#62L, region_id#62L, 8), ENSURE_REQUIREMENTS, [plan_id=141]
         +- *(3) Project [id#58L AS order_id#60L, (id#58L % 1000) AS user_id#61L, (id#58L % 10) AS region_id#62L, cast(id#58L as string) AS order_num#63]
            +- *(3) Filter (isnotnull((id#58L % 1000)) AND isnotnull((id#58L % 10)))
               +- *(3) Range (0, 10000, step=1, splits=10)
】AQE --> 【
Project [order_id#60L, user_id#61L, region_id#62L, order_num#63, user_name#72, region_name#79]
+- SortMergeJoin [user_id#70L, region_id#71L, region_id#78L], [user_id#61L, region_id#62L, region_id#62L], Inner
   :- Sort [user_id#70L ASC NULLS FIRST, region_id#71L ASC NULLS FIRST, region_id#78L ASC NULLS FIRST], false, 0
   :  +- ShuffleQueryStage 3
   :     +- Exchange hashpartitioning(user_id#70L, region_id#71L, region_id#71L, 8), ENSURE_REQUIREMENTS, [plan_id=223]
   :        +- *(6) SortMergeJoin [region_id#71L], [region_id#78L], Inner
   :           :- *(4) Sort [region_id#71L ASC NULLS FIRST], false, 0
   :           :  +- AQEShuffleRead coalesced
   :           :     +- ShuffleQueryStage 0
   :           :        +- Exchange hashpartitioning(region_id#71L, 8), ENSURE_REQUIREMENTS, [plan_id=117]
   :           :           +- *(1) Project [id#68L AS user_id#70L, (id#68L % 10) AS region_id#71L, cast(id#68L as string) AS user_name#72]
   :           :              +- *(1) Filter isnotnull((id#68L % 10))
   :           :                 +- *(1) Range (0, 1000, step=1, splits=5)
   :           +- *(5) Sort [region_id#78L ASC NULLS FIRST], false, 0
   :              +- AQEShuffleRead coalesced
   :                 +- ShuffleQueryStage 1
   :                    +- Exchange hashpartitioning(region_id#78L, 8), ENSURE_REQUIREMENTS, [plan_id=124]
   :                       +- *(2) Project [id#76L AS region_id#78L, cast(id#76L as string) AS region_name#79]
   :                          +- *(2) Range (0, 10, step=1, splits=1)
   +- Sort [user_id#61L ASC NULLS FIRST, region_id#62L ASC NULLS FIRST, region_id#62L ASC NULLS FIRST], false, 0
      +- ShuffleQueryStage 2
         +- Exchange hashpartitioning(user_id#61L, region_id#62L, region_id#62L, 8), ENSURE_REQUIREMENTS, [plan_id=141]
            +- *(3) Project [id#58L AS order_id#60L, (id#58L % 1000) AS user_id#61L, (id#58L % 10) AS region_id#62L, cast(id#58L as string) AS order_num#63]
               +- *(3) Filter (isnotnull((id#58L % 1000)) AND isnotnull((id#58L % 10)))
                  +- *(3) Range (0, 10000, step=1, splits=10)
】

21:32:55.626 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.execution.adaptive.OptimizeSkewedJoin -
Optimizing skewed join.
Left side partitions size info:
median size: 2602, max size: 2726, min size: 2253, avg size: 2573
Right side partitions size info:
median size: 23655, max size: 27012, min size: 18992, avg size: 23742

21:32:55.627 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.execution.adaptive.OptimizeSkewedJoin - number of skewed partitions: left 0, right 0
org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec$ final result ==>【
Project [order_id#60L, user_id#61L, region_id#62L, order_num#63, user_name#72, region_name#79]
+- SortMergeJoin [user_id#70L, region_id#71L, region_id#78L], [user_id#61L, region_id#62L, region_id#62L], Inner
   :- ShuffleQueryStage 3
   :  +- Exchange hashpartitioning(user_id#70L, region_id#71L, region_id#71L, 8), ENSURE_REQUIREMENTS, [plan_id=223]
   :     +- *(6) SortMergeJoin [region_id#71L], [region_id#78L], Inner
   :        :- *(4) Sort [region_id#71L ASC NULLS FIRST], false, 0
   :        :  +- AQEShuffleRead coalesced
   :        :     +- ShuffleQueryStage 0
   :        :        +- Exchange hashpartitioning(region_id#71L, 8), ENSURE_REQUIREMENTS, [plan_id=117]
   :        :           +- *(1) Project [id#68L AS user_id#70L, (id#68L % 10) AS region_id#71L, cast(id#68L as string) AS user_name#72]
   :        :              +- *(1) Filter isnotnull((id#68L % 10))
   :        :                 +- *(1) Range (0, 1000, step=1, splits=5)
   :        +- *(5) Sort [region_id#78L ASC NULLS FIRST], false, 0
   :           +- AQEShuffleRead coalesced
   :              +- ShuffleQueryStage 1
   :                 +- Exchange hashpartitioning(region_id#78L, 8), ENSURE_REQUIREMENTS, [plan_id=124]
   :                    +- *(2) Project [id#76L AS region_id#78L, cast(id#76L as string) AS region_name#79]
   :                       +- *(2) Range (0, 10, step=1, splits=1)
   +- ShuffleQueryStage 2
      +- Exchange hashpartitioning(user_id#61L, region_id#62L, region_id#62L, 8), ENSURE_REQUIREMENTS, [plan_id=141]
         +- *(3) Project [id#58L AS order_id#60L, (id#58L % 1000) AS user_id#61L, (id#58L % 10) AS region_id#62L, cast(id#58L as string) AS order_num#63]
            +- *(3) Filter (isnotnull((id#58L % 1000)) AND isnotnull((id#58L % 10)))
               +- *(3) Range (0, 10000, step=1, splits=10)
】AQE ==> 【
Project [order_id#60L, user_id#61L, region_id#62L, order_num#63, user_name#72, region_name#79]
+- SortMergeJoin [user_id#70L, region_id#71L, region_id#78L], [user_id#61L, region_id#62L, region_id#62L], Inner
   :- Sort [user_id#70L ASC NULLS FIRST, region_id#71L ASC NULLS FIRST, region_id#78L ASC NULLS FIRST], false, 0
   :  +- ShuffleQueryStage 3
   :     +- Exchange hashpartitioning(user_id#70L, region_id#71L, region_id#71L, 8), ENSURE_REQUIREMENTS, [plan_id=223]
   :        +- *(6) SortMergeJoin [region_id#71L], [region_id#78L], Inner
   :           :- *(4) Sort [region_id#71L ASC NULLS FIRST], false, 0
   :           :  +- AQEShuffleRead coalesced
   :           :     +- ShuffleQueryStage 0
   :           :        +- Exchange hashpartitioning(region_id#71L, 8), ENSURE_REQUIREMENTS, [plan_id=117]
   :           :           +- *(1) Project [id#68L AS user_id#70L, (id#68L % 10) AS region_id#71L, cast(id#68L as string) AS user_name#72]
   :           :              +- *(1) Filter isnotnull((id#68L % 10))
   :           :                 +- *(1) Range (0, 1000, step=1, splits=5)
   :           +- *(5) Sort [region_id#78L ASC NULLS FIRST], false, 0
   :              +- AQEShuffleRead coalesced
   :                 +- ShuffleQueryStage 1
   :                    +- Exchange hashpartitioning(region_id#78L, 8), ENSURE_REQUIREMENTS, [plan_id=124]
   :                       +- *(2) Project [id#76L AS region_id#78L, cast(id#76L as string) AS region_name#79]
   :                          +- *(2) Range (0, 10, step=1, splits=1)
   +- Sort [user_id#61L ASC NULLS FIRST, region_id#62L ASC NULLS FIRST, region_id#62L ASC NULLS FIRST], false, 0
      +- ShuffleQueryStage 2
         +- Exchange hashpartitioning(user_id#61L, region_id#62L, region_id#62L, 8), ENSURE_REQUIREMENTS, [plan_id=141]
            +- *(3) Project [id#58L AS order_id#60L, (id#58L % 1000) AS user_id#61L, (id#58L % 10) AS region_id#62L, cast(id#58L as string) AS order_num#63]
               +- *(3) Filter (isnotnull((id#58L % 1000)) AND isnotnull((id#58L % 10)))
                  +- *(3) Range (0, 10000, step=1, splits=10)
】

21:32:55.630 [ScalaTest-run-running-PhysicalPlanSpec] INFO  org.apache.spark.sql.execution.adaptive.ShufflePartitionsUtil - For shuffle(3, 2), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
optimizeQueryStage - org.apache.spark.sql.execution.adaptive.CoalesceShufflePartitions -> old【
Project [order_id#60L, user_id#61L, region_id#62L, order_num#63, user_name#72, region_name#79]
+- SortMergeJoin [user_id#70L, region_id#71L, region_id#78L], [user_id#61L, region_id#62L, region_id#62L], Inner
   :- Sort [user_id#70L ASC NULLS FIRST, region_id#71L ASC NULLS FIRST, region_id#78L ASC NULLS FIRST], false, 0
   :  +- ShuffleQueryStage 3
   :     +- Exchange hashpartitioning(user_id#70L, region_id#71L, region_id#71L, 8), ENSURE_REQUIREMENTS, [plan_id=223]
   :        +- *(6) SortMergeJoin [region_id#71L], [region_id#78L], Inner
   :           :- *(4) Sort [region_id#71L ASC NULLS FIRST], false, 0
   :           :  +- AQEShuffleRead coalesced
   :           :     +- ShuffleQueryStage 0
   :           :        +- Exchange hashpartitioning(region_id#71L, 8), ENSURE_REQUIREMENTS, [plan_id=117]
   :           :           +- *(1) Project [id#68L AS user_id#70L, (id#68L % 10) AS region_id#71L, cast(id#68L as string) AS user_name#72]
   :           :              +- *(1) Filter isnotnull((id#68L % 10))
   :           :                 +- *(1) Range (0, 1000, step=1, splits=5)
   :           +- *(5) Sort [region_id#78L ASC NULLS FIRST], false, 0
   :              +- AQEShuffleRead coalesced
   :                 +- ShuffleQueryStage 1
   :                    +- Exchange hashpartitioning(region_id#78L, 8), ENSURE_REQUIREMENTS, [plan_id=124]
   :                       +- *(2) Project [id#76L AS region_id#78L, cast(id#76L as string) AS region_name#79]
   :                          +- *(2) Range (0, 10, step=1, splits=1)
   +- Sort [user_id#61L ASC NULLS FIRST, region_id#62L ASC NULLS FIRST, region_id#62L ASC NULLS FIRST], false, 0
      +- ShuffleQueryStage 2
         +- Exchange hashpartitioning(user_id#61L, region_id#62L, region_id#62L, 8), ENSURE_REQUIREMENTS, [plan_id=141]
            +- *(3) Project [id#58L AS order_id#60L, (id#58L % 1000) AS user_id#61L, (id#58L % 10) AS region_id#62L, cast(id#58L as string) AS order_num#63]
               +- *(3) Filter (isnotnull((id#58L % 1000)) AND isnotnull((id#58L % 10)))
                  +- *(3) Range (0, 10000, step=1, splits=10)
】AQE --> new【
Project [order_id#60L, user_id#61L, region_id#62L, order_num#63, user_name#72, region_name#79]
+- SortMergeJoin [user_id#70L, region_id#71L, region_id#78L], [user_id#61L, region_id#62L, region_id#62L], Inner
   :- Sort [user_id#70L ASC NULLS FIRST, region_id#71L ASC NULLS FIRST, region_id#78L ASC NULLS FIRST], false, 0
   :  +- AQEShuffleRead coalesced
   :     +- ShuffleQueryStage 3
   :        +- Exchange hashpartitioning(user_id#70L, region_id#71L, region_id#71L, 8), ENSURE_REQUIREMENTS, [plan_id=223]
   :           +- *(6) SortMergeJoin [region_id#71L], [region_id#78L], Inner
   :              :- *(4) Sort [region_id#71L ASC NULLS FIRST], false, 0
   :              :  +- AQEShuffleRead coalesced
   :              :     +- ShuffleQueryStage 0
   :              :        +- Exchange hashpartitioning(region_id#71L, 8), ENSURE_REQUIREMENTS, [plan_id=117]
   :              :           +- *(1) Project [id#68L AS user_id#70L, (id#68L % 10) AS region_id#71L, cast(id#68L as string) AS user_name#72]
   :              :              +- *(1) Filter isnotnull((id#68L % 10))
   :              :                 +- *(1) Range (0, 1000, step=1, splits=5)
   :              +- *(5) Sort [region_id#78L ASC NULLS FIRST], false, 0
   :                 +- AQEShuffleRead coalesced
   :                    +- ShuffleQueryStage 1
   :                       +- Exchange hashpartitioning(region_id#78L, 8), ENSURE_REQUIREMENTS, [plan_id=124]
   :                          +- *(2) Project [id#76L AS region_id#78L, cast(id#76L as string) AS region_name#79]
   :                             +- *(2) Range (0, 10, step=1, splits=1)
   +- Sort [user_id#61L ASC NULLS FIRST, region_id#62L ASC NULLS FIRST, region_id#62L ASC NULLS FIRST], false, 0
      +- AQEShuffleRead coalesced
         +- ShuffleQueryStage 2
            +- Exchange hashpartitioning(user_id#61L, region_id#62L, region_id#62L, 8), ENSURE_REQUIREMENTS, [plan_id=141]
               +- *(3) Project [id#58L AS order_id#60L, (id#58L % 1000) AS user_id#61L, (id#58L % 10) AS region_id#62L, cast(id#58L as string) AS order_num#63]
                  +- *(3) Filter (isnotnull((id#58L % 1000)) AND isnotnull((id#58L % 10)))
                     +- *(3) Range (0, 10000, step=1, splits=10)
】


org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec$ - applyPhysicalRules - org.apache.spark.sql.execution.CollapseCodegenStages -->【
Project [order_id#60L, user_id#61L, region_id#62L, order_num#63, user_name#72, region_name#79]
+- SortMergeJoin [user_id#70L, region_id#71L, region_id#78L], [user_id#61L, region_id#62L, region_id#62L], Inner
   :- Sort [user_id#70L ASC NULLS FIRST, region_id#71L ASC NULLS FIRST, region_id#78L ASC NULLS FIRST], false, 0
   :  +- AQEShuffleRead coalesced
   :     +- ShuffleQueryStage 3
   :        +- Exchange hashpartitioning(user_id#70L, region_id#71L, region_id#71L, 8), ENSURE_REQUIREMENTS, [plan_id=223]
   :           +- *(6) SortMergeJoin [region_id#71L], [region_id#78L], Inner
   :              :- *(4) Sort [region_id#71L ASC NULLS FIRST], false, 0
   :              :  +- AQEShuffleRead coalesced
   :              :     +- ShuffleQueryStage 0
   :              :        +- Exchange hashpartitioning(region_id#71L, 8), ENSURE_REQUIREMENTS, [plan_id=117]
   :              :           +- *(1) Project [id#68L AS user_id#70L, (id#68L % 10) AS region_id#71L, cast(id#68L as string) AS user_name#72]
   :              :              +- *(1) Filter isnotnull((id#68L % 10))
   :              :                 +- *(1) Range (0, 1000, step=1, splits=5)
   :              +- *(5) Sort [region_id#78L ASC NULLS FIRST], false, 0
   :                 +- AQEShuffleRead coalesced
   :                    +- ShuffleQueryStage 1
   :                       +- Exchange hashpartitioning(region_id#78L, 8), ENSURE_REQUIREMENTS, [plan_id=124]
   :                          +- *(2) Project [id#76L AS region_id#78L, cast(id#76L as string) AS region_name#79]
   :                             +- *(2) Range (0, 10, step=1, splits=1)
   +- Sort [user_id#61L ASC NULLS FIRST, region_id#62L ASC NULLS FIRST, region_id#62L ASC NULLS FIRST], false, 0
      +- AQEShuffleRead coalesced
         +- ShuffleQueryStage 2
            +- Exchange hashpartitioning(user_id#61L, region_id#62L, region_id#62L, 8), ENSURE_REQUIREMENTS, [plan_id=141]
               +- *(3) Project [id#58L AS order_id#60L, (id#58L % 1000) AS user_id#61L, (id#58L % 10) AS region_id#62L, cast(id#58L as string) AS order_num#63]
                  +- *(3) Filter (isnotnull((id#58L % 1000)) AND isnotnull((id#58L % 10)))
                     +- *(3) Range (0, 10000, step=1, splits=10)
】AQE --> 【
*(9) Project [order_id#60L, user_id#61L, region_id#62L, order_num#63, user_name#72, region_name#79]
+- *(9) SortMergeJoin [user_id#70L, region_id#71L, region_id#78L], [user_id#61L, region_id#62L, region_id#62L], Inner
   :- *(7) Sort [user_id#70L ASC NULLS FIRST, region_id#71L ASC NULLS FIRST, region_id#78L ASC NULLS FIRST], false, 0
   :  +- AQEShuffleRead coalesced
   :     +- ShuffleQueryStage 3
   :        +- Exchange hashpartitioning(user_id#70L, region_id#71L, region_id#71L, 8), ENSURE_REQUIREMENTS, [plan_id=223]
   :           +- *(6) SortMergeJoin [region_id#71L], [region_id#78L], Inner
   :              :- *(4) Sort [region_id#71L ASC NULLS FIRST], false, 0
   :              :  +- AQEShuffleRead coalesced
   :              :     +- ShuffleQueryStage 0
   :              :        +- Exchange hashpartitioning(region_id#71L, 8), ENSURE_REQUIREMENTS, [plan_id=117]
   :              :           +- *(1) Project [id#68L AS user_id#70L, (id#68L % 10) AS region_id#71L, cast(id#68L as string) AS user_name#72]
   :              :              +- *(1) Filter isnotnull((id#68L % 10))
   :              :                 +- *(1) Range (0, 1000, step=1, splits=5)
   :              +- *(5) Sort [region_id#78L ASC NULLS FIRST], false, 0
   :                 +- AQEShuffleRead coalesced
   :                    +- ShuffleQueryStage 1
   :                       +- Exchange hashpartitioning(region_id#78L, 8), ENSURE_REQUIREMENTS, [plan_id=124]
   :                          +- *(2) Project [id#76L AS region_id#78L, cast(id#76L as string) AS region_name#79]
   :                             +- *(2) Range (0, 10, step=1, splits=1)
   +- *(8) Sort [user_id#61L ASC NULLS FIRST, region_id#62L ASC NULLS FIRST, region_id#62L ASC NULLS FIRST], false, 0
      +- AQEShuffleRead coalesced
         +- ShuffleQueryStage 2
            +- Exchange hashpartitioning(user_id#61L, region_id#62L, region_id#62L, 8), ENSURE_REQUIREMENTS, [plan_id=141]
               +- *(3) Project [id#58L AS order_id#60L, (id#58L % 1000) AS user_id#61L, (id#58L % 10) AS region_id#62L, cast(id#58L as string) AS order_num#63]
                  +- *(3) Filter (isnotnull((id#58L % 1000)) AND isnotnull((id#58L % 10)))
                     +- *(3) Range (0, 10000, step=1, splits=10)
】

org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec$ final result ==>【
Project [order_id#60L, user_id#61L, region_id#62L, order_num#63, user_name#72, region_name#79]
+- SortMergeJoin [user_id#70L, region_id#71L, region_id#78L], [user_id#61L, region_id#62L, region_id#62L], Inner
   :- Sort [user_id#70L ASC NULLS FIRST, region_id#71L ASC NULLS FIRST, region_id#78L ASC NULLS FIRST], false, 0
   :  +- AQEShuffleRead coalesced
   :     +- ShuffleQueryStage 3
   :        +- Exchange hashpartitioning(user_id#70L, region_id#71L, region_id#71L, 8), ENSURE_REQUIREMENTS, [plan_id=223]
   :           +- *(6) SortMergeJoin [region_id#71L], [region_id#78L], Inner
   :              :- *(4) Sort [region_id#71L ASC NULLS FIRST], false, 0
   :              :  +- AQEShuffleRead coalesced
   :              :     +- ShuffleQueryStage 0
   :              :        +- Exchange hashpartitioning(region_id#71L, 8), ENSURE_REQUIREMENTS, [plan_id=117]
   :              :           +- *(1) Project [id#68L AS user_id#70L, (id#68L % 10) AS region_id#71L, cast(id#68L as string) AS user_name#72]
   :              :              +- *(1) Filter isnotnull((id#68L % 10))
   :              :                 +- *(1) Range (0, 1000, step=1, splits=5)
   :              +- *(5) Sort [region_id#78L ASC NULLS FIRST], false, 0
   :                 +- AQEShuffleRead coalesced
   :                    +- ShuffleQueryStage 1
   :                       +- Exchange hashpartitioning(region_id#78L, 8), ENSURE_REQUIREMENTS, [plan_id=124]
   :                          +- *(2) Project [id#76L AS region_id#78L, cast(id#76L as string) AS region_name#79]
   :                             +- *(2) Range (0, 10, step=1, splits=1)
   +- Sort [user_id#61L ASC NULLS FIRST, region_id#62L ASC NULLS FIRST, region_id#62L ASC NULLS FIRST], false, 0
      +- AQEShuffleRead coalesced
         +- ShuffleQueryStage 2
            +- Exchange hashpartitioning(user_id#61L, region_id#62L, region_id#62L, 8), ENSURE_REQUIREMENTS, [plan_id=141]
               +- *(3) Project [id#58L AS order_id#60L, (id#58L % 1000) AS user_id#61L, (id#58L % 10) AS region_id#62L, cast(id#58L as string) AS order_num#63]
                  +- *(3) Filter (isnotnull((id#58L % 1000)) AND isnotnull((id#58L % 10)))
                     +- *(3) Range (0, 10000, step=1, splits=10)
】AQE ==> 【
*(9) Project [order_id#60L, user_id#61L, region_id#62L, order_num#63, user_name#72, region_name#79]
+- *(9) SortMergeJoin [user_id#70L, region_id#71L, region_id#78L], [user_id#61L, region_id#62L, region_id#62L], Inner
   :- *(7) Sort [user_id#70L ASC NULLS FIRST, region_id#71L ASC NULLS FIRST, region_id#78L ASC NULLS FIRST], false, 0
   :  +- AQEShuffleRead coalesced
   :     +- ShuffleQueryStage 3
   :        +- Exchange hashpartitioning(user_id#70L, region_id#71L, region_id#71L, 8), ENSURE_REQUIREMENTS, [plan_id=223]
   :           +- *(6) SortMergeJoin [region_id#71L], [region_id#78L], Inner
   :              :- *(4) Sort [region_id#71L ASC NULLS FIRST], false, 0
   :              :  +- AQEShuffleRead coalesced
   :              :     +- ShuffleQueryStage 0
   :              :        +- Exchange hashpartitioning(region_id#71L, 8), ENSURE_REQUIREMENTS, [plan_id=117]
   :              :           +- *(1) Project [id#68L AS user_id#70L, (id#68L % 10) AS region_id#71L, cast(id#68L as string) AS user_name#72]
   :              :              +- *(1) Filter isnotnull((id#68L % 10))
   :              :                 +- *(1) Range (0, 1000, step=1, splits=5)
   :              +- *(5) Sort [region_id#78L ASC NULLS FIRST], false, 0
   :                 +- AQEShuffleRead coalesced
   :                    +- ShuffleQueryStage 1
   :                       +- Exchange hashpartitioning(region_id#78L, 8), ENSURE_REQUIREMENTS, [plan_id=124]
   :                          +- *(2) Project [id#76L AS region_id#78L, cast(id#76L as string) AS region_name#79]
   :                             +- *(2) Range (0, 10, step=1, splits=1)
   +- *(8) Sort [user_id#61L ASC NULLS FIRST, region_id#62L ASC NULLS FIRST, region_id#62L ASC NULLS FIRST], false, 0
      +- AQEShuffleRead coalesced
         +- ShuffleQueryStage 2
            +- Exchange hashpartitioning(user_id#61L, region_id#62L, region_id#62L, 8), ENSURE_REQUIREMENTS, [plan_id=141]
               +- *(3) Project [id#58L AS order_id#60L, (id#58L % 1000) AS user_id#61L, (id#58L % 10) AS region_id#62L, cast(id#58L as string) AS order_num#63]
                  +- *(3) Filter (isnotnull((id#58L % 1000)) AND isnotnull((id#58L % 10)))
                     +- *(3) Range (0, 10000, step=1, splits=10)
】

21:32:55.653 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec - withFinalPlanUpdate.doExecute ==> after get final plan:
*(9) Project [order_id#60L, user_id#61L, region_id#62L, order_num#63, user_name#72, region_name#79]
+- *(9) SortMergeJoin [user_id#70L, region_id#71L, region_id#78L], [user_id#61L, region_id#62L, region_id#62L], Inner
   :- *(7) Sort [user_id#70L ASC NULLS FIRST, region_id#71L ASC NULLS FIRST, region_id#78L ASC NULLS FIRST], false, 0
   :  +- AQEShuffleRead coalesced
   :     +- ShuffleQueryStage 3
   :        +- Exchange hashpartitioning(user_id#70L, region_id#71L, region_id#71L, 8), ENSURE_REQUIREMENTS, [plan_id=223]
   :           +- *(6) SortMergeJoin [region_id#71L], [region_id#78L], Inner
   :              :- *(4) Sort [region_id#71L ASC NULLS FIRST], false, 0
   :              :  +- AQEShuffleRead coalesced
   :              :     +- ShuffleQueryStage 0
   :              :        +- Exchange hashpartitioning(region_id#71L, 8), ENSURE_REQUIREMENTS, [plan_id=117]
   :              :           +- *(1) Project [id#68L AS user_id#70L, (id#68L % 10) AS region_id#71L, cast(id#68L as string) AS user_name#72]
   :              :              +- *(1) Filter isnotnull((id#68L % 10))
   :              :                 +- *(1) Range (0, 1000, step=1, splits=5)
   :              +- *(5) Sort [region_id#78L ASC NULLS FIRST], false, 0
   :                 +- AQEShuffleRead coalesced
   :                    +- ShuffleQueryStage 1
   :                       +- Exchange hashpartitioning(region_id#78L, 8), ENSURE_REQUIREMENTS, [plan_id=124]
   :                          +- *(2) Project [id#76L AS region_id#78L, cast(id#76L as string) AS region_name#79]
   :                             +- *(2) Range (0, 10, step=1, splits=1)
   +- *(8) Sort [user_id#61L ASC NULLS FIRST, region_id#62L ASC NULLS FIRST, region_id#62L ASC NULLS FIRST], false, 0
      +- AQEShuffleRead coalesced
         +- ShuffleQueryStage 2
            +- Exchange hashpartitioning(user_id#61L, region_id#62L, region_id#62L, 8), ENSURE_REQUIREMENTS, [plan_id=141]
               +- *(3) Project [id#58L AS order_id#60L, (id#58L % 1000) AS user_id#61L, (id#58L % 10) AS region_id#62L, cast(id#58L as string) AS order_num#63]
                  +- *(3) Filter (isnotnull((id#58L % 1000)) AND isnotnull((id#58L % 10)))
                     +- *(3) Range (0, 10000, step=1, splits=10)

21:32:55.682 [ScalaTest-run-running-PhysicalPlanSpec] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 24.410994 ms
21:32:55.699 [ScalaTest-run-running-PhysicalPlanSpec] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 14.074898 ms
21:32:55.700 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.util.ClosureCleaner - Cleaning indylambda closure: $anonfun$doExecute$4$adapted
21:32:55.701 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.util.ClosureCleaner -  +++ indylambda closure ($anonfun$doExecute$4$adapted) is now cleaned +++
21:32:55.722 [ScalaTest-run-running-PhysicalPlanSpec] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 14.434318 ms
21:32:55.724 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.util.ClosureCleaner - Cleaning indylambda closure: $anonfun$doExecute$4$adapted
21:32:55.725 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.util.ClosureCleaner -  +++ indylambda closure ($anonfun$doExecute$4$adapted) is now cleaned +++
21:32:55.735 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.util.ClosureCleaner - Cleaning indylambda closure: $anonfun$doExecute$5
21:32:55.736 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.util.ClosureCleaner -  +++ indylambda closure ($anonfun$doExecute$5) is now cleaned +++
21:32:55.737 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.util.ClosureCleaner - Cleaning indylambda closure: $anonfun$doExecute$6$adapted
21:32:55.738 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.util.ClosureCleaner -  +++ indylambda closure ($anonfun$doExecute$6$adapted) is now cleaned +++
21:32:55.743 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec - withFinalPlanUpdate.doExecute was finished.
21:32:55.746 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec - Final plan: *(9) Project [order_id#60L, user_id#61L, region_id#62L, order_num#63, user_name#72, region_name#79]
+- *(9) SortMergeJoin [user_id#70L, region_id#71L, region_id#78L], [user_id#61L, region_id#62L, region_id#62L], Inner
   :- *(7) Sort [user_id#70L ASC NULLS FIRST, region_id#71L ASC NULLS FIRST, region_id#78L ASC NULLS FIRST], false, 0
   :  +- AQEShuffleRead coalesced
   :     +- ShuffleQueryStage 3
   :        +- Exchange hashpartitioning(user_id#70L, region_id#71L, region_id#71L, 8), ENSURE_REQUIREMENTS, [plan_id=223]
   :           +- *(6) SortMergeJoin [region_id#71L], [region_id#78L], Inner
   :              :- *(4) Sort [region_id#71L ASC NULLS FIRST], false, 0
   :              :  +- AQEShuffleRead coalesced
   :              :     +- ShuffleQueryStage 0
   :              :        +- Exchange hashpartitioning(region_id#71L, 8), ENSURE_REQUIREMENTS, [plan_id=117]
   :              :           +- *(1) Project [id#68L AS user_id#70L, (id#68L % 10) AS region_id#71L, cast(id#68L as string) AS user_name#72]
   :              :              +- *(1) Filter isnotnull((id#68L % 10))
   :              :                 +- *(1) Range (0, 1000, step=1, splits=5)
   :              +- *(5) Sort [region_id#78L ASC NULLS FIRST], false, 0
   :                 +- AQEShuffleRead coalesced
   :                    +- ShuffleQueryStage 1
   :                       +- Exchange hashpartitioning(region_id#78L, 8), ENSURE_REQUIREMENTS, [plan_id=124]
   :                          +- *(2) Project [id#76L AS region_id#78L, cast(id#76L as string) AS region_name#79]
   :                             +- *(2) Range (0, 10, step=1, splits=1)
   +- *(8) Sort [user_id#61L ASC NULLS FIRST, region_id#62L ASC NULLS FIRST, region_id#62L ASC NULLS FIRST], false, 0
      +- AQEShuffleRead coalesced
         +- ShuffleQueryStage 2
            +- Exchange hashpartitioning(user_id#61L, region_id#62L, region_id#62L, 8), ENSURE_REQUIREMENTS, [plan_id=141]
               +- *(3) Project [id#58L AS order_id#60L, (id#58L % 1000) AS user_id#61L, (id#58L % 10) AS region_id#62L, cast(id#58L as string) AS order_num#63]
                  +- *(3) Filter (isnotnull((id#58L % 1000)) AND isnotnull((id#58L % 10)))
                     +- *(3) Range (0, 10000, step=1, splits=10)

21:32:55.747 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec - withFinalPlanUpdate.doExecute.finalPlanUpdate was finished.
21:32:55.753 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.util.ClosureCleaner - Cleaning indylambda closure: $anonfun$write$21
21:32:55.754 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.util.ClosureCleaner -  +++ indylambda closure ($anonfun$write$21) is now cleaned +++
21:32:55.824 [ScalaTest-run-running-PhysicalPlanSpec] INFO  org.apache.spark.SparkContext - Starting job: saveAsTable at PhysicalPlanSpec.scala:338
21:32:55.826 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.scheduler.DAGScheduler - eagerlyComputePartitionsForRddAndAncestors for RDD 24 took 0.000375 seconds
21:32:55.826 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - Merging stage rdd profiles: Set()
21:32:55.827 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - Merging stage rdd profiles: Set()
21:32:55.827 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - Merging stage rdd profiles: Set()
21:32:55.827 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - Merging stage rdd profiles: Set()
21:32:55.827 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - Merging stage rdd profiles: Set()
21:32:55.829 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 4 (saveAsTable at PhysicalPlanSpec.scala:338) with 1 output partitions
21:32:55.830 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 10 (saveAsTable at PhysicalPlanSpec.scala:338)
21:32:55.830 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 9, ShuffleMapStage 8)
21:32:55.831 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
21:32:55.832 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - submitStage(ResultStage 10 (name=saveAsTable at PhysicalPlanSpec.scala:338;jobs=4))
21:32:55.833 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - missing: List()
21:32:55.833 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 10 (MapPartitionsRDD[24] at saveAsTable at PhysicalPlanSpec.scala:338), which has no missing parents
21:32:55.833 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - submitMissingTasks(ResultStage 10)
21:32:55.868 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_4 stored as values in memory (estimated size 247.2 KiB, free 4.1 GiB)
21:32:55.868 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager - Put block broadcast_4 locally took 0 ms
21:32:55.868 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager - Putting block broadcast_4 without replication took 0 ms
21:32:55.871 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_4_piece0 stored as bytes in memory (estimated size 90.3 KiB, free 4.1 GiB)
21:32:55.871 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_4_piece0 for BlockManagerId(driver, 192.168.31.69, 63737, None)
21:32:55.871 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_4_piece0 in memory on 192.168.31.69:63737 (size: 90.3 KiB, free: 4.1 GiB)
21:32:55.871 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_4_piece0
21:32:55.872 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager - Told master about block broadcast_4_piece0
21:32:55.872 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager - Put block broadcast_4_piece0 locally took 1 ms
21:32:55.872 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager - Putting block broadcast_4_piece0 without replication took 1 ms
21:32:55.872 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 4 from broadcast at DAGScheduler.scala:1513
21:32:55.873 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 10 (MapPartitionsRDD[24] at saveAsTable at PhysicalPlanSpec.scala:338) (first 15 tasks are for partitions Vector(0))
21:32:55.873 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 10.0 with 1 tasks resource profile 0
21:32:55.873 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.TaskSetManager - Epoch for TaskSet 10.0: 4
21:32:55.873 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.TaskSetManager - Adding pending tasks took 0 ms
21:32:55.873 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.TaskSetManager - Valid locality levels for TaskSet 10.0: NODE_LOCAL, ANY
21:32:55.874 [dispatcher-event-loop-1] DEBUG org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_10.0, runningTasks: 0
21:32:55.875 [dispatcher-event-loop-1] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 10.0 (TID 17) (192.168.31.69, executor driver, partition 0, NODE_LOCAL, 4735 bytes) taskResourceAssignments Map()
21:32:55.876 [Executor task launch worker for task 0.0 in stage 10.0 (TID 17)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 10.0 (TID 17)
21:32:55.877 [Executor task launch worker for task 0.0 in stage 10.0 (TID 17)] DEBUG org.apache.spark.executor.ExecutorMetricsPoller - stageTCMP: (10, 0) -> 1
21:32:55.878 [Executor task launch worker for task 0.0 in stage 10.0 (TID 17)] DEBUG org.apache.spark.storage.BlockManager - Getting local block broadcast_4
21:32:55.878 [Executor task launch worker for task 0.0 in stage 10.0 (TID 17)] DEBUG org.apache.spark.storage.BlockManager - Level for block broadcast_4 is StorageLevel(disk, memory, deserialized, 1 replicas)
21:32:55.927 [Executor task launch worker for task 0.0 in stage 10.0 (TID 17)] DEBUG org.apache.spark.MapOutputTrackerMaster - Fetching outputs for shuffle 3
21:32:55.927 [Executor task launch worker for task 0.0 in stage 10.0 (TID 17)] DEBUG org.apache.spark.MapOutputTrackerMaster - Convert map statuses for shuffle 3, mappers 0-1, partitions 0-8
21:32:55.928 [Executor task launch worker for task 0.0 in stage 10.0 (TID 17)] DEBUG org.apache.spark.storage.ShuffleBlockFetcherIterator - maxBytesInFlight: 50331648, targetRemoteRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
21:32:55.928 [Executor task launch worker for task 0.0 in stage 10.0 (TID 17)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 (20.1 KiB) non-empty blocks including 1 (20.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
21:32:55.928 [Executor task launch worker for task 0.0 in stage 10.0 (TID 17)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
21:32:55.928 [Executor task launch worker for task 0.0 in stage 10.0 (TID 17)] DEBUG org.apache.spark.storage.ShuffleBlockFetcherIterator - Start fetching local blocks: (shuffle_3_16_0_8,0)
21:32:55.928 [Executor task launch worker for task 0.0 in stage 10.0 (TID 17)] DEBUG org.apache.spark.storage.BlockManager - Getting local shuffle block shuffle_3_16_0_8
21:32:55.928 [Executor task launch worker for task 0.0 in stage 10.0 (TID 17)] DEBUG org.apache.spark.storage.ShuffleBlockFetcherIterator - Got local blocks in 0 ms
21:32:55.942 [Executor task launch worker for task 0.0 in stage 10.0 (TID 17)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 10.066569 ms
21:32:55.945 [Executor task launch worker for task 0.0 in stage 10.0 (TID 17)] DEBUG org.apache.spark.memory.TaskMemoryManager - Task 17 acquired 64.0 KiB for org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@1805ed4d
21:32:55.945 [Executor task launch worker for task 0.0 in stage 10.0 (TID 17)] DEBUG org.apache.spark.MapOutputTrackerMaster - Fetching outputs for shuffle 2
21:32:55.945 [Executor task launch worker for task 0.0 in stage 10.0 (TID 17)] DEBUG org.apache.spark.MapOutputTrackerMaster - Convert map statuses for shuffle 2, mappers 0-10, partitions 0-8
21:32:55.946 [Executor task launch worker for task 0.0 in stage 10.0 (TID 17)] DEBUG org.apache.spark.storage.ShuffleBlockFetcherIterator - maxBytesInFlight: 50331648, targetRemoteRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
21:32:55.946 [Executor task launch worker for task 0.0 in stage 10.0 (TID 17)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 10 (185.5 KiB) non-empty blocks including 10 (185.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
21:32:55.946 [Executor task launch worker for task 0.0 in stage 10.0 (TID 17)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
21:32:55.946 [Executor task launch worker for task 0.0 in stage 10.0 (TID 17)] DEBUG org.apache.spark.storage.ShuffleBlockFetcherIterator - Start fetching local blocks: (shuffle_2_6_0_8,0), (shuffle_2_7_0_8,1), (shuffle_2_8_0_8,2), (shuffle_2_9_0_8,3), (shuffle_2_10_0_8,4), (shuffle_2_11_0_8,5), (shuffle_2_12_0_8,6), (shuffle_2_13_0_8,7), (shuffle_2_14_0_8,8), (shuffle_2_15_0_8,9)
21:32:55.946 [Executor task launch worker for task 0.0 in stage 10.0 (TID 17)] DEBUG org.apache.spark.storage.BlockManager - Getting local shuffle block shuffle_2_6_0_8
21:32:55.946 [Executor task launch worker for task 0.0 in stage 10.0 (TID 17)] DEBUG org.apache.spark.storage.BlockManager - Getting local shuffle block shuffle_2_7_0_8
21:32:55.947 [Executor task launch worker for task 0.0 in stage 10.0 (TID 17)] DEBUG org.apache.spark.storage.BlockManager - Getting local shuffle block shuffle_2_8_0_8
21:32:55.947 [Executor task launch worker for task 0.0 in stage 10.0 (TID 17)] DEBUG org.apache.spark.storage.BlockManager - Getting local shuffle block shuffle_2_9_0_8
21:32:55.947 [Executor task launch worker for task 0.0 in stage 10.0 (TID 17)] DEBUG org.apache.spark.storage.BlockManager - Getting local shuffle block shuffle_2_10_0_8
21:32:55.947 [Executor task launch worker for task 0.0 in stage 10.0 (TID 17)] DEBUG org.apache.spark.storage.BlockManager - Getting local shuffle block shuffle_2_11_0_8
21:32:55.947 [Executor task launch worker for task 0.0 in stage 10.0 (TID 17)] DEBUG org.apache.spark.storage.BlockManager - Getting local shuffle block shuffle_2_12_0_8
21:32:55.948 [Executor task launch worker for task 0.0 in stage 10.0 (TID 17)] DEBUG org.apache.spark.storage.BlockManager - Getting local shuffle block shuffle_2_13_0_8
21:32:55.948 [Executor task launch worker for task 0.0 in stage 10.0 (TID 17)] DEBUG org.apache.spark.storage.BlockManager - Getting local shuffle block shuffle_2_14_0_8
21:32:55.948 [Executor task launch worker for task 0.0 in stage 10.0 (TID 17)] DEBUG org.apache.spark.storage.BlockManager - Getting local shuffle block shuffle_2_15_0_8
21:32:55.948 [Executor task launch worker for task 0.0 in stage 10.0 (TID 17)] DEBUG org.apache.spark.storage.ShuffleBlockFetcherIterator - Got local blocks in 2 ms
21:32:55.961 [Executor task launch worker for task 0.0 in stage 10.0 (TID 17)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 9.354923 ms
21:32:55.963 [Executor task launch worker for task 0.0 in stage 10.0 (TID 17)] DEBUG org.apache.spark.memory.TaskMemoryManager - Task 17 acquired 64.0 KiB for org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@2734d009
21:32:55.967 [Executor task launch worker for task 0.0 in stage 10.0 (TID 17)] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21:32:55.967 [Executor task launch worker for task 0.0 in stage 10.0 (TID 17)] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21:32:56.083 [Executor task launch worker for task 0.0 in stage 10.0 (TID 17)] INFO  org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "order_id",
    "type" : "long",
    "nullable" : false,
    "metadata" : { }
  }, {
    "name" : "user_id",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "region_id",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "order_num",
    "type" : "string",
    "nullable" : false,
    "metadata" : { }
  }, {
    "name" : "user_name",
    "type" : "string",
    "nullable" : false,
    "metadata" : { }
  }, {
    "name" : "region_name",
    "type" : "string",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  required int64 order_id;
  optional int64 user_id;
  optional int64 region_id;
  required binary order_num (STRING);
  required binary user_name (STRING);
  required binary region_name (STRING);
}


21:32:56.530 [Spark Context Cleaner] DEBUG org.apache.spark.broadcast.TorrentBroadcast - Unpersisting TorrentBroadcast 3
21:32:56.549 [block-manager-storage-async-thread-pool-0] DEBUG org.apache.spark.storage.BlockManagerStorageEndpoint - removing broadcast 3
21:32:56.549 [block-manager-storage-async-thread-pool-0] DEBUG org.apache.spark.storage.BlockManager - Removing broadcast 3
21:32:56.551 [block-manager-storage-async-thread-pool-0] DEBUG org.apache.spark.storage.BlockManager - Removing block broadcast_3
21:32:56.552 [block-manager-storage-async-thread-pool-0] DEBUG org.apache.spark.storage.memory.MemoryStore - Block broadcast_3 of size 38920 dropped from memory (free 4392132653)
21:32:56.553 [block-manager-storage-async-thread-pool-0] DEBUG org.apache.spark.storage.BlockManager - Removing block broadcast_3_piece0
21:32:56.555 [block-manager-storage-async-thread-pool-0] DEBUG org.apache.spark.storage.memory.MemoryStore - Block broadcast_3_piece0 of size 17565 dropped from memory (free 4392150218)
21:32:56.555 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_3_piece0 for BlockManagerId(driver, 192.168.31.69, 63737, None)
21:32:56.556 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_3_piece0 on 192.168.31.69:63737 in memory (size: 17.2 KiB, free: 4.1 GiB)
21:32:56.556 [block-manager-storage-async-thread-pool-0] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_3_piece0
21:32:56.556 [block-manager-storage-async-thread-pool-0] DEBUG org.apache.spark.storage.BlockManager - Told master about block broadcast_3_piece0
21:32:56.557 [block-manager-storage-async-thread-pool-2] DEBUG org.apache.spark.storage.BlockManagerStorageEndpoint - Done removing broadcast 3, response is 0
21:32:56.557 [block-manager-storage-async-thread-pool-2] DEBUG org.apache.spark.storage.BlockManagerStorageEndpoint - Sent response: 0 to 192.168.31.69:63736
21:32:56.558 [Spark Context Cleaner] DEBUG org.apache.spark.broadcast.TorrentBroadcast - Unpersisting TorrentBroadcast 1
21:32:56.558 [block-manager-storage-async-thread-pool-3] DEBUG org.apache.spark.storage.BlockManagerStorageEndpoint - removing broadcast 1
21:32:56.558 [block-manager-storage-async-thread-pool-3] DEBUG org.apache.spark.storage.BlockManager - Removing broadcast 1
21:32:56.559 [block-manager-storage-async-thread-pool-3] DEBUG org.apache.spark.storage.BlockManager - Removing block broadcast_1
21:32:56.559 [block-manager-storage-async-thread-pool-3] DEBUG org.apache.spark.storage.memory.MemoryStore - Block broadcast_1 of size 14416 dropped from memory (free 4392164634)
21:32:56.559 [block-manager-storage-async-thread-pool-3] DEBUG org.apache.spark.storage.BlockManager - Removing block broadcast_1_piece0
21:32:56.559 [block-manager-storage-async-thread-pool-3] DEBUG org.apache.spark.storage.memory.MemoryStore - Block broadcast_1_piece0 of size 7006 dropped from memory (free 4392171640)
21:32:56.560 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_1_piece0 for BlockManagerId(driver, 192.168.31.69, 63737, None)
21:32:56.560 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_1_piece0 on 192.168.31.69:63737 in memory (size: 6.8 KiB, free: 4.1 GiB)
21:32:56.560 [block-manager-storage-async-thread-pool-3] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_1_piece0
21:32:56.560 [block-manager-storage-async-thread-pool-3] DEBUG org.apache.spark.storage.BlockManager - Told master about block broadcast_1_piece0
21:32:56.560 [block-manager-storage-async-thread-pool-5] DEBUG org.apache.spark.storage.BlockManagerStorageEndpoint - Done removing broadcast 1, response is 0
21:32:56.560 [block-manager-storage-async-thread-pool-5] DEBUG org.apache.spark.storage.BlockManagerStorageEndpoint - Sent response: 0 to 192.168.31.69:63736
21:32:56.561 [Spark Context Cleaner] DEBUG org.apache.spark.broadcast.TorrentBroadcast - Unpersisting TorrentBroadcast 2
21:32:56.561 [block-manager-storage-async-thread-pool-6] DEBUG org.apache.spark.storage.BlockManagerStorageEndpoint - removing broadcast 2
21:32:56.561 [block-manager-storage-async-thread-pool-6] DEBUG org.apache.spark.storage.BlockManager - Removing broadcast 2
21:32:56.561 [block-manager-storage-async-thread-pool-6] DEBUG org.apache.spark.storage.BlockManager - Removing block broadcast_2_piece0
21:32:56.562 [block-manager-storage-async-thread-pool-6] DEBUG org.apache.spark.storage.memory.MemoryStore - Block broadcast_2_piece0 of size 7534 dropped from memory (free 4392179174)
21:32:56.562 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_2_piece0 for BlockManagerId(driver, 192.168.31.69, 63737, None)
21:32:56.562 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_2_piece0 on 192.168.31.69:63737 in memory (size: 7.4 KiB, free: 4.1 GiB)
21:32:56.562 [block-manager-storage-async-thread-pool-6] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_2_piece0
21:32:56.562 [block-manager-storage-async-thread-pool-6] DEBUG org.apache.spark.storage.BlockManager - Told master about block broadcast_2_piece0
21:32:56.562 [block-manager-storage-async-thread-pool-6] DEBUG org.apache.spark.storage.BlockManager - Removing block broadcast_2
21:32:56.562 [block-manager-storage-async-thread-pool-6] DEBUG org.apache.spark.storage.memory.MemoryStore - Block broadcast_2 of size 16088 dropped from memory (free 4392195262)
21:32:56.563 [block-manager-storage-async-thread-pool-8] DEBUG org.apache.spark.storage.BlockManagerStorageEndpoint - Done removing broadcast 2, response is 0
21:32:56.563 [block-manager-storage-async-thread-pool-8] DEBUG org.apache.spark.storage.BlockManagerStorageEndpoint - Sent response: 0 to 192.168.31.69:63736
21:32:56.563 [Spark Context Cleaner] DEBUG org.apache.spark.broadcast.TorrentBroadcast - Unpersisting TorrentBroadcast 0
21:32:56.563 [block-manager-storage-async-thread-pool-9] DEBUG org.apache.spark.storage.BlockManagerStorageEndpoint - removing broadcast 0
21:32:56.564 [block-manager-storage-async-thread-pool-9] DEBUG org.apache.spark.storage.BlockManager - Removing broadcast 0
21:32:56.564 [block-manager-storage-async-thread-pool-9] DEBUG org.apache.spark.storage.BlockManager - Removing block broadcast_0_piece0
21:32:56.564 [block-manager-storage-async-thread-pool-9] DEBUG org.apache.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 of size 7332 dropped from memory (free 4392202594)
21:32:56.564 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_0_piece0 for BlockManagerId(driver, 192.168.31.69, 63737, None)
21:32:56.564 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_0_piece0 on 192.168.31.69:63737 in memory (size: 7.2 KiB, free: 4.1 GiB)
21:32:56.564 [block-manager-storage-async-thread-pool-9] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_0_piece0
21:32:56.564 [block-manager-storage-async-thread-pool-9] DEBUG org.apache.spark.storage.BlockManager - Told master about block broadcast_0_piece0
21:32:56.564 [block-manager-storage-async-thread-pool-9] DEBUG org.apache.spark.storage.BlockManager - Removing block broadcast_0
21:32:56.564 [block-manager-storage-async-thread-pool-9] DEBUG org.apache.spark.storage.memory.MemoryStore - Block broadcast_0 of size 15272 dropped from memory (free 4392217866)
21:32:56.565 [block-manager-storage-async-thread-pool-11] DEBUG org.apache.spark.storage.BlockManagerStorageEndpoint - Done removing broadcast 0, response is 0
21:32:56.565 [block-manager-storage-async-thread-pool-11] DEBUG org.apache.spark.storage.BlockManagerStorageEndpoint - Sent response: 0 to 192.168.31.69:63736
21:32:56.603 [Executor task launch worker for task 0.0 in stage 10.0 (TID 17)] DEBUG org.apache.spark.memory.TaskMemoryManager - Task 17 acquired 64.0 MiB for org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@1805ed4d
21:32:56.615 [Executor task launch worker for task 0.0 in stage 10.0 (TID 17)] DEBUG org.apache.spark.memory.TaskMemoryManager - Task 17 acquired 64.0 MiB for org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@2734d009
21:32:56.624 [Executor task launch worker for task 0.0 in stage 10.0 (TID 17)] DEBUG org.apache.spark.memory.TaskMemoryManager - Task 17 acquired 128.0 KiB for org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@2734d009
21:32:56.624 [Executor task launch worker for task 0.0 in stage 10.0 (TID 17)] DEBUG org.apache.spark.memory.TaskMemoryManager - Task 17 release 64.0 KiB from org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@2734d009
21:32:56.627 [Executor task launch worker for task 0.0 in stage 10.0 (TID 17)] DEBUG org.apache.spark.memory.TaskMemoryManager - Task 17 acquired 256.0 KiB for org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@2734d009
21:32:56.627 [Executor task launch worker for task 0.0 in stage 10.0 (TID 17)] DEBUG org.apache.spark.memory.TaskMemoryManager - Task 17 release 128.0 KiB from org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@2734d009
21:32:56.714 [Executor task launch worker for task 0.0 in stage 10.0 (TID 17)] DEBUG org.apache.spark.memory.TaskMemoryManager - Task 17 release 64.0 MiB from org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@1805ed4d
21:32:56.714 [Executor task launch worker for task 0.0 in stage 10.0 (TID 17)] DEBUG org.apache.spark.memory.TaskMemoryManager - Task 17 release 64.0 KiB from org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@1805ed4d
21:32:56.714 [Executor task launch worker for task 0.0 in stage 10.0 (TID 17)] DEBUG org.apache.spark.memory.TaskMemoryManager - Task 17 release 64.0 MiB from org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@2734d009
21:32:56.714 [Executor task launch worker for task 0.0 in stage 10.0 (TID 17)] DEBUG org.apache.spark.memory.TaskMemoryManager - Task 17 release 256.0 KiB from org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@2734d009
21:32:57.553 [dispatcher-event-loop-1] DEBUG org.apache.spark.scheduler.OutputCommitCoordinator - Commit allowed for stage=10.0, partition=0, task attempt 0
21:32:57.555 [Executor task launch worker for task 0.0 in stage 10.0 (TID 17)] INFO  org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_202512152132554562794222457361680_0010_m_000000_17: Committed. Elapsed time: 1 ms.
21:32:57.561 [Executor task launch worker for task 0.0 in stage 10.0 (TID 17)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 10.0 (TID 17). 7432 bytes result sent to driver
21:32:57.561 [Executor task launch worker for task 0.0 in stage 10.0 (TID 17)] DEBUG org.apache.spark.executor.ExecutorMetricsPoller - stageTCMP: (10, 0) -> 0
21:32:57.562 [dispatcher-event-loop-0] DEBUG org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_10.0, runningTasks: 0
21:32:57.562 [dispatcher-event-loop-0] DEBUG org.apache.spark.scheduler.TaskSetManager - No tasks for locality level NODE_LOCAL, so moving to locality level ANY
21:32:57.562 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 10.0 (TID 17) in 1688 ms on 192.168.31.69 (executor driver) (1/1)
21:32:57.562 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 10.0, whose tasks have all completed, from pool
21:32:57.563 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 10 (saveAsTable at PhysicalPlanSpec.scala:338) finished in 1.728 s
21:32:57.564 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - After removal of stage 8, remaining stages = 4
21:32:57.564 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - After removal of stage 7, remaining stages = 3
21:32:57.564 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - After removal of stage 10, remaining stages = 2
21:32:57.564 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - After removal of stage 9, remaining stages = 1
21:32:57.564 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - After removal of stage 6, remaining stages = 0
21:32:57.564 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
21:32:57.565 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 10: Stage finished
21:32:57.565 [dag-scheduler-event-loop] DEBUG org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - onTaskCommit(org.apache.spark.internal.io.FileCommitProtocol$TaskCommitMessage@40903aa5)
21:32:57.566 [ScalaTest-run-running-PhysicalPlanSpec] INFO  org.apache.spark.scheduler.DAGScheduler - Job 4 finished: saveAsTable at PhysicalPlanSpec.scala:338, took 1.741668 s
21:32:57.570 [ScalaTest-run-running-PhysicalPlanSpec] INFO  org.apache.spark.sql.execution.datasources.FileFormatWriter - Start to commit write Job 32870cd2-a1d5-4de1-8a71-d0ad6f253f5b.
21:32:57.590 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Committing files staged for absolute locations Map()
21:32:57.590 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Create absolute parent directories: Set()
21:32:57.591 [ScalaTest-run-running-PhysicalPlanSpec] INFO  org.apache.spark.sql.execution.datasources.FileFormatWriter - Write Job 32870cd2-a1d5-4de1-8a71-d0ad6f253f5b committed. Elapsed time: 20 ms.
21:32:57.594 [ScalaTest-run-running-PhysicalPlanSpec] INFO  org.apache.spark.sql.execution.datasources.FileFormatWriter - Finished processing stats for write job 32870cd2-a1d5-4de1-8a71-d0ad6f253f5b.
21:32:57.602 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.execution.datasources.DataSource - Some paths were ignored:

21:32:57.604 [ScalaTest-run-running-PhysicalPlanSpec] INFO  org.apache.spark.sql.execution.datasources.InMemoryFileIndex - It took 1 ms to list leaf files for 1 paths.
21:32:57.622 [ScalaTest-run-running-PhysicalPlanSpec] INFO  org.apache.spark.sql.hive.HiveExternalCatalog - Persisting file based data source table `default`.`order_all` into Hive metastore in Hive compatible format.
21:32:57.624 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
org.apache.spark.sql.catalyst.parser.SqlBaseParser$SingleDataTypeContext =>  bigint <EOF>
org.apache.spark.sql.catalyst.parser.SqlBaseParser$PrimitiveDataTypeContext =>  bigint
21:32:57.624 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
org.apache.spark.sql.catalyst.parser.SqlBaseParser$SingleDataTypeContext =>  bigint <EOF>
org.apache.spark.sql.catalyst.parser.SqlBaseParser$PrimitiveDataTypeContext =>  bigint
21:32:57.624 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
org.apache.spark.sql.catalyst.parser.SqlBaseParser$SingleDataTypeContext =>  bigint <EOF>
org.apache.spark.sql.catalyst.parser.SqlBaseParser$PrimitiveDataTypeContext =>  bigint
21:32:57.624 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
org.apache.spark.sql.catalyst.parser.SqlBaseParser$SingleDataTypeContext =>  string <EOF>
org.apache.spark.sql.catalyst.parser.SqlBaseParser$PrimitiveDataTypeContext =>  string
21:32:57.624 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
org.apache.spark.sql.catalyst.parser.SqlBaseParser$SingleDataTypeContext =>  string <EOF>
org.apache.spark.sql.catalyst.parser.SqlBaseParser$PrimitiveDataTypeContext =>  string
21:32:57.624 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
org.apache.spark.sql.catalyst.parser.SqlBaseParser$SingleDataTypeContext =>  string <EOF>
org.apache.spark.sql.catalyst.parser.SqlBaseParser$PrimitiveDataTypeContext =>  string
21:32:57.662 [ScalaTest-run-running-PhysicalPlanSpec] WARN  org.apache.hadoop.hive.ql.session.SessionState - METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.
21:32:57.663 [ScalaTest-run-running-PhysicalPlanSpec] INFO  hive.metastore - Mestastore configuration hive.metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
21:32:57.663 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG hive.ql.metadata.Hive - Creating new db. db = org.apache.hadoop.hive.ql.metadata.Hive@13cdfd26, needsRefresh = false, db.isCurrentUserOwner = true
21:32:57.663 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG hive.ql.metadata.Hive - Closing current thread's connection to Hive Metastore.
21:32:57.664 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG hive.ql.metadata.Hive - Closing current thread's connection to Hive Metastore.
21:32:57.680 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG hive.log - DDL: struct order_all { i64 order_id, i64 user_id, i64 region_id, string order_num, string user_name, string region_name}
21:32:57.746 [ScalaTest-run-running-PhysicalPlanSpec] WARN  org.apache.hadoop.hive.conf.HiveConf - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist
21:32:57.746 [ScalaTest-run-running-PhysicalPlanSpec] WARN  org.apache.hadoop.hive.conf.HiveConf - HiveConf of name hive.stats.jdbc.timeout does not exist
21:32:57.746 [ScalaTest-run-running-PhysicalPlanSpec] WARN  org.apache.hadoop.hive.conf.HiveConf - HiveConf of name hive.stats.retries.wait does not exist
21:32:57.754 [ScalaTest-run-running-PhysicalPlanSpec] INFO  hive.log - Updating table stats fast for order_all
21:32:57.754 [ScalaTest-run-running-PhysicalPlanSpec] INFO  hive.log - Updated size of table order_all to 97265
