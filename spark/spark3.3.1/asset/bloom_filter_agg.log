21:16:22.849 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.execution.SparkSqlParser - Parsing command:
SELECT t1.key, t1.value1, t2.value2
FROM t1 JOIN t2 ON t1.key = t2.key
where t2.value2 > 0.01

org.apache.spark.sql.catalyst.parser.SqlBaseParser$SingleStatementContext =>  SELECTt1.key,t1.value1,t2.value2FROMt1JOINt2ONt1.key=t2.keywheret2.value2>0.01 <EOF>
org.apache.spark.sql.catalyst.parser.SqlBaseParser$SingleStatementContext =>  SELECTt1.key,t1.value1,t2.value2FROMt1JOINt2ONt1.key=t2.keywheret2.value2>0.01 <EOF>
org.apache.spark.sql.catalyst.parser.SqlBaseParser$QueryContext =>  SELECTt1.key,t1.value1,t2.value2FROMt1JOINt2ONt1.key=t2.keywheret2.value2>0.01
org.apache.spark.sql.catalyst.parser.SqlBaseParser$RegularQuerySpecificationContext =>  SELECTt1.key,t1.value1,t2.value2 FROMt1JOINt2ONt1.key=t2.key wheret2.value2>0.01
org.apache.spark.sql.catalyst.parser.SqlBaseParser$FromClauseContext =>  FROM t1JOINt2ONt1.key=t2.key
org.apache.spark.sql.catalyst.parser.SqlBaseParser$TableNameContext =>  t1
org.apache.spark.sql.catalyst.parser.SqlBaseParser$MultipartIdentifierContext =>  t1
org.apache.spark.sql.catalyst.parser.SqlBaseParser$JoinRelationContext =>   JOIN t2 ONt1.key=t2.key
org.apache.spark.sql.catalyst.parser.SqlBaseParser$PredicatedContext =>  t1.key=t2.key
org.apache.spark.sql.catalyst.parser.SqlBaseParser$ComparisonContext =>  t1.key = t2.key
org.apache.spark.sql.catalyst.parser.SqlBaseParser$DereferenceContext =>  t1 . key
org.apache.spark.sql.catalyst.parser.SqlBaseParser$ColumnReferenceContext =>  t1
org.apache.spark.sql.catalyst.parser.SqlBaseParser$DereferenceContext =>  t2 . key
org.apache.spark.sql.catalyst.parser.SqlBaseParser$ColumnReferenceContext =>  t2
org.apache.spark.sql.catalyst.parser.SqlBaseParser$TableNameContext =>  t2
org.apache.spark.sql.catalyst.parser.SqlBaseParser$MultipartIdentifierContext =>  t2
org.apache.spark.sql.catalyst.parser.SqlBaseParser$RegularQuerySpecificationContext =>  SELECTt1.key,t1.value1,t2.value2 FROMt1JOINt2ONt1.key=t2.key wheret2.value2>0.01
org.apache.spark.sql.catalyst.parser.SqlBaseParser$NamedExpressionContext =>  t1.key
org.apache.spark.sql.catalyst.parser.SqlBaseParser$PredicatedContext =>  t1.key
org.apache.spark.sql.catalyst.parser.SqlBaseParser$DereferenceContext =>  t1 . key
org.apache.spark.sql.catalyst.parser.SqlBaseParser$ColumnReferenceContext =>  t1
org.apache.spark.sql.catalyst.parser.SqlBaseParser$NamedExpressionContext =>  t1.value1
org.apache.spark.sql.catalyst.parser.SqlBaseParser$PredicatedContext =>  t1.value1
org.apache.spark.sql.catalyst.parser.SqlBaseParser$DereferenceContext =>  t1 . value1
org.apache.spark.sql.catalyst.parser.SqlBaseParser$ColumnReferenceContext =>  t1
org.apache.spark.sql.catalyst.parser.SqlBaseParser$NamedExpressionContext =>  t2.value2
org.apache.spark.sql.catalyst.parser.SqlBaseParser$PredicatedContext =>  t2.value2
org.apache.spark.sql.catalyst.parser.SqlBaseParser$DereferenceContext =>  t2 . value2
org.apache.spark.sql.catalyst.parser.SqlBaseParser$ColumnReferenceContext =>  t2
org.apache.spark.sql.catalyst.parser.SqlBaseParser$PredicatedContext =>  t2.value2>0.01
org.apache.spark.sql.catalyst.parser.SqlBaseParser$ComparisonContext =>  t2.value2 > 0.01
org.apache.spark.sql.catalyst.parser.SqlBaseParser$DereferenceContext =>  t2 . value2
org.apache.spark.sql.catalyst.parser.SqlBaseParser$ColumnReferenceContext =>  t2
org.apache.spark.sql.catalyst.parser.SqlBaseParser$DecimalLiteralContext =>  0.01
org.apache.spark.sql.catalyst.parser.SqlBaseParser$QueryOrganizationContext =>
Unresolved Logical Plan ====>
'Project ['t1.key, 't1.value1, 't2.value2]
+- 'Filter ('t2.value2 > 0.01)
   +- 'Join Inner, ('t1.key = 't2.key)
      :- 'UnresolvedRelation [t1], [], false
      +- 'UnresolvedRelation [t2], [], false

21:16:22.931 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
org.apache.spark.sql.catalyst.parser.SqlBaseParser$SingleDataTypeContext =>  bigint <EOF>
org.apache.spark.sql.catalyst.parser.SqlBaseParser$PrimitiveDataTypeContext =>  bigint
21:16:22.931 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: double
org.apache.spark.sql.catalyst.parser.SqlBaseParser$SingleDataTypeContext =>  double <EOF>
org.apache.spark.sql.catalyst.parser.SqlBaseParser$PrimitiveDataTypeContext =>  double
21:16:22.962 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
org.apache.spark.sql.catalyst.parser.SqlBaseParser$SingleDataTypeContext =>  bigint <EOF>
org.apache.spark.sql.catalyst.parser.SqlBaseParser$PrimitiveDataTypeContext =>  bigint
21:16:22.962 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: double
org.apache.spark.sql.catalyst.parser.SqlBaseParser$SingleDataTypeContext =>  double <EOF>
org.apache.spark.sql.catalyst.parser.SqlBaseParser$PrimitiveDataTypeContext =>  double
Analyzer(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$1@5f346355) org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations --> old【
'Project ['t1.key, 't1.value1, 't2.value2]
+- 'Filter ('t2.value2 > 0.01)
   +- 'Join Inner, ('t1.key = 't2.key)
      :- 'UnresolvedRelation [t1], [], false
      +- 'UnresolvedRelation [t2], [], false
】--> new【
'Project ['t1.key, 't1.value1, 't2.value2]
+- 'Filter ('t2.value2 > 0.01)
   +- 'Join Inner, ('t1.key = 't2.key)
      :- 'SubqueryAlias spark_catalog.default.t1
      :  +- 'UnresolvedCatalogRelation `default`.`t1`, org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, [], false
      +- 'SubqueryAlias spark_catalog.default.t2
         +- 'UnresolvedCatalogRelation `default`.`t2`, org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, [], false
】


21:16:22.981 [ScalaTest-run-running-PhysicalPlanSpec] INFO  org.apache.spark.sql.execution.datasources.InMemoryFileIndex - It took 4 ms to list leaf files for 1 paths.
21:16:22.989 [ScalaTest-run-running-PhysicalPlanSpec] INFO  org.apache.spark.sql.execution.datasources.InMemoryFileIndex - It took 2 ms to list leaf files for 1 paths.
Analyzer(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$1@5f346355) org.apache.spark.sql.execution.datasources.FindDataSourceTable --> old【
'Project ['t1.key, 't1.value1, 't2.value2]
+- 'Filter ('t2.value2 > 0.01)
   +- 'Join Inner, ('t1.key = 't2.key)
      :- 'SubqueryAlias spark_catalog.default.t1
      :  +- 'UnresolvedCatalogRelation `default`.`t1`, org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, [], false
      +- 'SubqueryAlias spark_catalog.default.t2
         +- 'UnresolvedCatalogRelation `default`.`t2`, org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, [], false
】--> new【
'Project ['t1.key, 't1.value1, 't2.value2]
+- 'Filter ('t2.value2 > 0.01)
   +- 'Join Inner, ('t1.key = 't2.key)
      :- SubqueryAlias spark_catalog.default.t1
      :  +- Relation default.t1[key#59L,value1#60] parquet
      +- SubqueryAlias spark_catalog.default.t2
         +- Relation default.t2[key#61L,value2#62] parquet
】


Analyzer(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$1@5f346355) org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences --> old【
'Project ['t1.key, 't1.value1, 't2.value2]
+- 'Filter ('t2.value2 > 0.01)
   +- 'Join Inner, ('t1.key = 't2.key)
      :- SubqueryAlias spark_catalog.default.t1
      :  +- Relation default.t1[key#59L,value1#60] parquet
      +- SubqueryAlias spark_catalog.default.t2
         +- Relation default.t2[key#61L,value2#62] parquet
】--> new【
'Project ['t1.key, 't1.value1, 't2.value2]
+- 'Filter (value2#62 > 0.01)
   +- Join Inner, (key#59L = key#61L)
      :- SubqueryAlias spark_catalog.default.t1
      :  +- Relation default.t1[key#59L,value1#60] parquet
      +- SubqueryAlias spark_catalog.default.t2
         +- Relation default.t2[key#61L,value2#62] parquet
】


Analyzer(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$1@5f346355) org.apache.spark.sql.catalyst.analysis.TypeCoercionBase$CombinedTypeCoercionRule --> old【
'Project ['t1.key, 't1.value1, 't2.value2]
+- 'Filter (value2#62 > 0.01)
   +- Join Inner, (key#59L = key#61L)
      :- SubqueryAlias spark_catalog.default.t1
      :  +- Relation default.t1[key#59L,value1#60] parquet
      +- SubqueryAlias spark_catalog.default.t2
         +- Relation default.t2[key#61L,value2#62] parquet
】--> new【
'Project ['t1.key, 't1.value1, 't2.value2]
+- Filter (value2#62 > cast(0.01 as double))
   +- Join Inner, (key#59L = key#61L)
      :- SubqueryAlias spark_catalog.default.t1
      :  +- Relation default.t1[key#59L,value1#60] parquet
      +- SubqueryAlias spark_catalog.default.t2
         +- Relation default.t2[key#61L,value2#62] parquet
】


Analyzer(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$1@5f346355) org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences --> old【
'Project ['t1.key, 't1.value1, 't2.value2]
+- Filter (value2#62 > cast(0.01 as double))
   +- Join Inner, (key#59L = key#61L)
      :- SubqueryAlias spark_catalog.default.t1
      :  +- Relation default.t1[key#59L,value1#60] parquet
      +- SubqueryAlias spark_catalog.default.t2
         +- Relation default.t2[key#61L,value2#62] parquet
】--> new【
Project [key#59L, value1#60, value2#62]
+- Filter (value2#62 > cast(0.01 as double))
   +- Join Inner, (key#59L = key#61L)
      :- SubqueryAlias spark_catalog.default.t1
      :  +- Relation default.t1[key#59L,value1#60] parquet
      +- SubqueryAlias spark_catalog.default.t2
         +- Relation default.t2[key#61L,value2#62] parquet
】


Analyzer(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$1@5f346355) org.apache.spark.sql.catalyst.analysis.ResolveTimeZone --> old【
Project [key#59L, value1#60, value2#62]
+- Filter (value2#62 > cast(0.01 as double))
   +- Join Inner, (key#59L = key#61L)
      :- SubqueryAlias spark_catalog.default.t1
      :  +- Relation default.t1[key#59L,value1#60] parquet
      +- SubqueryAlias spark_catalog.default.t2
         +- Relation default.t2[key#61L,value2#62] parquet
】--> new【
Project [key#59L, value1#60, value2#62]
+- Filter (value2#62 > cast(0.01 as double))
   +- Join Inner, (key#59L = key#61L)
      :- SubqueryAlias spark_catalog.default.t1
      :  +- Relation default.t1[key#59L,value1#60] parquet
      +- SubqueryAlias spark_catalog.default.t2
         +- Relation default.t2[key#61L,value2#62] parquet
】


RuleExecutor.execute[Analyzer(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$1@5f346355)] final result ==> old【
'Project ['t1.key, 't1.value1, 't2.value2]
+- 'Filter ('t2.value2 > 0.01)
   +- 'Join Inner, ('t1.key = 't2.key)
      :- 'UnresolvedRelation [t1], [], false
      +- 'UnresolvedRelation [t2], [], false
】==> new【
Project [key#59L, value1#60, value2#62]
+- Filter (value2#62 > cast(0.01 as double))
   +- Join Inner, (key#59L = key#61L)
      :- SubqueryAlias spark_catalog.default.t1
      :  +- Relation default.t1[key#59L,value1#60] parquet
      +- SubqueryAlias spark_catalog.default.t2
         +- Relation default.t2[key#61L,value2#62] parquet
】


analyzed ====>
Project [key#59L, value1#60, value2#62]
+- Filter (value2#62 > cast(0.01 as double))
   +- Join Inner, (key#59L = key#61L)
      :- SubqueryAlias spark_catalog.default.t1
      :  +- Relation default.t1[key#59L,value1#60] parquet
      +- SubqueryAlias spark_catalog.default.t2
         +- Relation default.t2[key#61L,value2#62] parquet

21:16:23.093 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.execution.SparkSqlParser - Parsing command: t3
org.apache.spark.sql.catalyst.parser.SqlBaseParser$SingleMultipartIdentifierContext =>  t3 <EOF>
org.apache.spark.sql.catalyst.parser.SqlBaseParser$MultipartIdentifierContext =>  t3
Analyzer(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$1@5f346355) org.apache.spark.sql.execution.datasources.DataSourceAnalysis --> old【
'CreateTable `t3`, Overwrite
+- Project [key#59L, value1#60, value2#62]
   +- Filter (value2#62 > cast(0.01 as double))
      +- Join Inner, (key#59L = key#61L)
         :- SubqueryAlias spark_catalog.default.t1
         :  +- Relation default.t1[key#59L,value1#60] parquet
         +- SubqueryAlias spark_catalog.default.t2
            +- Relation default.t2[key#61L,value2#62] parquet
】--> new【
CreateDataSourceTableAsSelectCommand `t3`, Overwrite, [key, value1, value2]
+- Project [key#59L, value1#60, value2#62]
   +- Filter (value2#62 > cast(0.01 as double))
      +- Join Inner, (key#59L = key#61L)
         :- SubqueryAlias spark_catalog.default.t1
         :  +- Relation default.t1[key#59L,value1#60] parquet
         +- SubqueryAlias spark_catalog.default.t2
            +- Relation default.t2[key#61L,value2#62] parquet
】


RuleExecutor.execute[Analyzer(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$1@5f346355)] final result ==> old【
'CreateTable `t3`, Overwrite
+- Project [key#59L, value1#60, value2#62]
   +- Filter (value2#62 > cast(0.01 as double))
      +- Join Inner, (key#59L = key#61L)
         :- SubqueryAlias spark_catalog.default.t1
         :  +- Relation default.t1[key#59L,value1#60] parquet
         +- SubqueryAlias spark_catalog.default.t2
            +- Relation default.t2[key#61L,value2#62] parquet
】==> new【
CreateDataSourceTableAsSelectCommand `t3`, Overwrite, [key, value1, value2]
+- Project [key#59L, value1#60, value2#62]
   +- Filter (value2#62 > cast(0.01 as double))
      +- Join Inner, (key#59L = key#61L)
         :- SubqueryAlias spark_catalog.default.t1
         :  +- Relation default.t1[key#59L,value1#60] parquet
         +- SubqueryAlias spark_catalog.default.t2
            +- Relation default.t2[key#61L,value2#62] parquet
】


analyzed ====>
CreateDataSourceTableAsSelectCommand `t3`, Overwrite, [key, value1, value2]
+- Project [key#59L, value1#60, value2#62]
   +- Filter (value2#62 > cast(0.01 as double))
      +- Join Inner, (key#59L = key#61L)
         :- SubqueryAlias spark_catalog.default.t1
         :  +- Relation default.t1[key#59L,value1#60] parquet
         +- SubqueryAlias spark_catalog.default.t2
            +- Relation default.t2[key#61L,value2#62] parquet

analyzed ====>
CreateDataSourceTableAsSelectCommand `t3`, Overwrite, [key, value1, value2]
+- Project [key#59L, value1#60, value2#62]
   +- Filter (value2#62 > cast(0.01 as double))
      +- Join Inner, (key#59L = key#61L)
         :- SubqueryAlias spark_catalog.default.t1
         :  +- Relation default.t1[key#59L,value1#60] parquet
         +- SubqueryAlias spark_catalog.default.t2
            +- Relation default.t2[key#61L,value2#62] parquet

SparkOptimizer(org.apache.spark.sql.internal.BaseSessionStateBuilder$$anon$2@4f31e47a) org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis --> old【
CreateDataSourceTableAsSelectCommand `t3`, Overwrite, [key, value1, value2]
+- Project [key#59L, value1#60, value2#62]
   +- Filter (value2#62 > cast(0.01 as double))
      +- Join Inner, (key#59L = key#61L)
         :- SubqueryAlias spark_catalog.default.t1
         :  +- Relation default.t1[key#59L,value1#60] parquet
         +- SubqueryAlias spark_catalog.default.t2
            +- Relation default.t2[key#61L,value2#62] parquet
】--> new【
CreateDataSourceTableAsSelectCommand `t3`, Overwrite, [key, value1, value2]
+- Project [key#59L, value1#60, value2#62]
   +- Filter (value2#62 > cast(0.01 as double))
      +- Join Inner, (key#59L = key#61L)
         :- Relation default.t1[key#59L,value1#60] parquet
         +- Relation default.t2[key#61L,value2#62] parquet
】


SparkOptimizer(org.apache.spark.sql.internal.BaseSessionStateBuilder$$anon$2@4f31e47a) org.apache.spark.sql.catalyst.optimizer.PushDownPredicates --> old【
CreateDataSourceTableAsSelectCommand `t3`, Overwrite, [key, value1, value2]
+- Project [key#59L, value1#60, value2#62]
   +- Filter (value2#62 > cast(0.01 as double))
      +- Join Inner, (key#59L = key#61L)
         :- Relation default.t1[key#59L,value1#60] parquet
         +- Relation default.t2[key#61L,value2#62] parquet
】--> new【
CreateDataSourceTableAsSelectCommand `t3`, Overwrite, [key, value1, value2]
+- Project [key#59L, value1#60, value2#62]
   +- Join Inner, (key#59L = key#61L)
      :- Relation default.t1[key#59L,value1#60] parquet
      +- Filter (value2#62 > cast(0.01 as double))
         +- Relation default.t2[key#61L,value2#62] parquet
】


SparkOptimizer(org.apache.spark.sql.internal.BaseSessionStateBuilder$$anon$2@4f31e47a) org.apache.spark.sql.catalyst.optimizer.ConstantFolding --> old【
CreateDataSourceTableAsSelectCommand `t3`, Overwrite, [key, value1, value2]
+- Project [key#59L, value1#60, value2#62]
   +- Join Inner, (key#59L = key#61L)
      :- Relation default.t1[key#59L,value1#60] parquet
      +- Filter (value2#62 > cast(0.01 as double))
         +- Relation default.t2[key#61L,value2#62] parquet
】--> new【
CreateDataSourceTableAsSelectCommand `t3`, Overwrite, [key, value1, value2]
+- Project [key#59L, value1#60, value2#62]
   +- Join Inner, (key#59L = key#61L)
      :- Relation default.t1[key#59L,value1#60] parquet
      +- Filter (value2#62 > 0.01)
         +- Relation default.t2[key#61L,value2#62] parquet
】


SparkOptimizer(org.apache.spark.sql.internal.BaseSessionStateBuilder$$anon$2@4f31e47a) org.apache.spark.sql.catalyst.optimizer.InferFiltersFromConstraints --> old【
CreateDataSourceTableAsSelectCommand `t3`, Overwrite, [key, value1, value2]
+- Project [key#59L, value1#60, value2#62]
   +- Join Inner, (key#59L = key#61L)
      :- Relation default.t1[key#59L,value1#60] parquet
      +- Filter (value2#62 > 0.01)
         +- Relation default.t2[key#61L,value2#62] parquet
】--> new【
CreateDataSourceTableAsSelectCommand `t3`, Overwrite, [key, value1, value2]
+- Project [key#59L, value1#60, value2#62]
   +- Join Inner, (key#59L = key#61L)
      :- Filter isnotnull(key#59L)
      :  +- Relation default.t1[key#59L,value1#60] parquet
      +- Filter isnotnull(key#61L)
         +- Filter (isnotnull(value2#62) AND (value2#62 > 0.01))
            +- Relation default.t2[key#61L,value2#62] parquet
】


SparkOptimizer(org.apache.spark.sql.internal.BaseSessionStateBuilder$$anon$2@4f31e47a) org.apache.spark.sql.catalyst.optimizer.PushDownPredicates --> old【
CreateDataSourceTableAsSelectCommand `t3`, Overwrite, [key, value1, value2]
+- Project [key#59L, value1#60, value2#62]
   +- Join Inner, (key#59L = key#61L)
      :- Filter isnotnull(key#59L)
      :  +- Relation default.t1[key#59L,value1#60] parquet
      +- Filter isnotnull(key#61L)
         +- Filter (isnotnull(value2#62) AND (value2#62 > 0.01))
            +- Relation default.t2[key#61L,value2#62] parquet
】--> new【
CreateDataSourceTableAsSelectCommand `t3`, Overwrite, [key, value1, value2]
+- Project [key#59L, value1#60, value2#62]
   +- Join Inner, (key#59L = key#61L)
      :- Filter isnotnull(key#59L)
      :  +- Relation default.t1[key#59L,value1#60] parquet
      +- Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L))
         +- Relation default.t2[key#61L,value2#62] parquet
】


21:16:23.234 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - Considering join on: Some((key#59L = key#61L))
21:16:23.236 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - leftKeys:List(key#59L) | rightKeys:List(key#61L)
21:16:23.237 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - Considering join on: Some((key#59L = key#61L))
21:16:23.237 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - leftKeys:List(key#59L) | rightKeys:List(key#61L)
21:16:23.239 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - Considering join on: Some((key#59L = key#61L))
21:16:23.239 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - leftKeys:List(key#59L) | rightKeys:List(key#61L)
SparkOptimizer(org.apache.spark.sql.internal.BaseSessionStateBuilder$$anon$2@4f31e47a) org.apache.spark.sql.catalyst.optimizer.InjectRuntimeFilter --> old【
CreateDataSourceTableAsSelectCommand `t3`, Overwrite, [key, value1, value2]
+- Project [key#59L, value1#60, value2#62]
   +- Join Inner, (key#59L = key#61L)
      :- Filter isnotnull(key#59L)
      :  +- Relation default.t1[key#59L,value1#60] parquet
      +- Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L))
         +- Relation default.t2[key#61L,value2#62] parquet
】--> new【
CreateDataSourceTableAsSelectCommand `t3`, Overwrite, [key, value1, value2]
+- Project [key#59L, value1#60, value2#62]
   +- Join Inner, (key#59L = key#61L)
      :- Filter might_contain(scalar-subquery#73 [], xxhash64(key#59L, 42))
      :  :  +- Aggregate [bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0) AS bloomFilter#72]
      :  :     +- Project [key#61L]
      :  :        +- Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L))
      :  :           +- Relation default.t2[key#61L,value2#62] parquet
      :  +- Filter isnotnull(key#59L)
      :     +- Relation default.t1[key#59L,value1#60] parquet
      +- Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L))
         +- Relation default.t2[key#61L,value2#62] parquet
】


SparkOptimizer(org.apache.spark.sql.internal.BaseSessionStateBuilder$$anon$2@4f31e47a) org.apache.spark.sql.catalyst.optimizer.PushDownPredicates --> old【
CreateDataSourceTableAsSelectCommand `t3`, Overwrite, [key, value1, value2]
+- Project [key#59L, value1#60, value2#62]
   +- Join Inner, (key#59L = key#61L)
      :- Filter might_contain(scalar-subquery#73 [], xxhash64(key#59L, 42))
      :  :  +- Aggregate [bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0) AS bloomFilter#72]
      :  :     +- Project [key#61L]
      :  :        +- Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L))
      :  :           +- Relation default.t2[key#61L,value2#62] parquet
      :  +- Filter isnotnull(key#59L)
      :     +- Relation default.t1[key#59L,value1#60] parquet
      +- Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L))
         +- Relation default.t2[key#61L,value2#62] parquet
】--> new【
CreateDataSourceTableAsSelectCommand `t3`, Overwrite, [key, value1, value2]
+- Project [key#59L, value1#60, value2#62]
   +- Join Inner, (key#59L = key#61L)
      :- Filter (isnotnull(key#59L) AND might_contain(scalar-subquery#73 [], xxhash64(key#59L, 42)))
      :  :  +- Aggregate [bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0) AS bloomFilter#72]
      :  :     +- Project [key#61L]
      :  :        +- Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L))
      :  :           +- Relation default.t2[key#61L,value2#62] parquet
      :  +- Relation default.t1[key#59L,value1#60] parquet
      +- Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L))
         +- Relation default.t2[key#61L,value2#62] parquet
】


RuleExecutor.execute[SparkOptimizer(org.apache.spark.sql.internal.BaseSessionStateBuilder$$anon$2@4f31e47a)] final result ==> old【
CreateDataSourceTableAsSelectCommand `t3`, Overwrite, [key, value1, value2]
+- Project [key#59L, value1#60, value2#62]
   +- Filter (value2#62 > cast(0.01 as double))
      +- Join Inner, (key#59L = key#61L)
         :- SubqueryAlias spark_catalog.default.t1
         :  +- Relation default.t1[key#59L,value1#60] parquet
         +- SubqueryAlias spark_catalog.default.t2
            +- Relation default.t2[key#61L,value2#62] parquet
】==> new【
CreateDataSourceTableAsSelectCommand `t3`, Overwrite, [key, value1, value2]
+- Project [key#59L, value1#60, value2#62]
   +- Join Inner, (key#59L = key#61L)
      :- Filter (isnotnull(key#59L) AND might_contain(scalar-subquery#73 [], xxhash64(key#59L, 42)))
      :  :  +- Aggregate [bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0) AS bloomFilter#72]
      :  :     +- Project [key#61L]
      :  :        +- Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L))
      :  :           +- Relation default.t2[key#61L,value2#62] parquet
      :  +- Relation default.t1[key#59L,value1#60] parquet
      +- Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L))
         +- Relation default.t2[key#61L,value2#62] parquet
】


optimizedPlan ====>
CreateDataSourceTableAsSelectCommand `t3`, Overwrite, [key, value1, value2]
+- Project [key#59L, value1#60, value2#62]
   +- Join Inner, (key#59L = key#61L)
      :- Filter (isnotnull(key#59L) AND might_contain(scalar-subquery#73 [], xxhash64(key#59L, 42)))
      :  :  +- Aggregate [bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0) AS bloomFilter#72]
      :  :     +- Project [key#61L]
      :  :        +- Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L))
      :  :           +- Relation default.t2[key#61L,value2#62] parquet
      :  +- Relation default.t1[key#59L,value1#60] parquet
      +- Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L))
         +- Relation default.t2[key#61L,value2#62] parquet

SparkPlanner(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2@125ace20) - org.apache.spark.sql.execution.SparkStrategies$SpecialLimits$ -->【
ReturnAnswer
+- CreateDataSourceTableAsSelectCommand `t3`, Overwrite, [key, value1, value2]
   +- Project [key#59L, value1#60, value2#62]
      +- Join Inner, (key#59L = key#61L)
         :- Filter (isnotnull(key#59L) AND might_contain(scalar-subquery#73 [], xxhash64(key#59L, 42)))
         :  :  +- Aggregate [bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0) AS bloomFilter#72]
         :  :     +- Project [key#61L]
         :  :        +- Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L))
         :  :           +- Relation default.t2[key#61L,value2#62] parquet
         :  +- Relation default.t1[key#59L,value1#60] parquet
         +- Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L))
            +- Relation default.t2[key#61L,value2#62] parquet
】 --> 【
PlanLater CreateDataSourceTableAsSelectCommand `t3`, Overwrite, [key, value1, value2]
】

SparkPlanner(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2@125ace20) - org.apache.spark.sql.execution.SparkStrategies$BasicOperators$ -->【
CreateDataSourceTableAsSelectCommand `t3`, Overwrite, [key, value1, value2]
+- Project [key#59L, value1#60, value2#62]
   +- Join Inner, (key#59L = key#61L)
      :- Filter (isnotnull(key#59L) AND might_contain(scalar-subquery#73 [], xxhash64(key#59L, 42)))
      :  :  +- Aggregate [bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0) AS bloomFilter#72]
      :  :     +- Project [key#61L]
      :  :        +- Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L))
      :  :           +- Relation default.t2[key#61L,value2#62] parquet
      :  +- Relation default.t1[key#59L,value1#60] parquet
      +- Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L))
         +- Relation default.t2[key#61L,value2#62] parquet
】 --> 【
Execute CreateDataSourceTableAsSelectCommand `t3`, Overwrite, [key, value1, value2]
+- PlanLater Project [key#59L, value1#60, value2#62]
】

SparkPlanner(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2@125ace20) - org.apache.spark.sql.execution.SparkStrategies$BasicOperators$ -->【
Project [key#59L, value1#60, value2#62]
+- Join Inner, (key#59L = key#61L)
   :- Filter (isnotnull(key#59L) AND might_contain(scalar-subquery#73 [], xxhash64(key#59L, 42)))
   :  :  +- Aggregate [bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0) AS bloomFilter#72]
   :  :     +- Project [key#61L]
   :  :        +- Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L))
   :  :           +- Relation default.t2[key#61L,value2#62] parquet
   :  +- Relation default.t1[key#59L,value1#60] parquet
   +- Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L))
      +- Relation default.t2[key#61L,value2#62] parquet
】 --> 【
Project [key#59L, value1#60, value2#62]
+- PlanLater Join Inner, (key#59L = key#61L)
】

21:16:23.329 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - Considering join on: Some((key#59L = key#61L))
21:16:23.329 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - leftKeys:List(key#59L) | rightKeys:List(key#61L)
21:16:23.332 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - Considering join on: Some((key#59L = key#61L))
21:16:23.332 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - leftKeys:List(key#59L) | rightKeys:List(key#61L)
SparkPlanner(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2@125ace20) - org.apache.spark.sql.execution.SparkStrategies$JoinSelection$ -->【
Join Inner, (key#59L = key#61L)
:- Filter (isnotnull(key#59L) AND might_contain(scalar-subquery#73 [], xxhash64(key#59L, 42)))
:  :  +- Aggregate [bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0) AS bloomFilter#72]
:  :     +- Project [key#61L]
:  :        +- Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L))
:  :           +- Relation default.t2[key#61L,value2#62] parquet
:  +- Relation default.t1[key#59L,value1#60] parquet
+- Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L))
   +- Relation default.t2[key#61L,value2#62] parquet
】 --> 【
SortMergeJoin [key#59L], [key#61L], Inner
:- PlanLater Filter (isnotnull(key#59L) AND might_contain(scalar-subquery#73 [], xxhash64(key#59L, 42)))
+- PlanLater Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L))
】

21:16:23.356 [ScalaTest-run-running-PhysicalPlanSpec] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: IsNotNull(key)
21:16:23.357 [ScalaTest-run-running-PhysicalPlanSpec] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(key#59L),might_contain(scalar-subquery#73 [], xxhash64(key#59L, 42))
21:16:23.359 [ScalaTest-run-running-PhysicalPlanSpec] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<key: bigint, value1: double>
SparkPlanner(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2@125ace20) - org.apache.spark.sql.execution.datasources.FileSourceStrategy$ -->【
Filter (isnotnull(key#59L) AND might_contain(scalar-subquery#73 [], xxhash64(key#59L, 42)))
:  +- Aggregate [bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0) AS bloomFilter#72]
:     +- Project [key#61L]
:        +- Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L))
:           +- Relation default.t2[key#61L,value2#62] parquet
+- Relation default.t1[key#59L,value1#60] parquet
】 --> 【
Project [key#59L, value1#60]
+- Filter (isnotnull(key#59L) AND might_contain(scalar-subquery#73 [], xxhash64(key#59L, 42)))
   :  +- Aggregate [bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0) AS bloomFilter#72]
   :     +- Project [key#61L]
   :        +- Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L))
   :           +- Relation default.t2[key#61L,value2#62] parquet
   +- FileScan parquet default.t1[key#59L,value1#60] Batched: true, DataFilters: [isnotnull(key#59L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(key)], ReadSchema: struct<key:bigint,value1:double>
】

21:16:23.385 [ScalaTest-run-running-PhysicalPlanSpec] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: IsNotNull(value2),GreaterThan(value2,0.01),IsNotNull(key)
21:16:23.385 [ScalaTest-run-running-PhysicalPlanSpec] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(value2#62),(value2#62 > 0.01),isnotnull(key#61L)
21:16:23.385 [ScalaTest-run-running-PhysicalPlanSpec] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<key: bigint, value2: double>
SparkPlanner(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2@125ace20) - org.apache.spark.sql.execution.datasources.FileSourceStrategy$ -->【
Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L))
+- Relation default.t2[key#61L,value2#62] parquet
】 --> 【
Project [key#61L, value2#62]
+- Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L))
   +- FileScan parquet default.t2[key#61L,value2#62] Batched: true, DataFilters: [isnotnull(value2#62), (value2#62 > 0.01), isnotnull(key#61L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(value2), GreaterThan(value2,0.01), IsNotNull(key)], ReadSchema: struct<key:bigint,value2:double>
】

sparkPlan ====>
Execute CreateDataSourceTableAsSelectCommand `t3`, Overwrite, [key, value1, value2]
+- Project [key#59L, value1#60, value2#62]
   +- SortMergeJoin [key#59L], [key#61L], Inner
      :- Project [key#59L, value1#60]
      :  +- Filter (isnotnull(key#59L) AND might_contain(scalar-subquery#73 [], xxhash64(key#59L, 42)))
      :     :  +- Aggregate [bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0) AS bloomFilter#72]
      :     :     +- Project [key#61L]
      :     :        +- Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L))
      :     :           +- Relation default.t2[key#61L,value2#62] parquet
      :     +- FileScan parquet default.t1[key#59L,value1#60] Batched: true, DataFilters: [isnotnull(key#59L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(key)], ReadSchema: struct<key:bigint,value1:double>
      +- Project [key#61L, value2#62]
         +- Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L))
            +- FileScan parquet default.t2[key#61L,value2#62] Batched: true, DataFilters: [isnotnull(value2#62), (value2#62 > 0.01), isnotnull(key#61L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(value2), GreaterThan(value2,0.01), IsNotNull(key)], ReadSchema: struct<key:bigint,value2:double>

SparkPlanner(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2@125ace20) - org.apache.spark.sql.execution.SparkStrategies$SpecialLimits$ -->【
ReturnAnswer
+- Aggregate [bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0) AS bloomFilter#72]
   +- Project [key#61L]
      +- Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L))
         +- Relation default.t2[key#61L,value2#62] parquet
】 --> 【
PlanLater Aggregate [bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0) AS bloomFilter#72]
】

SparkPlanner(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2@125ace20) - org.apache.spark.sql.execution.SparkStrategies$Aggregation$ -->【
Aggregate [bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0) AS bloomFilter#72]
+- Project [key#61L]
   +- Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L))
      +- Relation default.t2[key#61L,value2#62] parquet
】 --> 【
ObjectHashAggregate(keys=[], functions=[bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0)], output=[bloomFilter#72])
+- ObjectHashAggregate(keys=[], functions=[partial_bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0)], output=[buf#75])
   +- PlanLater Project [key#61L]
】

21:16:23.463 [ScalaTest-run-running-PhysicalPlanSpec] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: IsNotNull(value2),GreaterThan(value2,0.01),IsNotNull(key)
21:16:23.463 [ScalaTest-run-running-PhysicalPlanSpec] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(value2#62),(value2#62 > 0.01),isnotnull(key#61L)
21:16:23.463 [ScalaTest-run-running-PhysicalPlanSpec] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<key: bigint, value2: double>
SparkPlanner(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2@125ace20) - org.apache.spark.sql.execution.datasources.FileSourceStrategy$ -->【
Project [key#61L]
+- Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L))
   +- Relation default.t2[key#61L,value2#62] parquet
】 --> 【
Project [key#61L]
+- Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L))
   +- FileScan parquet default.t2[key#61L,value2#62] Batched: true, DataFilters: [isnotnull(value2#62), (value2#62 > 0.01), isnotnull(key#61L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(value2), GreaterThan(value2,0.01), IsNotNull(key)], ReadSchema: struct<key:bigint,value2:double>
】

21:16:23.471 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.execution.adaptive.InsertAdaptiveSparkPlan - Adaptive execution enabled for plan:
ObjectHashAggregate(keys=[], functions=[bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0)], output=[bloomFilter#72])
+- ObjectHashAggregate(keys=[], functions=[partial_bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0)], output=[buf#75])
   +- Project [key#61L]
      +- Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L))
         +- FileScan parquet default.t2[key#61L,value2#62] Batched: true, DataFilters: [isnotnull(value2#62), (value2#62 > 0.01), isnotnull(key#61L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(value2), GreaterThan(value2,0.01), IsNotNull(key)], ReadSchema: struct<key:bigint,value2:double>
 ==>
ObjectHashAggregate(keys=[], functions=[bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0)], output=[bloomFilter#72])
+- ObjectHashAggregate(keys=[], functions=[partial_bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0)], output=[buf#75])
   +- Project [key#61L]
      +- Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L))
         +- FileScan parquet default.t2[key#61L,value2#62] Batched: true, DataFilters: [isnotnull(value2#62), (value2#62 > 0.01), isnotnull(key#61L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(value2), GreaterThan(value2,0.01), IsNotNull(key)], ReadSchema: struct<key:bigint,value2:double>

org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec$ - applyPhysicalRules - org.apache.spark.sql.execution.exchange.EnsureRequirements -->【
ObjectHashAggregate(keys=[], functions=[bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0)], output=[bloomFilter#72])
+- ObjectHashAggregate(keys=[], functions=[partial_bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0)], output=[buf#75])
   +- Project [key#61L]
      +- Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L))
         +- FileScan parquet default.t2[key#61L,value2#62] Batched: true, DataFilters: [isnotnull(value2#62), (value2#62 > 0.01), isnotnull(key#61L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(value2), GreaterThan(value2,0.01), IsNotNull(key)], ReadSchema: struct<key:bigint,value2:double>
】AQE --> 【
ObjectHashAggregate(keys=[], functions=[bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0)], output=[bloomFilter#72])
+- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=55]
   +- ObjectHashAggregate(keys=[], functions=[partial_bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0)], output=[buf#75])
      +- Project [key#61L]
         +- Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L))
            +- FileScan parquet default.t2[key#61L,value2#62] Batched: true, DataFilters: [isnotnull(value2#62), (value2#62 > 0.01), isnotnull(key#61L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(value2), GreaterThan(value2,0.01), IsNotNull(key)], ReadSchema: struct<key:bigint,value2:double>
】

org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec$ final result ==>【
ObjectHashAggregate(keys=[], functions=[bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0)], output=[bloomFilter#72])
+- ObjectHashAggregate(keys=[], functions=[partial_bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0)], output=[buf#75])
   +- Project [key#61L]
      +- Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L))
         +- FileScan parquet default.t2[key#61L,value2#62] Batched: true, DataFilters: [isnotnull(value2#62), (value2#62 > 0.01), isnotnull(key#61L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(value2), GreaterThan(value2,0.01), IsNotNull(key)], ReadSchema: struct<key:bigint,value2:double>
】AQE ==> 【
ObjectHashAggregate(keys=[], functions=[bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0)], output=[bloomFilter#72])
+- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=55]
   +- ObjectHashAggregate(keys=[], functions=[partial_bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0)], output=[buf#75])
      +- Project [key#61L]
         +- Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L))
            +- FileScan parquet default.t2[key#61L,value2#62] Batched: true, DataFilters: [isnotnull(value2#62), (value2#62 > 0.01), isnotnull(key#61L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(value2), GreaterThan(value2,0.01), IsNotNull(key)], ReadSchema: struct<key:bigint,value2:double>
】

21:16:23.541 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.execution.adaptive.InsertAdaptiveSparkPlan - Adaptive execution enabled for plan:
Project [key#59L, value1#60, value2#62]
+- SortMergeJoin [key#59L], [key#61L], Inner
   :- Project [key#59L, value1#60]
   :  +- Filter (isnotnull(key#59L) AND might_contain(scalar-subquery#73 [], xxhash64(key#59L, 42)))
   :     :  +- Aggregate [bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0) AS bloomFilter#72]
   :     :     +- Project [key#61L]
   :     :        +- Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L))
   :     :           +- Relation default.t2[key#61L,value2#62] parquet
   :     +- FileScan parquet default.t1[key#59L,value1#60] Batched: true, DataFilters: [isnotnull(key#59L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(key)], ReadSchema: struct<key:bigint,value1:double>
   +- Project [key#61L, value2#62]
      +- Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L))
         +- FileScan parquet default.t2[key#61L,value2#62] Batched: true, DataFilters: [isnotnull(value2#62), (value2#62 > 0.01), isnotnull(key#61L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(value2), GreaterThan(value2,0.01), IsNotNull(key)], ReadSchema: struct<key:bigint,value2:double>
 ==>
Project [key#59L, value1#60, value2#62]
+- SortMergeJoin [key#59L], [key#61L], Inner
   :- Project [key#59L, value1#60]
   :  +- Filter (isnotnull(key#59L) AND might_contain(Subquery subquery#73, [id=#57], xxhash64(key#59L, 42)))
   :     :  +- Subquery subquery#73, [id=#57]
   :     :     +- AdaptiveSparkPlan isFinalPlan=false
   :     :        +- ObjectHashAggregate(keys=[], functions=[bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0)], output=[bloomFilter#72])
   :     :           +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=55]
   :     :              +- ObjectHashAggregate(keys=[], functions=[partial_bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0)], output=[buf#75])
   :     :                 +- Project [key#61L]
   :     :                    +- Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L))
   :     :                       +- FileScan parquet default.t2[key#61L,value2#62] Batched: true, DataFilters: [isnotnull(value2#62), (value2#62 > 0.01), isnotnull(key#61L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(value2), GreaterThan(value2,0.01), IsNotNull(key)], ReadSchema: struct<key:bigint,value2:double>
   :     +- FileScan parquet default.t1[key#59L,value1#60] Batched: true, DataFilters: [isnotnull(key#59L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(key)], ReadSchema: struct<key:bigint,value1:double>
   +- Project [key#61L, value2#62]
      +- Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L))
         +- FileScan parquet default.t2[key#61L,value2#62] Batched: true, DataFilters: [isnotnull(value2#62), (value2#62 > 0.01), isnotnull(key#61L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(value2), GreaterThan(value2,0.01), IsNotNull(key)], ReadSchema: struct<key:bigint,value2:double>

org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec$ - applyPhysicalRules - org.apache.spark.sql.execution.RemoveRedundantProjects$ -->【
Project [key#59L, value1#60, value2#62]
+- SortMergeJoin [key#59L], [key#61L], Inner
   :- Project [key#59L, value1#60]
   :  +- Filter (isnotnull(key#59L) AND might_contain(Subquery subquery#73, [id=#57], xxhash64(key#59L, 42)))
   :     :  +- Subquery subquery#73, [id=#57]
   :     :     +- AdaptiveSparkPlan isFinalPlan=false
   :     :        +- ObjectHashAggregate(keys=[], functions=[bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0)], output=[bloomFilter#72])
   :     :           +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=55]
   :     :              +- ObjectHashAggregate(keys=[], functions=[partial_bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0)], output=[buf#75])
   :     :                 +- Project [key#61L]
   :     :                    +- Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L))
   :     :                       +- FileScan parquet default.t2[key#61L,value2#62] Batched: true, DataFilters: [isnotnull(value2#62), (value2#62 > 0.01), isnotnull(key#61L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(value2), GreaterThan(value2,0.01), IsNotNull(key)], ReadSchema: struct<key:bigint,value2:double>
   :     +- FileScan parquet default.t1[key#59L,value1#60] Batched: true, DataFilters: [isnotnull(key#59L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(key)], ReadSchema: struct<key:bigint,value1:double>
   +- Project [key#61L, value2#62]
      +- Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L))
         +- FileScan parquet default.t2[key#61L,value2#62] Batched: true, DataFilters: [isnotnull(value2#62), (value2#62 > 0.01), isnotnull(key#61L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(value2), GreaterThan(value2,0.01), IsNotNull(key)], ReadSchema: struct<key:bigint,value2:double>
】AQE --> 【
Project [key#59L, value1#60, value2#62]
+- SortMergeJoin [key#59L], [key#61L], Inner
   :- Filter (isnotnull(key#59L) AND might_contain(Subquery subquery#73, [id=#57], xxhash64(key#59L, 42)))
   :  :  +- Subquery subquery#73, [id=#57]
   :  :     +- AdaptiveSparkPlan isFinalPlan=false
   :  :        +- ObjectHashAggregate(keys=[], functions=[bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0)], output=[bloomFilter#72])
   :  :           +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=55]
   :  :              +- ObjectHashAggregate(keys=[], functions=[partial_bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0)], output=[buf#75])
   :  :                 +- Project [key#61L]
   :  :                    +- Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L))
   :  :                       +- FileScan parquet default.t2[key#61L,value2#62] Batched: true, DataFilters: [isnotnull(value2#62), (value2#62 > 0.01), isnotnull(key#61L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(value2), GreaterThan(value2,0.01), IsNotNull(key)], ReadSchema: struct<key:bigint,value2:double>
   :  +- FileScan parquet default.t1[key#59L,value1#60] Batched: true, DataFilters: [isnotnull(key#59L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(key)], ReadSchema: struct<key:bigint,value1:double>
   +- Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L))
      +- FileScan parquet default.t2[key#61L,value2#62] Batched: true, DataFilters: [isnotnull(value2#62), (value2#62 > 0.01), isnotnull(key#61L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(value2), GreaterThan(value2,0.01), IsNotNull(key)], ReadSchema: struct<key:bigint,value2:double>
】

org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec$ - applyPhysicalRules - org.apache.spark.sql.execution.exchange.EnsureRequirements -->【
Project [key#59L, value1#60, value2#62]
+- SortMergeJoin [key#59L], [key#61L], Inner
   :- Filter (isnotnull(key#59L) AND might_contain(Subquery subquery#73, [id=#57], xxhash64(key#59L, 42)))
   :  :  +- Subquery subquery#73, [id=#57]
   :  :     +- AdaptiveSparkPlan isFinalPlan=false
   :  :        +- ObjectHashAggregate(keys=[], functions=[bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0)], output=[bloomFilter#72])
   :  :           +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=55]
   :  :              +- ObjectHashAggregate(keys=[], functions=[partial_bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0)], output=[buf#75])
   :  :                 +- Project [key#61L]
   :  :                    +- Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L))
   :  :                       +- FileScan parquet default.t2[key#61L,value2#62] Batched: true, DataFilters: [isnotnull(value2#62), (value2#62 > 0.01), isnotnull(key#61L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(value2), GreaterThan(value2,0.01), IsNotNull(key)], ReadSchema: struct<key:bigint,value2:double>
   :  +- FileScan parquet default.t1[key#59L,value1#60] Batched: true, DataFilters: [isnotnull(key#59L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(key)], ReadSchema: struct<key:bigint,value1:double>
   +- Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L))
      +- FileScan parquet default.t2[key#61L,value2#62] Batched: true, DataFilters: [isnotnull(value2#62), (value2#62 > 0.01), isnotnull(key#61L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(value2), GreaterThan(value2,0.01), IsNotNull(key)], ReadSchema: struct<key:bigint,value2:double>
】AQE --> 【
Project [key#59L, value1#60, value2#62]
+- SortMergeJoin [key#59L], [key#61L], Inner
   :- Sort [key#59L ASC NULLS FIRST], false, 0
   :  +- Exchange hashpartitioning(key#59L, 3), ENSURE_REQUIREMENTS, [plan_id=66]
   :     +- Filter (isnotnull(key#59L) AND might_contain(Subquery subquery#73, [id=#57], xxhash64(key#59L, 42)))
   :        :  +- Subquery subquery#73, [id=#57]
   :        :     +- AdaptiveSparkPlan isFinalPlan=false
   :        :        +- ObjectHashAggregate(keys=[], functions=[bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0)], output=[bloomFilter#72])
   :        :           +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=55]
   :        :              +- ObjectHashAggregate(keys=[], functions=[partial_bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0)], output=[buf#75])
   :        :                 +- Project [key#61L]
   :        :                    +- Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L))
   :        :                       +- FileScan parquet default.t2[key#61L,value2#62] Batched: true, DataFilters: [isnotnull(value2#62), (value2#62 > 0.01), isnotnull(key#61L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(value2), GreaterThan(value2,0.01), IsNotNull(key)], ReadSchema: struct<key:bigint,value2:double>
   :        +- FileScan parquet default.t1[key#59L,value1#60] Batched: true, DataFilters: [isnotnull(key#59L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(key)], ReadSchema: struct<key:bigint,value1:double>
   +- Sort [key#61L ASC NULLS FIRST], false, 0
      +- Exchange hashpartitioning(key#61L, 3), ENSURE_REQUIREMENTS, [plan_id=67]
         +- Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L))
            +- FileScan parquet default.t2[key#61L,value2#62] Batched: true, DataFilters: [isnotnull(value2#62), (value2#62 > 0.01), isnotnull(key#61L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(value2), GreaterThan(value2,0.01), IsNotNull(key)], ReadSchema: struct<key:bigint,value2:double>
】

org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec$ final result ==>【
Project [key#59L, value1#60, value2#62]
+- SortMergeJoin [key#59L], [key#61L], Inner
   :- Project [key#59L, value1#60]
   :  +- Filter (isnotnull(key#59L) AND might_contain(Subquery subquery#73, [id=#57], xxhash64(key#59L, 42)))
   :     :  +- Subquery subquery#73, [id=#57]
   :     :     +- AdaptiveSparkPlan isFinalPlan=false
   :     :        +- ObjectHashAggregate(keys=[], functions=[bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0)], output=[bloomFilter#72])
   :     :           +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=55]
   :     :              +- ObjectHashAggregate(keys=[], functions=[partial_bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0)], output=[buf#75])
   :     :                 +- Project [key#61L]
   :     :                    +- Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L))
   :     :                       +- FileScan parquet default.t2[key#61L,value2#62] Batched: true, DataFilters: [isnotnull(value2#62), (value2#62 > 0.01), isnotnull(key#61L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(value2), GreaterThan(value2,0.01), IsNotNull(key)], ReadSchema: struct<key:bigint,value2:double>
   :     +- FileScan parquet default.t1[key#59L,value1#60] Batched: true, DataFilters: [isnotnull(key#59L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(key)], ReadSchema: struct<key:bigint,value1:double>
   +- Project [key#61L, value2#62]
      +- Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L))
         +- FileScan parquet default.t2[key#61L,value2#62] Batched: true, DataFilters: [isnotnull(value2#62), (value2#62 > 0.01), isnotnull(key#61L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(value2), GreaterThan(value2,0.01), IsNotNull(key)], ReadSchema: struct<key:bigint,value2:double>
】AQE ==> 【
Project [key#59L, value1#60, value2#62]
+- SortMergeJoin [key#59L], [key#61L], Inner
   :- Sort [key#59L ASC NULLS FIRST], false, 0
   :  +- Exchange hashpartitioning(key#59L, 3), ENSURE_REQUIREMENTS, [plan_id=66]
   :     +- Filter (isnotnull(key#59L) AND might_contain(Subquery subquery#73, [id=#57], xxhash64(key#59L, 42)))
   :        :  +- Subquery subquery#73, [id=#57]
   :        :     +- AdaptiveSparkPlan isFinalPlan=false
   :        :        +- ObjectHashAggregate(keys=[], functions=[bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0)], output=[bloomFilter#72])
   :        :           +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=55]
   :        :              +- ObjectHashAggregate(keys=[], functions=[partial_bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0)], output=[buf#75])
   :        :                 +- Project [key#61L]
   :        :                    +- Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L))
   :        :                       +- FileScan parquet default.t2[key#61L,value2#62] Batched: true, DataFilters: [isnotnull(value2#62), (value2#62 > 0.01), isnotnull(key#61L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(value2), GreaterThan(value2,0.01), IsNotNull(key)], ReadSchema: struct<key:bigint,value2:double>
   :        +- FileScan parquet default.t1[key#59L,value1#60] Batched: true, DataFilters: [isnotnull(key#59L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(key)], ReadSchema: struct<key:bigint,value1:double>
   +- Sort [key#61L ASC NULLS FIRST], false, 0
      +- Exchange hashpartitioning(key#61L, 3), ENSURE_REQUIREMENTS, [plan_id=67]
         +- Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L))
            +- FileScan parquet default.t2[key#61L,value2#62] Batched: true, DataFilters: [isnotnull(value2#62), (value2#62 > 0.01), isnotnull(key#61L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(value2), GreaterThan(value2,0.01), IsNotNull(key)], ReadSchema: struct<key:bigint,value2:double>
】

org.apache.spark.sql.execution.QueryExecution$ - org.apache.spark.sql.execution.adaptive.InsertAdaptiveSparkPlan -->【
Execute CreateDataSourceTableAsSelectCommand `t3`, Overwrite, [key, value1, value2]
+- Project [key#59L, value1#60, value2#62]
   +- SortMergeJoin [key#59L], [key#61L], Inner
      :- Project [key#59L, value1#60]
      :  +- Filter (isnotnull(key#59L) AND might_contain(scalar-subquery#73 [], xxhash64(key#59L, 42)))
      :     :  +- Aggregate [bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0) AS bloomFilter#72]
      :     :     +- Project [key#61L]
      :     :        +- Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L))
      :     :           +- Relation default.t2[key#61L,value2#62] parquet
      :     +- FileScan parquet default.t1[key#59L,value1#60] Batched: true, DataFilters: [isnotnull(key#59L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(key)], ReadSchema: struct<key:bigint,value1:double>
      +- Project [key#61L, value2#62]
         +- Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L))
            +- FileScan parquet default.t2[key#61L,value2#62] Batched: true, DataFilters: [isnotnull(value2#62), (value2#62 > 0.01), isnotnull(key#61L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(value2), GreaterThan(value2,0.01), IsNotNull(key)], ReadSchema: struct<key:bigint,value2:double>
】 --> 【
Execute CreateDataSourceTableAsSelectCommand `t3`, Overwrite, [key, value1, value2]
+- AdaptiveSparkPlan isFinalPlan=false
   +- Project [key#59L, value1#60, value2#62]
      +- SortMergeJoin [key#59L], [key#61L], Inner
         :- Sort [key#59L ASC NULLS FIRST], false, 0
         :  +- Exchange hashpartitioning(key#59L, 3), ENSURE_REQUIREMENTS, [plan_id=66]
         :     +- Filter (isnotnull(key#59L) AND might_contain(Subquery subquery#73, [id=#57], xxhash64(key#59L, 42)))
         :        :  +- Subquery subquery#73, [id=#57]
         :        :     +- AdaptiveSparkPlan isFinalPlan=false
         :        :        +- ObjectHashAggregate(keys=[], functions=[bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0)], output=[bloomFilter#72])
         :        :           +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=55]
         :        :              +- ObjectHashAggregate(keys=[], functions=[partial_bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0)], output=[buf#75])
         :        :                 +- Project [key#61L]
         :        :                    +- Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L))
         :        :                       +- FileScan parquet default.t2[key#61L,value2#62] Batched: true, DataFilters: [isnotnull(value2#62), (value2#62 > 0.01), isnotnull(key#61L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(value2), GreaterThan(value2,0.01), IsNotNull(key)], ReadSchema: struct<key:bigint,value2:double>
         :        +- FileScan parquet default.t1[key#59L,value1#60] Batched: true, DataFilters: [isnotnull(key#59L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(key)], ReadSchema: struct<key:bigint,value1:double>
         +- Sort [key#61L ASC NULLS FIRST], false, 0
            +- Exchange hashpartitioning(key#61L, 3), ENSURE_REQUIREMENTS, [plan_id=67]
               +- Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L))
                  +- FileScan parquet default.t2[key#61L,value2#62] Batched: true, DataFilters: [isnotnull(value2#62), (value2#62 > 0.01), isnotnull(key#61L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(value2), GreaterThan(value2,0.01), IsNotNull(key)], ReadSchema: struct<key:bigint,value2:double>
】

org.apache.spark.sql.execution.QueryExecution$ prepareForExecution final result ==>【
Execute CreateDataSourceTableAsSelectCommand `t3`, Overwrite, [key, value1, value2]
+- Project [key#59L, value1#60, value2#62]
   +- SortMergeJoin [key#59L], [key#61L], Inner
      :- Project [key#59L, value1#60]
      :  +- Filter (isnotnull(key#59L) AND might_contain(scalar-subquery#73 [], xxhash64(key#59L, 42)))
      :     :  +- Aggregate [bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0) AS bloomFilter#72]
      :     :     +- Project [key#61L]
      :     :        +- Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L))
      :     :           +- Relation default.t2[key#61L,value2#62] parquet
      :     +- FileScan parquet default.t1[key#59L,value1#60] Batched: true, DataFilters: [isnotnull(key#59L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(key)], ReadSchema: struct<key:bigint,value1:double>
      +- Project [key#61L, value2#62]
         +- Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L))
            +- FileScan parquet default.t2[key#61L,value2#62] Batched: true, DataFilters: [isnotnull(value2#62), (value2#62 > 0.01), isnotnull(key#61L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(value2), GreaterThan(value2,0.01), IsNotNull(key)], ReadSchema: struct<key:bigint,value2:double>
】 ==> 【
Execute CreateDataSourceTableAsSelectCommand `t3`, Overwrite, [key, value1, value2]
+- AdaptiveSparkPlan isFinalPlan=false
   +- Project [key#59L, value1#60, value2#62]
      +- SortMergeJoin [key#59L], [key#61L], Inner
         :- Sort [key#59L ASC NULLS FIRST], false, 0
         :  +- Exchange hashpartitioning(key#59L, 3), ENSURE_REQUIREMENTS, [plan_id=66]
         :     +- Filter (isnotnull(key#59L) AND might_contain(Subquery subquery#73, [id=#57], xxhash64(key#59L, 42)))
         :        :  +- Subquery subquery#73, [id=#57]
         :        :     +- AdaptiveSparkPlan isFinalPlan=false
         :        :        +- ObjectHashAggregate(keys=[], functions=[bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0)], output=[bloomFilter#72])
         :        :           +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=55]
         :        :              +- ObjectHashAggregate(keys=[], functions=[partial_bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0)], output=[buf#75])
         :        :                 +- Project [key#61L]
         :        :                    +- Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L))
         :        :                       +- FileScan parquet default.t2[key#61L,value2#62] Batched: true, DataFilters: [isnotnull(value2#62), (value2#62 > 0.01), isnotnull(key#61L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(value2), GreaterThan(value2,0.01), IsNotNull(key)], ReadSchema: struct<key:bigint,value2:double>
         :        +- FileScan parquet default.t1[key#59L,value1#60] Batched: true, DataFilters: [isnotnull(key#59L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(key)], ReadSchema: struct<key:bigint,value1:double>
         +- Sort [key#61L ASC NULLS FIRST], false, 0
            +- Exchange hashpartitioning(key#61L, 3), ENSURE_REQUIREMENTS, [plan_id=67]
               +- Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L))
                  +- FileScan parquet default.t2[key#61L,value2#62] Batched: true, DataFilters: [isnotnull(value2#62), (value2#62 > 0.01), isnotnull(key#61L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(value2), GreaterThan(value2,0.01), IsNotNull(key)], ReadSchema: struct<key:bigint,value2:double>
】

executedPlan ====>
Execute CreateDataSourceTableAsSelectCommand `t3`, Overwrite, [key, value1, value2]
+- AdaptiveSparkPlan isFinalPlan=false
   +- Project [key#59L, value1#60, value2#62]
      +- SortMergeJoin [key#59L], [key#61L], Inner
         :- Sort [key#59L ASC NULLS FIRST], false, 0
         :  +- Exchange hashpartitioning(key#59L, 3), ENSURE_REQUIREMENTS, [plan_id=66]
         :     +- Filter (isnotnull(key#59L) AND might_contain(Subquery subquery#73, [id=#57], xxhash64(key#59L, 42)))
         :        :  +- Subquery subquery#73, [id=#57]
         :        :     +- AdaptiveSparkPlan isFinalPlan=false
         :        :        +- ObjectHashAggregate(keys=[], functions=[bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0)], output=[bloomFilter#72])
         :        :           +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=55]
         :        :              +- ObjectHashAggregate(keys=[], functions=[partial_bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0)], output=[buf#75])
         :        :                 +- Project [key#61L]
         :        :                    +- Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L))
         :        :                       +- FileScan parquet default.t2[key#61L,value2#62] Batched: true, DataFilters: [isnotnull(value2#62), (value2#62 > 0.01), isnotnull(key#61L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(value2), GreaterThan(value2,0.01), IsNotNull(key)], ReadSchema: struct<key:bigint,value2:double>
         :        +- FileScan parquet default.t1[key#59L,value1#60] Batched: true, DataFilters: [isnotnull(key#59L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(key)], ReadSchema: struct<key:bigint,value1:double>
         +- Sort [key#61L ASC NULLS FIRST], false, 0
            +- Exchange hashpartitioning(key#61L, 3), ENSURE_REQUIREMENTS, [plan_id=67]
               +- Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L))
                  +- FileScan parquet default.t2[key#61L,value2#62] Batched: true, DataFilters: [isnotnull(value2#62), (value2#62 > 0.01), isnotnull(key#61L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(value2), GreaterThan(value2,0.01), IsNotNull(key)], ReadSchema: struct<key:bigint,value2:double>

21:16:23.644 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.internal.io.FileCommitProtocol - Creating committer org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol; job 5ba29cf7-2c20-4057-bc1e-2e0a7abe896d; output=file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-warehouse/t3; dynamic=false
21:16:23.650 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.internal.io.FileCommitProtocol - Using (String, String, Boolean) constructor
21:16:23.687 [ScalaTest-run-running-PhysicalPlanSpec] INFO  org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat - Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
21:16:23.717 [ScalaTest-run-running-PhysicalPlanSpec] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21:16:23.718 [ScalaTest-run-running-PhysicalPlanSpec] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21:16:23.938 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec - withFinalPlanUpdate.doExecute ==> before get final plan:
Project [key#59L, value1#60, value2#62]
+- SortMergeJoin [key#59L], [key#61L], Inner
   :- Sort [key#59L ASC NULLS FIRST], false, 0
   :  +- Exchange hashpartitioning(key#59L, 3), ENSURE_REQUIREMENTS, [plan_id=66]
   :     +- Filter (isnotnull(key#59L) AND might_contain(Subquery subquery#73, [id=#57], xxhash64(key#59L, 42)))
   :        :  +- Subquery subquery#73, [id=#57]
   :        :     +- AdaptiveSparkPlan isFinalPlan=false
   :        :        +- ObjectHashAggregate(keys=[], functions=[bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0)], output=[bloomFilter#72])
   :        :           +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=55]
   :        :              +- ObjectHashAggregate(keys=[], functions=[partial_bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0)], output=[buf#75])
   :        :                 +- Project [key#61L]
   :        :                    +- Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L))
   :        :                       +- FileScan parquet default.t2[key#61L,value2#62] Batched: true, DataFilters: [isnotnull(value2#62), (value2#62 > 0.01), isnotnull(key#61L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(value2), GreaterThan(value2,0.01), IsNotNull(key)], ReadSchema: struct<key:bigint,value2:double>
   :        +- FileScan parquet default.t1[key#59L,value1#60] Batched: true, DataFilters: [isnotnull(key#59L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(key)], ReadSchema: struct<key:bigint,value1:double>
   +- Sort [key#61L ASC NULLS FIRST], false, 0
      +- Exchange hashpartitioning(key#61L, 3), ENSURE_REQUIREMENTS, [plan_id=67]
         +- Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L))
            +- FileScan parquet default.t2[key#61L,value2#62] Batched: true, DataFilters: [isnotnull(value2#62), (value2#62 > 0.01), isnotnull(key#61L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(value2), GreaterThan(value2,0.01), IsNotNull(key)], ReadSchema: struct<key:bigint,value2:double>

org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec$ - applyPhysicalRules - org.apache.spark.sql.execution.ApplyColumnarRulesAndInsertTransitions -->【
Exchange hashpartitioning(key#59L, 3), ENSURE_REQUIREMENTS, [plan_id=66]
+- Filter (isnotnull(key#59L) AND might_contain(Subquery subquery#73, [id=#57], xxhash64(key#59L, 42)))
   :  +- Subquery subquery#73, [id=#57]
   :     +- AdaptiveSparkPlan isFinalPlan=false
   :        +- ObjectHashAggregate(keys=[], functions=[bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0)], output=[bloomFilter#72])
   :           +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=55]
   :              +- ObjectHashAggregate(keys=[], functions=[partial_bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0)], output=[buf#75])
   :                 +- Project [key#61L]
   :                    +- Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L))
   :                       +- FileScan parquet default.t2[key#61L,value2#62] Batched: true, DataFilters: [isnotnull(value2#62), (value2#62 > 0.01), isnotnull(key#61L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(value2), GreaterThan(value2,0.01), IsNotNull(key)], ReadSchema: struct<key:bigint,value2:double>
   +- FileScan parquet default.t1[key#59L,value1#60] Batched: true, DataFilters: [isnotnull(key#59L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(key)], ReadSchema: struct<key:bigint,value1:double>
】AQE --> 【
Exchange hashpartitioning(key#59L, 3), ENSURE_REQUIREMENTS, [plan_id=90]
+- Filter (isnotnull(key#59L) AND might_contain(Subquery subquery#73, [id=#57], xxhash64(key#59L, 42)))
   :  +- Subquery subquery#73, [id=#57]
   :     +- AdaptiveSparkPlan isFinalPlan=false
   :        +- ObjectHashAggregate(keys=[], functions=[bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0)], output=[bloomFilter#72])
   :           +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=55]
   :              +- ObjectHashAggregate(keys=[], functions=[partial_bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0)], output=[buf#75])
   :                 +- Project [key#61L]
   :                    +- Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L))
   :                       +- FileScan parquet default.t2[key#61L,value2#62] Batched: true, DataFilters: [isnotnull(value2#62), (value2#62 > 0.01), isnotnull(key#61L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(value2), GreaterThan(value2,0.01), IsNotNull(key)], ReadSchema: struct<key:bigint,value2:double>
   +- ColumnarToRow
      +- FileScan parquet default.t1[key#59L,value1#60] Batched: true, DataFilters: [isnotnull(key#59L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(key)], ReadSchema: struct<key:bigint,value1:double>
】

org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec$ - applyPhysicalRules - org.apache.spark.sql.execution.CollapseCodegenStages -->【
Exchange hashpartitioning(key#59L, 3), ENSURE_REQUIREMENTS, [plan_id=90]
+- Filter (isnotnull(key#59L) AND might_contain(Subquery subquery#73, [id=#57], xxhash64(key#59L, 42)))
   :  +- Subquery subquery#73, [id=#57]
   :     +- AdaptiveSparkPlan isFinalPlan=false
   :        +- ObjectHashAggregate(keys=[], functions=[bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0)], output=[bloomFilter#72])
   :           +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=55]
   :              +- ObjectHashAggregate(keys=[], functions=[partial_bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0)], output=[buf#75])
   :                 +- Project [key#61L]
   :                    +- Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L))
   :                       +- FileScan parquet default.t2[key#61L,value2#62] Batched: true, DataFilters: [isnotnull(value2#62), (value2#62 > 0.01), isnotnull(key#61L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(value2), GreaterThan(value2,0.01), IsNotNull(key)], ReadSchema: struct<key:bigint,value2:double>
   +- ColumnarToRow
      +- FileScan parquet default.t1[key#59L,value1#60] Batched: true, DataFilters: [isnotnull(key#59L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(key)], ReadSchema: struct<key:bigint,value1:double>
】AQE --> 【
Exchange hashpartitioning(key#59L, 3), ENSURE_REQUIREMENTS, [plan_id=95]
+- *(1) Filter (isnotnull(key#59L) AND might_contain(Subquery subquery#73, [id=#57], xxhash64(key#59L, 42)))
   :  +- Subquery subquery#73, [id=#57]
   :     +- AdaptiveSparkPlan isFinalPlan=false
   :        +- ObjectHashAggregate(keys=[], functions=[bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0)], output=[bloomFilter#72])
   :           +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=55]
   :              +- ObjectHashAggregate(keys=[], functions=[partial_bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0)], output=[buf#75])
   :                 +- Project [key#61L]
   :                    +- Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L))
   :                       +- FileScan parquet default.t2[key#61L,value2#62] Batched: true, DataFilters: [isnotnull(value2#62), (value2#62 > 0.01), isnotnull(key#61L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(value2), GreaterThan(value2,0.01), IsNotNull(key)], ReadSchema: struct<key:bigint,value2:double>
   +- *(1) ColumnarToRow
      +- FileScan parquet default.t1[key#59L,value1#60] Batched: true, DataFilters: [isnotnull(key#59L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(key)], ReadSchema: struct<key:bigint,value1:double>
】

org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec$ final result ==>【
Exchange hashpartitioning(key#59L, 3), ENSURE_REQUIREMENTS, [plan_id=66]
+- Filter (isnotnull(key#59L) AND might_contain(Subquery subquery#73, [id=#57], xxhash64(key#59L, 42)))
   :  +- Subquery subquery#73, [id=#57]
   :     +- AdaptiveSparkPlan isFinalPlan=false
   :        +- ObjectHashAggregate(keys=[], functions=[bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0)], output=[bloomFilter#72])
   :           +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=55]
   :              +- ObjectHashAggregate(keys=[], functions=[partial_bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0)], output=[buf#75])
   :                 +- Project [key#61L]
   :                    +- Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L))
   :                       +- FileScan parquet default.t2[key#61L,value2#62] Batched: true, DataFilters: [isnotnull(value2#62), (value2#62 > 0.01), isnotnull(key#61L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(value2), GreaterThan(value2,0.01), IsNotNull(key)], ReadSchema: struct<key:bigint,value2:double>
   +- FileScan parquet default.t1[key#59L,value1#60] Batched: true, DataFilters: [isnotnull(key#59L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(key)], ReadSchema: struct<key:bigint,value1:double>
】AQE ==> 【
Exchange hashpartitioning(key#59L, 3), ENSURE_REQUIREMENTS, [plan_id=95]
+- *(1) Filter (isnotnull(key#59L) AND might_contain(Subquery subquery#73, [id=#57], xxhash64(key#59L, 42)))
   :  +- Subquery subquery#73, [id=#57]
   :     +- AdaptiveSparkPlan isFinalPlan=false
   :        +- ObjectHashAggregate(keys=[], functions=[bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0)], output=[bloomFilter#72])
   :           +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=55]
   :              +- ObjectHashAggregate(keys=[], functions=[partial_bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0)], output=[buf#75])
   :                 +- Project [key#61L]
   :                    +- Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L))
   :                       +- FileScan parquet default.t2[key#61L,value2#62] Batched: true, DataFilters: [isnotnull(value2#62), (value2#62 > 0.01), isnotnull(key#61L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(value2), GreaterThan(value2,0.01), IsNotNull(key)], ReadSchema: struct<key:bigint,value2:double>
   +- *(1) ColumnarToRow
      +- FileScan parquet default.t1[key#59L,value1#60] Batched: true, DataFilters: [isnotnull(key#59L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(key)], ReadSchema: struct<key:bigint,value1:double>
】

org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec$ - applyPhysicalRules - org.apache.spark.sql.execution.ApplyColumnarRulesAndInsertTransitions -->【
Exchange hashpartitioning(key#61L, 3), ENSURE_REQUIREMENTS, [plan_id=67]
+- Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L))
   +- FileScan parquet default.t2[key#61L,value2#62] Batched: true, DataFilters: [isnotnull(value2#62), (value2#62 > 0.01), isnotnull(key#61L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(value2), GreaterThan(value2,0.01), IsNotNull(key)], ReadSchema: struct<key:bigint,value2:double>
】AQE --> 【
Exchange hashpartitioning(key#61L, 3), ENSURE_REQUIREMENTS, [plan_id=112]
+- Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L))
   +- ColumnarToRow
      +- FileScan parquet default.t2[key#61L,value2#62] Batched: true, DataFilters: [isnotnull(value2#62), (value2#62 > 0.01), isnotnull(key#61L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(value2), GreaterThan(value2,0.01), IsNotNull(key)], ReadSchema: struct<key:bigint,value2:double>
】

org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec$ - applyPhysicalRules - org.apache.spark.sql.execution.CollapseCodegenStages -->【
Exchange hashpartitioning(key#61L, 3), ENSURE_REQUIREMENTS, [plan_id=112]
+- Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L))
   +- ColumnarToRow
      +- FileScan parquet default.t2[key#61L,value2#62] Batched: true, DataFilters: [isnotnull(value2#62), (value2#62 > 0.01), isnotnull(key#61L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(value2), GreaterThan(value2,0.01), IsNotNull(key)], ReadSchema: struct<key:bigint,value2:double>
】AQE --> 【
Exchange hashpartitioning(key#61L, 3), ENSURE_REQUIREMENTS, [plan_id=117]
+- *(2) Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L))
   +- *(2) ColumnarToRow
      +- FileScan parquet default.t2[key#61L,value2#62] Batched: true, DataFilters: [isnotnull(value2#62), (value2#62 > 0.01), isnotnull(key#61L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(value2), GreaterThan(value2,0.01), IsNotNull(key)], ReadSchema: struct<key:bigint,value2:double>
】

org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec$ final result ==>【
Exchange hashpartitioning(key#61L, 3), ENSURE_REQUIREMENTS, [plan_id=67]
+- Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L))
   +- FileScan parquet default.t2[key#61L,value2#62] Batched: true, DataFilters: [isnotnull(value2#62), (value2#62 > 0.01), isnotnull(key#61L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(value2), GreaterThan(value2,0.01), IsNotNull(key)], ReadSchema: struct<key:bigint,value2:double>
】AQE ==> 【
Exchange hashpartitioning(key#61L, 3), ENSURE_REQUIREMENTS, [plan_id=117]
+- *(2) Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L))
   +- *(2) ColumnarToRow
      +- FileScan parquet default.t2[key#61L,value2#62] Batched: true, DataFilters: [isnotnull(value2#62), (value2#62 > 0.01), isnotnull(key#61L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(value2), GreaterThan(value2,0.01), IsNotNull(key)], ReadSchema: struct<key:bigint,value2:double>
】

21:16:23.981 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec - Materialize query stage ShuffleQueryStageExec: 0, query stage plan:
Exchange hashpartitioning(key#59L, 3), ENSURE_REQUIREMENTS, [plan_id=95]
+- *(1) Filter (isnotnull(key#59L) AND might_contain(Subquery subquery#73, [id=#57], xxhash64(key#59L, 42)))
   :  +- Subquery subquery#73, [id=#57]
   :     +- AdaptiveSparkPlan isFinalPlan=false
   :        +- ObjectHashAggregate(keys=[], functions=[bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0)], output=[bloomFilter#72])
   :           +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=55]
   :              +- ObjectHashAggregate(keys=[], functions=[partial_bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0)], output=[buf#75])
   :                 +- Project [key#61L]
   :                    +- Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L))
   :                       +- FileScan parquet default.t2[key#61L,value2#62] Batched: true, DataFilters: [isnotnull(value2#62), (value2#62 > 0.01), isnotnull(key#61L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(value2), GreaterThan(value2,0.01), IsNotNull(key)], ReadSchema: struct<key:bigint,value2:double>
   +- *(1) ColumnarToRow
      +- FileScan parquet default.t1[key#59L,value1#60] Batched: true, DataFilters: [isnotnull(key#59L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(key)], ReadSchema: struct<key:bigint,value1:double>

21:16:24.032 [subquery-0] DEBUG org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec - withFinalPlanUpdate.executeTake ==> before get final plan:
ObjectHashAggregate(keys=[], functions=[bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0)], output=[bloomFilter#72])
+- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=55]
   +- ObjectHashAggregate(keys=[], functions=[partial_bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0)], output=[buf#75])
      +- Project [key#61L]
         +- Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L))
            +- FileScan parquet default.t2[key#61L,value2#62] Batched: true, DataFilters: [isnotnull(value2#62), (value2#62 > 0.01), isnotnull(key#61L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(value2), GreaterThan(value2,0.01), IsNotNull(key)], ReadSchema: struct<key:bigint,value2:double>

org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec$ - applyPhysicalRules - org.apache.spark.sql.execution.ApplyColumnarRulesAndInsertTransitions -->【
Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=55]
+- ObjectHashAggregate(keys=[], functions=[partial_bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0)], output=[buf#75])
   +- Project [key#61L]
      +- Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L))
         +- FileScan parquet default.t2[key#61L,value2#62] Batched: true, DataFilters: [isnotnull(value2#62), (value2#62 > 0.01), isnotnull(key#61L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(value2), GreaterThan(value2,0.01), IsNotNull(key)], ReadSchema: struct<key:bigint,value2:double>
】AQE --> 【
Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=134]
+- ObjectHashAggregate(keys=[], functions=[partial_bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0)], output=[buf#75])
   +- Project [key#61L]
      +- Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L))
         +- ColumnarToRow
            +- FileScan parquet default.t2[key#61L,value2#62] Batched: true, DataFilters: [isnotnull(value2#62), (value2#62 > 0.01), isnotnull(key#61L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(value2), GreaterThan(value2,0.01), IsNotNull(key)], ReadSchema: struct<key:bigint,value2:double>
】

org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec$ - applyPhysicalRules - org.apache.spark.sql.execution.CollapseCodegenStages -->【
Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=134]
+- ObjectHashAggregate(keys=[], functions=[partial_bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0)], output=[buf#75])
   +- Project [key#61L]
      +- Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L))
         +- ColumnarToRow
            +- FileScan parquet default.t2[key#61L,value2#62] Batched: true, DataFilters: [isnotnull(value2#62), (value2#62 > 0.01), isnotnull(key#61L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(value2), GreaterThan(value2,0.01), IsNotNull(key)], ReadSchema: struct<key:bigint,value2:double>
】AQE --> 【
Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=141]
+- ObjectHashAggregate(keys=[], functions=[partial_bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0)], output=[buf#75])
   +- *(1) Project [key#61L]
      +- *(1) Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L))
         +- *(1) ColumnarToRow
            +- FileScan parquet default.t2[key#61L,value2#62] Batched: true, DataFilters: [isnotnull(value2#62), (value2#62 > 0.01), isnotnull(key#61L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(value2), GreaterThan(value2,0.01), IsNotNull(key)], ReadSchema: struct<key:bigint,value2:double>
】

org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec$ final result ==>【
Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=55]
+- ObjectHashAggregate(keys=[], functions=[partial_bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0)], output=[buf#75])
   +- Project [key#61L]
      +- Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L))
         +- FileScan parquet default.t2[key#61L,value2#62] Batched: true, DataFilters: [isnotnull(value2#62), (value2#62 > 0.01), isnotnull(key#61L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(value2), GreaterThan(value2,0.01), IsNotNull(key)], ReadSchema: struct<key:bigint,value2:double>
】AQE ==> 【
Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=141]
+- ObjectHashAggregate(keys=[], functions=[partial_bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0)], output=[buf#75])
   +- *(1) Project [key#61L]
      +- *(1) Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L))
         +- *(1) ColumnarToRow
            +- FileScan parquet default.t2[key#61L,value2#62] Batched: true, DataFilters: [isnotnull(value2#62), (value2#62 > 0.01), isnotnull(key#61L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(value2), GreaterThan(value2,0.01), IsNotNull(key)], ReadSchema: struct<key:bigint,value2:double>
】

21:16:24.041 [subquery-0] DEBUG org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec - Materialize query stage ShuffleQueryStageExec: 0, query stage plan:
Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=141]
+- ObjectHashAggregate(keys=[], functions=[partial_bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0)], output=[buf#75])
   +- *(1) Project [key#61L]
      +- *(1) Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L))
         +- *(1) ColumnarToRow
            +- FileScan parquet default.t2[key#61L,value2#62] Batched: true, DataFilters: [isnotnull(value2#62), (value2#62 > 0.01), isnotnull(key#61L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(value2), GreaterThan(value2,0.01), IsNotNull(key)], ReadSchema: struct<key:bigint,value2:double>

21:16:24.346 [subquery-0] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 231.195052 ms
21:16:24.406 [subquery-0] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 356.5 KiB, free 4.1 GiB)
21:16:24.408 [subquery-0] DEBUG org.apache.spark.storage.BlockManager - Put block broadcast_0 locally took 38 ms
21:16:24.409 [subquery-0] DEBUG org.apache.spark.storage.BlockManager - Putting block broadcast_0 without replication took 39 ms
21:16:24.740 [subquery-0] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.6 KiB, free 4.1 GiB)
21:16:24.742 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_0_piece0 for BlockManagerId(driver, 192.168.31.69, 58425, None)
21:16:24.743 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on 192.168.31.69:58425 (size: 34.6 KiB, free: 4.1 GiB)
21:16:24.746 [subquery-0] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_0_piece0
21:16:24.746 [subquery-0] DEBUG org.apache.spark.storage.BlockManager - Told master about block broadcast_0_piece0
21:16:24.746 [subquery-0] DEBUG org.apache.spark.storage.BlockManager - Put block broadcast_0_piece0 locally took 8 ms
21:16:24.746 [subquery-0] DEBUG org.apache.spark.storage.BlockManager - Putting block broadcast_0_piece0 without replication took 8 ms
21:16:24.747 [subquery-0] INFO  org.apache.spark.SparkContext - Created broadcast 0 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266
21:16:24.760 [subquery-0] INFO  org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 8402097 bytes, open cost is considered as scanning 4194304 bytes.
21:16:24.823 [subquery-0] DEBUG org.apache.spark.util.ClosureCleaner - Cleaning indylambda closure: $anonfun$doExecute$4$adapted
21:16:24.837 [subquery-0] DEBUG org.apache.spark.util.ClosureCleaner -  +++ indylambda closure ($anonfun$doExecute$4$adapted) is now cleaned +++
21:16:24.908 [subquery-0] DEBUG org.apache.spark.scheduler.DAGScheduler - eagerlyComputePartitionsForRddAndAncestors for RDD 4 took 0.000953 seconds
21:16:24.915 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - Merging stage rdd profiles: Set()
21:16:24.926 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Registering RDD 4 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) as input to shuffle 0
21:16:24.933 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got map stage job 0 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) with 1 output partitions
21:16:24.934 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ShuffleMapStage 0 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266)
21:16:24.934 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
21:16:24.935 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
21:16:24.937 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - submitStage(ShuffleMapStage 0 (name=$anonfun$withThreadLocalCaptured$1 at FutureTask.java:266;jobs=0))
21:16:24.938 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - missing: List()
21:16:24.939 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 0 (MapPartitionsRDD[4] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266), which has no missing parents
21:16:24.939 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - submitMissingTasks(ShuffleMapStage 0)
21:16:25.108 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 29.0 KiB, free 4.1 GiB)
21:16:25.108 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager - Put block broadcast_1 locally took 1 ms
21:16:25.108 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager - Putting block broadcast_1 without replication took 1 ms
21:16:25.109 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 13.1 KiB, free 4.1 GiB)
21:16:25.109 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_1_piece0 for BlockManagerId(driver, 192.168.31.69, 58425, None)
21:16:25.110 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on 192.168.31.69:58425 (size: 13.1 KiB, free: 4.1 GiB)
21:16:25.110 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_1_piece0
21:16:25.110 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager - Told master about block broadcast_1_piece0
21:16:25.110 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager - Put block broadcast_1_piece0 locally took 0 ms
21:16:25.110 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager - Putting block broadcast_1_piece0 without replication took 0 ms
21:16:25.110 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1513
21:16:25.125 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[4] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) (first 15 tasks are for partitions Vector(0))
21:16:25.126 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks resource profile 0
21:16:25.149 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.TaskSetManager - Epoch for TaskSet 0.0: 0
21:16:25.152 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.TaskSetManager - Adding pending tasks took 2 ms
21:16:25.153 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.TaskSetManager - Valid locality levels for TaskSet 0.0: NO_PREF, ANY
21:16:25.165 [dispatcher-event-loop-0] DEBUG org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0.0, runningTasks: 0
21:16:25.165 [dispatcher-event-loop-0] DEBUG org.apache.spark.scheduler.TaskSetManager - Valid locality levels for TaskSet 0.0: NO_PREF, ANY
21:16:25.179 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0) (192.168.31.69, executor driver, partition 0, PROCESS_LOCAL, 5202 bytes) taskResourceAssignments Map()
21:16:25.195 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
21:16:25.203 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] DEBUG org.apache.spark.executor.ExecutorMetricsPoller - stageTCMP: (0, 0) -> 1
21:16:25.226 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] DEBUG org.apache.spark.storage.BlockManager - Getting local block broadcast_1
21:16:25.228 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] DEBUG org.apache.spark.storage.BlockManager - Level for block broadcast_1 is StorageLevel(disk, memory, deserialized, 1 replicas)
21:16:25.459 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-warehouse/t2/part-00001-a8e715dd-f214-4cc1-b612-0d05521c18e2-c000.snappy.parquet, range: 0-6748, partition values: [empty row]
21:16:25.460 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] DEBUG org.apache.spark.storage.BlockManager - Getting local block broadcast_0
21:16:25.460 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] DEBUG org.apache.spark.storage.BlockManager - Level for block broadcast_0 is StorageLevel(disk, memory, deserialized, 1 replicas)
21:16:25.942 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] DEBUG org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport - Going to read the following fields from the Parquet file with the following schema:
Parquet file schema:
message spark_schema {
  required int64 key;
  required double value2;
}

Parquet clipped schema:
message spark_schema {
  required int64 key;
  required double value2;
}

Parquet requested schema:
message spark_schema {
  required int64 key;
  required double value2;
}

Catalyst requested schema:
root
-- key: long (nullable = true)
-- value2: double (nullable = true)


21:16:26.023 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] DEBUG org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat - Appending StructType() [empty row]
21:16:26.542 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 8.75447 ms
21:16:26.556 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 6.148467 ms
21:16:26.568 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 7.968417 ms
21:16:26.597 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-warehouse/t2/part-00000-a8e715dd-f214-4cc1-b612-0d05521c18e2-c000.snappy.parquet, range: 0-6741, partition values: [empty row]
21:16:26.603 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] DEBUG org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport - Going to read the following fields from the Parquet file with the following schema:
Parquet file schema:
message spark_schema {
  required int64 key;
  required double value2;
}

Parquet clipped schema:
message spark_schema {
  required int64 key;
  required double value2;
}

Parquet requested schema:
message spark_schema {
  required int64 key;
  required double value2;
}

Catalyst requested schema:
root
-- key: long (nullable = true)
-- value2: double (nullable = true)


21:16:26.603 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] DEBUG org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat - Appending StructType() [empty row]
21:16:26.656 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] DEBUG org.apache.spark.shuffle.sort.io.LocalDiskShuffleMapOutputWriter - Writing shuffle index file for mapId 0 with length 1
21:16:26.662 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] DEBUG org.apache.spark.shuffle.IndexShuffleBlockResolver - Shuffle index for mapId 0: [36261]
21:16:26.679 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 2665 bytes result sent to driver
21:16:26.680 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] DEBUG org.apache.spark.executor.ExecutorMetricsPoller - stageTCMP: (0, 0) -> 0
21:16:26.682 [dispatcher-event-loop-0] DEBUG org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0.0, runningTasks: 0
21:16:26.682 [dispatcher-event-loop-0] DEBUG org.apache.spark.scheduler.TaskSetManager - No tasks for locality level NO_PREF, so moving to locality level ANY
21:16:26.686 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 1517 ms on 192.168.31.69 (executor driver) (1/1)
21:16:26.689 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool
21:16:26.696 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - ShuffleMapTask finished on driver
21:16:26.697 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 0 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) finished in 1.739 s
21:16:26.697 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
21:16:26.698 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - running: Set()
21:16:26.698 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - waiting: Set()
21:16:26.698 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - failed: Set()
21:16:26.698 [dag-scheduler-event-loop] DEBUG org.apache.spark.MapOutputTrackerMaster - Increasing epoch to 1
21:16:26.703 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - After removal of stage 0, remaining stages = 0
21:16:26.924 [Spark Context Cleaner] DEBUG org.apache.spark.broadcast.TorrentBroadcast - Unpersisting TorrentBroadcast 1
SparkPlanner(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2@125ace20) - org.apache.spark.sql.execution.SparkStrategies$SpecialLimits$ -->【
ReturnAnswer
+- LogicalQueryStage Aggregate [bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0) AS bloomFilter#72], ObjectHashAggregate(keys=[], functions=[bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0)])
】 --> 【
PlanLater LogicalQueryStage Aggregate [bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0) AS bloomFilter#72], ObjectHashAggregate(keys=[], functions=[bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0)])
】

SparkPlanner(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2@125ace20) - org.apache.spark.sql.execution.adaptive.LogicalQueryStageStrategy$ -->【
LogicalQueryStage Aggregate [bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0) AS bloomFilter#72], ObjectHashAggregate(keys=[], functions=[bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0)])
】 --> 【
ObjectHashAggregate(keys=[], functions=[bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0)], output=[bloomFilter#72])
+- ShuffleQueryStage 0
   +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=141]
      +- ObjectHashAggregate(keys=[], functions=[partial_bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0)], output=[buf#75])
         +- *(1) Project [key#61L]
            +- *(1) Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L))
               +- *(1) ColumnarToRow
                  +- FileScan parquet default.t2[key#61L,value2#62] Batched: true, DataFilters: [isnotnull(value2#62), (value2#62 > 0.01), isnotnull(key#61L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(value2), GreaterThan(value2,0.01), IsNotNull(key)], ReadSchema: struct<key:bigint,value2:double>
】

21:16:26.935 [subquery-0] DEBUG org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec - withFinalPlanUpdate.executeTake ==> after get final plan:
ObjectHashAggregate(keys=[], functions=[bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0)], output=[bloomFilter#72])
+- ShuffleQueryStage 0
   +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=141]
      +- ObjectHashAggregate(keys=[], functions=[partial_bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0)], output=[buf#75])
         +- *(1) Project [key#61L]
            +- *(1) Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L))
               +- *(1) ColumnarToRow
                  +- FileScan parquet default.t2[key#61L,value2#62] Batched: true, DataFilters: [isnotnull(value2#62), (value2#62 > 0.01), isnotnull(key#61L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(value2), GreaterThan(value2,0.01), IsNotNull(key)], ReadSchema: struct<key:bigint,value2:double>

21:16:26.941 [block-manager-storage-async-thread-pool-0] DEBUG org.apache.spark.storage.BlockManagerStorageEndpoint - removing broadcast 1
21:16:26.941 [block-manager-storage-async-thread-pool-0] DEBUG org.apache.spark.storage.BlockManager - Removing broadcast 1
21:16:26.944 [block-manager-storage-async-thread-pool-0] DEBUG org.apache.spark.storage.BlockManager - Removing block broadcast_1
21:16:26.944 [subquery-0] DEBUG org.apache.spark.util.ClosureCleaner - Cleaning indylambda closure: $anonfun$executeTake$2
21:16:26.944 [block-manager-storage-async-thread-pool-0] DEBUG org.apache.spark.storage.memory.MemoryStore - Block broadcast_1 of size 29744 dropped from memory (free 4392280651)
21:16:26.946 [block-manager-storage-async-thread-pool-0] DEBUG org.apache.spark.storage.BlockManager - Removing block broadcast_1_piece0
21:16:26.946 [subquery-0] DEBUG org.apache.spark.util.ClosureCleaner -  +++ indylambda closure ($anonfun$executeTake$2) is now cleaned +++
21:16:26.947 [block-manager-storage-async-thread-pool-0] DEBUG org.apache.spark.storage.memory.MemoryStore - Block broadcast_1_piece0 of size 13443 dropped from memory (free 4392294094)
21:16:26.948 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_1_piece0 for BlockManagerId(driver, 192.168.31.69, 58425, None)
21:16:26.949 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_1_piece0 on 192.168.31.69:58425 in memory (size: 13.1 KiB, free: 4.1 GiB)
21:16:26.949 [block-manager-storage-async-thread-pool-0] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_1_piece0
21:16:26.949 [block-manager-storage-async-thread-pool-0] DEBUG org.apache.spark.storage.BlockManager - Told master about block broadcast_1_piece0
21:16:26.949 [subquery-0] DEBUG org.apache.spark.util.ClosureCleaner - Cleaning indylambda closure: $anonfun$runJob$5
21:16:26.950 [block-manager-storage-async-thread-pool-2] DEBUG org.apache.spark.storage.BlockManagerStorageEndpoint - Done removing broadcast 1, response is 0
21:16:26.951 [block-manager-storage-async-thread-pool-2] DEBUG org.apache.spark.storage.BlockManagerStorageEndpoint - Sent response: 0 to 192.168.31.69:58424
21:16:26.957 [subquery-0] DEBUG org.apache.spark.util.ClosureCleaner -  +++ indylambda closure ($anonfun$runJob$5) is now cleaned +++
21:16:26.959 [subquery-0] INFO  org.apache.spark.SparkContext - Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266
21:16:26.960 [subquery-0] DEBUG org.apache.spark.scheduler.DAGScheduler - eagerlyComputePartitionsForRddAndAncestors for RDD 7 took 0.000163 seconds
21:16:26.960 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - Merging stage rdd profiles: Set()
21:16:26.960 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - Merging stage rdd profiles: Set()
21:16:26.962 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 1 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) with 1 output partitions
21:16:26.962 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 2 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266)
21:16:26.963 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 1)
21:16:26.963 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
21:16:26.964 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - submitStage(ResultStage 2 (name=$anonfun$withThreadLocalCaptured$1 at FutureTask.java:266;jobs=1))
21:16:26.965 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - missing: List()
21:16:26.965 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 2 (MapPartitionsRDD[7] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266), which has no missing parents
21:16:26.965 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - submitMissingTasks(ResultStage 2)
21:16:26.997 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 28.6 KiB, free 4.1 GiB)
21:16:26.997 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager - Put block broadcast_2 locally took 0 ms
21:16:26.997 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager - Putting block broadcast_2 without replication took 0 ms
21:16:26.998 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 13.5 KiB, free 4.1 GiB)
21:16:26.998 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_2_piece0 for BlockManagerId(driver, 192.168.31.69, 58425, None)
21:16:26.998 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on 192.168.31.69:58425 (size: 13.5 KiB, free: 4.1 GiB)
21:16:26.999 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_2_piece0
21:16:26.999 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager - Told master about block broadcast_2_piece0
21:16:26.999 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager - Put block broadcast_2_piece0 locally took 1 ms
21:16:26.999 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager - Putting block broadcast_2_piece0 without replication took 1 ms
21:16:26.999 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1513
21:16:27.000 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[7] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) (first 15 tasks are for partitions Vector(0))
21:16:27.000 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 2.0 with 1 tasks resource profile 0
21:16:27.000 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.TaskSetManager - Epoch for TaskSet 2.0: 1
21:16:27.002 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.TaskSetManager - Adding pending tasks took 1 ms
21:16:27.003 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.TaskSetManager - Valid locality levels for TaskSet 2.0: NODE_LOCAL, ANY
21:16:27.003 [dispatcher-event-loop-1] DEBUG org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_2.0, runningTasks: 0
21:16:27.005 [dispatcher-event-loop-1] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 2.0 (TID 1) (192.168.31.69, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
21:16:27.006 [Executor task launch worker for task 0.0 in stage 2.0 (TID 1)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 2.0 (TID 1)
21:16:27.006 [Executor task launch worker for task 0.0 in stage 2.0 (TID 1)] DEBUG org.apache.spark.executor.ExecutorMetricsPoller - stageTCMP: (2, 0) -> 1
21:16:27.007 [Executor task launch worker for task 0.0 in stage 2.0 (TID 1)] DEBUG org.apache.spark.storage.BlockManager - Getting local block broadcast_2
21:16:27.007 [Executor task launch worker for task 0.0 in stage 2.0 (TID 1)] DEBUG org.apache.spark.storage.BlockManager - Level for block broadcast_2 is StorageLevel(disk, memory, deserialized, 1 replicas)
21:16:27.075 [Executor task launch worker for task 0.0 in stage 2.0 (TID 1)] DEBUG org.apache.spark.MapOutputTrackerMaster - Fetching outputs for shuffle 0
21:16:27.077 [Executor task launch worker for task 0.0 in stage 2.0 (TID 1)] DEBUG org.apache.spark.MapOutputTrackerMaster - Convert map statuses for shuffle 0, mappers 0-1, partitions 0-1
21:16:27.097 [Executor task launch worker for task 0.0 in stage 2.0 (TID 1)] DEBUG org.apache.spark.storage.ShuffleBlockFetcherIterator - maxBytesInFlight: 50331648, targetRemoteRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
21:16:27.103 [Executor task launch worker for task 0.0 in stage 2.0 (TID 1)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 (38.4 KiB) non-empty blocks including 1 (38.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
21:16:27.105 [Executor task launch worker for task 0.0 in stage 2.0 (TID 1)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 11 ms
21:16:27.105 [Executor task launch worker for task 0.0 in stage 2.0 (TID 1)] DEBUG org.apache.spark.storage.ShuffleBlockFetcherIterator - Start fetching local blocks: (shuffle_0_0_0,0)
21:16:27.105 [Executor task launch worker for task 0.0 in stage 2.0 (TID 1)] DEBUG org.apache.spark.storage.BlockManager - Getting local shuffle block shuffle_0_0_0
21:16:27.110 [Executor task launch worker for task 0.0 in stage 2.0 (TID 1)] DEBUG org.apache.spark.storage.ShuffleBlockFetcherIterator - Got local blocks in 17 ms
org.apache.spark.sql.catalyst.expressions.codegen.package$ExpressionCanonicalizer$@1c3cff6b org.apache.spark.sql.catalyst.expressions.codegen.package$ExpressionCanonicalizer$CleanExpressions --> old【
input[0, binary, true] AS bloomFilter#72】--> new【
input[0, binary, true]】


RuleExecutor.execute[org.apache.spark.sql.catalyst.expressions.codegen.package$ExpressionCanonicalizer$@1c3cff6b] final result ==> old【
input[0, binary, true] AS bloomFilter#72】==> new【
input[0, binary, true]】


21:16:27.143 [Executor task launch worker for task 0.0 in stage 2.0 (TID 1)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 2.0 (TID 1). 40007 bytes result sent to driver
21:16:27.143 [Executor task launch worker for task 0.0 in stage 2.0 (TID 1)] DEBUG org.apache.spark.executor.ExecutorMetricsPoller - stageTCMP: (2, 0) -> 0
21:16:27.144 [dispatcher-event-loop-1] DEBUG org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_2.0, runningTasks: 0
21:16:27.144 [dispatcher-event-loop-1] DEBUG org.apache.spark.scheduler.TaskSetManager - No tasks for locality level NODE_LOCAL, so moving to locality level ANY
21:16:27.145 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 2.0 (TID 1) in 141 ms on 192.168.31.69 (executor driver) (1/1)
21:16:27.145 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 2.0, whose tasks have all completed, from pool
21:16:27.145 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 2 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) finished in 0.172 s
21:16:27.146 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - After removal of stage 2, remaining stages = 1
21:16:27.146 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - After removal of stage 1, remaining stages = 0
21:16:27.147 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
21:16:27.147 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 2: Stage finished
21:16:27.148 [subquery-0] INFO  org.apache.spark.scheduler.DAGScheduler - Job 1 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266, took 0.189507 s
21:16:27.154 [subquery-0] DEBUG org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec - withFinalPlanUpdate.executeTake was finished.
21:16:27.155 [subquery-0] DEBUG org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec - Final plan: ObjectHashAggregate(keys=[], functions=[bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0)], output=[bloomFilter#72])
+- ShuffleQueryStage 0
   +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=141]
      +- ObjectHashAggregate(keys=[], functions=[partial_bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0)], output=[buf#75])
         +- *(1) Project [key#61L]
            +- *(1) Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L))
               +- *(1) ColumnarToRow
                  +- FileScan parquet default.t2[key#61L,value2#62] Batched: true, DataFilters: [isnotnull(value2#62), (value2#62 > 0.01), isnotnull(key#61L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(value2), GreaterThan(value2,0.01), IsNotNull(key)], ReadSchema: struct<key:bigint,value2:double>

21:16:27.155 [subquery-0] DEBUG org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec - withFinalPlanUpdate.executeTake.finalPlanUpdate was finished.
21:16:27.188 [ScalaTest-run-running-PhysicalPlanSpec] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 21.653582 ms
21:16:27.192 [ScalaTest-run-running-PhysicalPlanSpec] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_3 stored as values in memory (estimated size 356.5 KiB, free 4.1 GiB)
21:16:27.193 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.storage.BlockManager - Put block broadcast_3 locally took 3 ms
21:16:27.193 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.storage.BlockManager - Putting block broadcast_3 without replication took 3 ms
21:16:27.204 [ScalaTest-run-running-PhysicalPlanSpec] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_3_piece0 stored as bytes in memory (estimated size 34.6 KiB, free 4.1 GiB)
21:16:27.204 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_3_piece0 for BlockManagerId(driver, 192.168.31.69, 58425, None)
21:16:27.204 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_3_piece0 in memory on 192.168.31.69:58425 (size: 34.6 KiB, free: 4.1 GiB)
21:16:27.204 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_3_piece0
21:16:27.204 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.storage.BlockManager - Told master about block broadcast_3_piece0
21:16:27.204 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.storage.BlockManager - Put block broadcast_3_piece0 locally took 1 ms
21:16:27.204 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.storage.BlockManager - Putting block broadcast_3_piece0 without replication took 1 ms
21:16:27.205 [ScalaTest-run-running-PhysicalPlanSpec] INFO  org.apache.spark.SparkContext - Created broadcast 3 from saveAsTable at PhysicalPlanSpec.scala:372
21:16:27.206 [ScalaTest-run-running-PhysicalPlanSpec] INFO  org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 28789636 bytes, open cost is considered as scanning 4194304 bytes.
21:16:27.208 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.util.ClosureCleaner - Cleaning indylambda closure: $anonfun$doExecute$4$adapted
21:16:27.210 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.util.ClosureCleaner -  +++ indylambda closure ($anonfun$doExecute$4$adapted) is now cleaned +++
21:16:27.216 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.scheduler.DAGScheduler - eagerlyComputePartitionsForRddAndAncestors for RDD 11 took 0.000054 seconds
21:16:27.217 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - Merging stage rdd profiles: Set()
21:16:27.217 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Registering RDD 11 (saveAsTable at PhysicalPlanSpec.scala:372) as input to shuffle 1
21:16:27.217 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got map stage job 2 (saveAsTable at PhysicalPlanSpec.scala:372) with 1 output partitions
21:16:27.217 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ShuffleMapStage 3 (saveAsTable at PhysicalPlanSpec.scala:372)
21:16:27.217 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec - Materialize query stage ShuffleQueryStageExec: 1, query stage plan:
Exchange hashpartitioning(key#61L, 3), ENSURE_REQUIREMENTS, [plan_id=117]
+- *(2) Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L))
   +- *(2) ColumnarToRow
      +- FileScan parquet default.t2[key#61L,value2#62] Batched: true, DataFilters: [isnotnull(value2#62), (value2#62 > 0.01), isnotnull(key#61L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(value2), GreaterThan(value2,0.01), IsNotNull(key)], ReadSchema: struct<key:bigint,value2:double>

21:16:27.217 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
21:16:27.217 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
21:16:27.217 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - submitStage(ShuffleMapStage 3 (name=saveAsTable at PhysicalPlanSpec.scala:372;jobs=2))
21:16:27.218 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - missing: List()
21:16:27.218 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 3 (MapPartitionsRDD[11] at saveAsTable at PhysicalPlanSpec.scala:372), which has no missing parents
21:16:27.218 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - submitMissingTasks(ShuffleMapStage 3)
21:16:27.229 [dag-scheduler-event-loop] WARN  org.apache.spark.scheduler.DAGScheduler - Broadcasting large task binary with size 1046.0 KiB
21:16:27.229 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_4 stored as values in memory (estimated size 1046.0 KiB, free 4.1 GiB)
21:16:27.230 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager - Put block broadcast_4 locally took 0 ms
21:16:27.230 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager - Putting block broadcast_4 without replication took 0 ms
21:16:27.232 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_4_piece0 stored as bytes in memory (estimated size 48.8 KiB, free 4.1 GiB)
21:16:27.232 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_4_piece0 for BlockManagerId(driver, 192.168.31.69, 58425, None)
21:16:27.232 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_4_piece0 in memory on 192.168.31.69:58425 (size: 48.8 KiB, free: 4.1 GiB)
21:16:27.233 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_4_piece0
21:16:27.233 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager - Told master about block broadcast_4_piece0
21:16:27.233 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager - Put block broadcast_4_piece0 locally took 1 ms
21:16:27.233 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager - Putting block broadcast_4_piece0 without replication took 1 ms
21:16:27.233 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 4 from broadcast at DAGScheduler.scala:1513
21:16:27.233 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[11] at saveAsTable at PhysicalPlanSpec.scala:372) (first 15 tasks are for partitions Vector(0))
21:16:27.234 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 3.0 with 1 tasks resource profile 0
21:16:27.234 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.TaskSetManager - Epoch for TaskSet 3.0: 1
21:16:27.234 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.TaskSetManager - Adding pending tasks took 0 ms
21:16:27.234 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.TaskSetManager - Valid locality levels for TaskSet 3.0: NO_PREF, ANY
21:16:27.234 [dispatcher-event-loop-0] DEBUG org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_3.0, runningTasks: 0
21:16:27.235 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 3.0 (TID 2) (192.168.31.69, executor driver, partition 0, PROCESS_LOCAL, 5590 bytes) taskResourceAssignments Map()
21:16:27.235 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 3.0 (TID 2)
21:16:27.236 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] DEBUG org.apache.spark.executor.ExecutorMetricsPoller - stageTCMP: (3, 0) -> 1
21:16:27.236 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] DEBUG org.apache.spark.storage.BlockManager - Getting local block broadcast_4
21:16:27.236 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] DEBUG org.apache.spark.storage.BlockManager - Level for block broadcast_4 is StorageLevel(disk, memory, deserialized, 1 replicas)
21:16:27.245 [ScalaTest-run-running-PhysicalPlanSpec] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 22.994141 ms
21:16:27.249 [ScalaTest-run-running-PhysicalPlanSpec] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_5 stored as values in memory (estimated size 356.5 KiB, free 4.1 GiB)
21:16:27.249 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.storage.BlockManager - Put block broadcast_5 locally took 3 ms
21:16:27.249 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.storage.BlockManager - Putting block broadcast_5 without replication took 3 ms
21:16:27.268 [ScalaTest-run-running-PhysicalPlanSpec] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_5_piece0 stored as bytes in memory (estimated size 34.6 KiB, free 4.1 GiB)
21:16:27.269 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_5_piece0 for BlockManagerId(driver, 192.168.31.69, 58425, None)
21:16:27.269 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 16.330466 ms
21:16:27.269 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_5_piece0 in memory on 192.168.31.69:58425 (size: 34.6 KiB, free: 4.1 GiB)
21:16:27.270 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_5_piece0
21:16:27.270 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.storage.BlockManager - Told master about block broadcast_5_piece0
21:16:27.270 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.storage.BlockManager - Put block broadcast_5_piece0 locally took 2 ms
21:16:27.270 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.storage.BlockManager - Putting block broadcast_5_piece0 without replication took 2 ms
21:16:27.271 [ScalaTest-run-running-PhysicalPlanSpec] INFO  org.apache.spark.SparkContext - Created broadcast 5 from saveAsTable at PhysicalPlanSpec.scala:372
21:16:27.271 [ScalaTest-run-running-PhysicalPlanSpec] INFO  org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 8402097 bytes, open cost is considered as scanning 4194304 bytes.
21:16:27.272 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-warehouse/t1/part-00002-7df4f35f-6bca-4c50-bf40-768a572edd6e-c000.snappy.parquet, range: 0-3003114, partition values: [empty row]
21:16:27.272 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] DEBUG org.apache.spark.storage.BlockManager - Getting local block broadcast_3
21:16:27.272 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] DEBUG org.apache.spark.storage.BlockManager - Level for block broadcast_3 is StorageLevel(disk, memory, deserialized, 1 replicas)
21:16:27.273 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.util.ClosureCleaner - Cleaning indylambda closure: $anonfun$doExecute$4$adapted
21:16:27.275 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.util.ClosureCleaner -  +++ indylambda closure ($anonfun$doExecute$4$adapted) is now cleaned +++
21:16:27.277 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] DEBUG org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport - Going to read the following fields from the Parquet file with the following schema:
Parquet file schema:
message spark_schema {
  required int64 key;
  required double value1;
}

Parquet clipped schema:
message spark_schema {
  required int64 key;
  required double value1;
}

Parquet requested schema:
message spark_schema {
  required int64 key;
  required double value1;
}

Catalyst requested schema:
root
-- key: long (nullable = true)
-- value1: double (nullable = true)


21:16:27.277 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.scheduler.DAGScheduler - eagerlyComputePartitionsForRddAndAncestors for RDD 15 took 0.000047 seconds
21:16:27.278 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - Merging stage rdd profiles: Set()
21:16:27.278 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Registering RDD 15 (saveAsTable at PhysicalPlanSpec.scala:372) as input to shuffle 2
21:16:27.278 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got map stage job 3 (saveAsTable at PhysicalPlanSpec.scala:372) with 1 output partitions
21:16:27.278 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ShuffleMapStage 4 (saveAsTable at PhysicalPlanSpec.scala:372)
21:16:27.278 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
21:16:27.279 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
21:16:27.279 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] DEBUG org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat - Appending StructType() [empty row]
21:16:27.279 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - submitStage(ShuffleMapStage 4 (name=saveAsTable at PhysicalPlanSpec.scala:372;jobs=3))
21:16:27.279 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - missing: List()
21:16:27.279 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 4 (MapPartitionsRDD[15] at saveAsTable at PhysicalPlanSpec.scala:372), which has no missing parents
21:16:27.279 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - submitMissingTasks(ShuffleMapStage 4)
21:16:27.285 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_6 stored as values in memory (estimated size 16.7 KiB, free 4.1 GiB)
21:16:27.285 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager - Put block broadcast_6 locally took 1 ms
21:16:27.285 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager - Putting block broadcast_6 without replication took 1 ms
21:16:27.287 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_6_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 4.1 GiB)
21:16:27.287 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_6_piece0 for BlockManagerId(driver, 192.168.31.69, 58425, None)
21:16:27.288 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_6_piece0 in memory on 192.168.31.69:58425 (size: 7.7 KiB, free: 4.1 GiB)
21:16:27.288 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_6_piece0
21:16:27.288 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager - Told master about block broadcast_6_piece0
21:16:27.288 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager - Put block broadcast_6_piece0 locally took 1 ms
21:16:27.288 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager - Putting block broadcast_6_piece0 without replication took 1 ms
21:16:27.288 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 6 from broadcast at DAGScheduler.scala:1513
21:16:27.289 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 4 (MapPartitionsRDD[15] at saveAsTable at PhysicalPlanSpec.scala:372) (first 15 tasks are for partitions Vector(0))
21:16:27.289 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 4.0 with 1 tasks resource profile 0
21:16:27.289 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.TaskSetManager - Epoch for TaskSet 4.0: 1
21:16:27.289 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.TaskSetManager - Adding pending tasks took 0 ms
21:16:27.289 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.TaskSetManager - Valid locality levels for TaskSet 4.0: NO_PREF, ANY
21:16:27.290 [dispatcher-event-loop-0] DEBUG org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_3.0, runningTasks: 1
21:16:27.290 [dispatcher-event-loop-0] DEBUG org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_4.0, runningTasks: 0
21:16:27.338 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-warehouse/t1/part-00000-7df4f35f-6bca-4c50-bf40-768a572edd6e-c000.snappy.parquet, range: 0-3003106, partition values: [empty row]
21:16:27.344 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] DEBUG org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport - Going to read the following fields from the Parquet file with the following schema:
Parquet file schema:
message spark_schema {
  required int64 key;
  required double value1;
}

Parquet clipped schema:
message spark_schema {
  required int64 key;
  required double value1;
}

Parquet requested schema:
message spark_schema {
  required int64 key;
  required double value1;
}

Catalyst requested schema:
root
-- key: long (nullable = true)
-- value1: double (nullable = true)


21:16:27.345 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] DEBUG org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat - Appending StructType() [empty row]
21:16:27.390 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-warehouse/t1/part-00003-7df4f35f-6bca-4c50-bf40-768a572edd6e-c000.snappy.parquet, range: 0-3003104, partition values: [empty row]
21:16:27.394 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] DEBUG org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport - Going to read the following fields from the Parquet file with the following schema:
Parquet file schema:
message spark_schema {
  required int64 key;
  required double value1;
}

Parquet clipped schema:
message spark_schema {
  required int64 key;
  required double value1;
}

Parquet requested schema:
message spark_schema {
  required int64 key;
  required double value1;
}

Catalyst requested schema:
root
-- key: long (nullable = true)
-- value1: double (nullable = true)


21:16:27.394 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] DEBUG org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat - Appending StructType() [empty row]
21:16:27.432 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-warehouse/t1/part-00001-7df4f35f-6bca-4c50-bf40-768a572edd6e-c000.snappy.parquet, range: 0-3003096, partition values: [empty row]
21:16:27.437 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] DEBUG org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport - Going to read the following fields from the Parquet file with the following schema:
Parquet file schema:
message spark_schema {
  required int64 key;
  required double value1;
}

Parquet clipped schema:
message spark_schema {
  required int64 key;
  required double value1;
}

Parquet requested schema:
message spark_schema {
  required int64 key;
  required double value1;
}

Catalyst requested schema:
root
-- key: long (nullable = true)
-- value1: double (nullable = true)


21:16:27.438 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] DEBUG org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat - Appending StructType() [empty row]
21:16:27.463 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] DEBUG org.apache.spark.shuffle.sort.io.LocalDiskShuffleMapOutputWriter - Writing shuffle index file for mapId 2 with length 3
21:16:27.464 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] DEBUG org.apache.spark.shuffle.IndexShuffleBlockResolver - Shuffle index for mapId 2: [4587,4657,4229]
21:16:27.465 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 3.0 (TID 2). 2050 bytes result sent to driver
21:16:27.465 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] DEBUG org.apache.spark.executor.ExecutorMetricsPoller - stageTCMP: (3, 0) -> 0
21:16:27.466 [dispatcher-event-loop-1] DEBUG org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_3.0, runningTasks: 0
21:16:27.466 [dispatcher-event-loop-1] DEBUG org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_4.0, runningTasks: 0
21:16:27.466 [dispatcher-event-loop-1] DEBUG org.apache.spark.scheduler.TaskSetManager - No tasks for locality level NO_PREF, so moving to locality level ANY
21:16:27.466 [dispatcher-event-loop-1] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 4.0 (TID 3) (192.168.31.69, executor driver, partition 0, PROCESS_LOCAL, 5202 bytes) taskResourceAssignments Map()
21:16:27.466 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 4.0 (TID 3)
21:16:27.466 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 3.0 (TID 2) in 231 ms on 192.168.31.69 (executor driver) (1/1)
21:16:27.466 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 3.0, whose tasks have all completed, from pool
21:16:27.467 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] DEBUG org.apache.spark.executor.ExecutorMetricsPoller - stageTCMP: (4, 0) -> 1
21:16:27.467 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - ShuffleMapTask finished on driver
21:16:27.467 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 3 (saveAsTable at PhysicalPlanSpec.scala:372) finished in 0.248 s
21:16:27.467 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
21:16:27.467 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] DEBUG org.apache.spark.storage.BlockManager - Getting local block broadcast_6
21:16:27.467 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - running: Set(ShuffleMapStage 4)
21:16:27.467 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - waiting: Set()
21:16:27.467 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - failed: Set()
21:16:27.467 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] DEBUG org.apache.spark.storage.BlockManager - Level for block broadcast_6 is StorageLevel(disk, memory, deserialized, 1 replicas)
21:16:27.467 [dag-scheduler-event-loop] DEBUG org.apache.spark.MapOutputTrackerMaster - Increasing epoch to 2
21:16:27.468 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - After removal of stage 3, remaining stages = 1
21:16:27.470 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - Considering join on: Some((key#59L = key#61L))
21:16:27.470 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - leftKeys:List(key#59L) | rightKeys:List(key#61L)
21:16:27.473 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-warehouse/t2/part-00001-a8e715dd-f214-4cc1-b612-0d05521c18e2-c000.snappy.parquet, range: 0-6748, partition values: [empty row]
SparkPlanner(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2@125ace20) - org.apache.spark.sql.execution.SparkStrategies$SpecialLimits$ -->【
ReturnAnswer
+- Project [key#59L, value1#60, value2#62]
   +- Join Inner, (key#59L = key#61L)
      :- LogicalQueryStage Filter (isnotnull(key#59L) AND might_contain(scalar-subquery#73 [], xxhash64(key#59L, 42))), ShuffleQueryStage 0
      +- LogicalQueryStage Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L)), ShuffleQueryStage 1
】 --> 【
PlanLater Project [key#59L, value1#60, value2#62]
】

21:16:27.473 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] DEBUG org.apache.spark.storage.BlockManager - Getting local block broadcast_5
21:16:27.473 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] DEBUG org.apache.spark.storage.BlockManager - Level for block broadcast_5 is StorageLevel(disk, memory, deserialized, 1 replicas)
SparkPlanner(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2@125ace20) - org.apache.spark.sql.execution.SparkStrategies$BasicOperators$ -->【
Project [key#59L, value1#60, value2#62]
+- Join Inner, (key#59L = key#61L)
   :- LogicalQueryStage Filter (isnotnull(key#59L) AND might_contain(scalar-subquery#73 [], xxhash64(key#59L, 42))), ShuffleQueryStage 0
   +- LogicalQueryStage Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L)), ShuffleQueryStage 1
】 --> 【
Project [key#59L, value1#60, value2#62]
+- PlanLater Join Inner, (key#59L = key#61L)
】

21:16:27.474 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - Considering join on: Some((key#59L = key#61L))
21:16:27.474 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - leftKeys:List(key#59L) | rightKeys:List(key#61L)
21:16:27.474 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - Considering join on: Some((key#59L = key#61L))
21:16:27.474 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - leftKeys:List(key#59L) | rightKeys:List(key#61L)
21:16:27.478 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] DEBUG org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport - Going to read the following fields from the Parquet file with the following schema:
Parquet file schema:
message spark_schema {
  required int64 key;
  required double value2;
}

Parquet clipped schema:
message spark_schema {
  required int64 key;
  required double value2;
}

Parquet requested schema:
message spark_schema {
  required int64 key;
  required double value2;
}

Catalyst requested schema:
root
-- key: long (nullable = true)
-- value2: double (nullable = true)


21:16:27.479 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] DEBUG org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat - Appending StructType() [empty row]
21:16:27.479 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.execution.adaptive.LogicalQueryStage - Physical stats available as Statistics(sizeInBytes=23.3 KiB, rowCount=994) for plan: ShuffleQueryStage 0
+- Exchange hashpartitioning(key#59L, 3), ENSURE_REQUIREMENTS, [plan_id=95]
   +- *(1) Filter (isnotnull(key#59L) AND might_contain(Subquery subquery#73, [id=#57], xxhash64(key#59L, 42)))
      :  +- Subquery subquery#73, [id=#57]
      :     +- AdaptiveSparkPlan isFinalPlan=true
               +- == Final Plan ==
                  ObjectHashAggregate(keys=[], functions=[bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0)], output=[bloomFilter#72])
                  +- ShuffleQueryStage 0
                     +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=141]
                        +- ObjectHashAggregate(keys=[], functions=[partial_bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0)], output=[buf#75])
                           +- *(1) Project [key#61L]
                              +- *(1) Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L))
                                 +- *(1) ColumnarToRow
                                    +- FileScan parquet default.t2[key#61L,value2#62] Batched: true, DataFilters: [isnotnull(value2#62), (value2#62 > 0.01), isnotnull(key#61L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(value2), GreaterThan(value2,0.01), IsNotNull(key)], ReadSchema: struct<key:bigint,value2:double>
               +- == Initial Plan ==
                  ObjectHashAggregate(keys=[], functions=[bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0)], output=[bloomFilter#72])
                  +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=55]
                     +- ObjectHashAggregate(keys=[], functions=[partial_bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0)], output=[buf#75])
                        +- Project [key#61L]
                           +- Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L))
                              +- FileScan parquet default.t2[key#61L,value2#62] Batched: true, DataFilters: [isnotnull(value2#62), (value2#62 > 0.01), isnotnull(key#61L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(value2), GreaterThan(value2,0.01), IsNotNull(key)], ReadSchema: struct<key:bigint,value2:double>
      +- *(1) ColumnarToRow
         +- FileScan parquet default.t1[key#59L,value1#60] Batched: true, DataFilters: [isnotnull(key#59L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(key)], ReadSchema: struct<key:bigint,value1:double>

21:16:27.481 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.execution.adaptive.LogicalQueryStage - Physical stats not available for plan: ShuffleQueryStage 1
+- Exchange hashpartitioning(key#61L, 3), ENSURE_REQUIREMENTS, [plan_id=117]
   +- *(2) Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L))
      +- *(2) ColumnarToRow
         +- FileScan parquet default.t2[key#61L,value2#62] Batched: true, DataFilters: [isnotnull(value2#62), (value2#62 > 0.01), isnotnull(key#61L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(value2), GreaterThan(value2,0.01), IsNotNull(key)], ReadSchema: struct<key:bigint,value2:double>

SparkPlanner(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2@125ace20) - org.apache.spark.sql.execution.SparkStrategies$JoinSelection$ -->【
Join Inner, (key#59L = key#61L)
:- LogicalQueryStage Filter (isnotnull(key#59L) AND might_contain(scalar-subquery#73 [], xxhash64(key#59L, 42))), ShuffleQueryStage 0
+- LogicalQueryStage Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L)), ShuffleQueryStage 1
】 --> 【
SortMergeJoin [key#59L], [key#61L], Inner
:- PlanLater LogicalQueryStage Filter (isnotnull(key#59L) AND might_contain(scalar-subquery#73 [], xxhash64(key#59L, 42))), ShuffleQueryStage 0
+- PlanLater LogicalQueryStage Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L)), ShuffleQueryStage 1
】

21:16:27.482 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-warehouse/t2/part-00000-a8e715dd-f214-4cc1-b612-0d05521c18e2-c000.snappy.parquet, range: 0-6741, partition values: [empty row]
SparkPlanner(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2@125ace20) - org.apache.spark.sql.execution.adaptive.LogicalQueryStageStrategy$ -->【
LogicalQueryStage Filter (isnotnull(key#59L) AND might_contain(scalar-subquery#73 [], xxhash64(key#59L, 42))), ShuffleQueryStage 0
】 --> 【
ShuffleQueryStage 0
+- Exchange hashpartitioning(key#59L, 3), ENSURE_REQUIREMENTS, [plan_id=95]
   +- *(1) Filter (isnotnull(key#59L) AND might_contain(Subquery subquery#73, [id=#57], xxhash64(key#59L, 42)))
      :  +- Subquery subquery#73, [id=#57]
      :     +- AdaptiveSparkPlan isFinalPlan=true
               +- == Final Plan ==
                  ObjectHashAggregate(keys=[], functions=[bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0)], output=[bloomFilter#72])
                  +- ShuffleQueryStage 0
                     +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=141]
                        +- ObjectHashAggregate(keys=[], functions=[partial_bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0)], output=[buf#75])
                           +- *(1) Project [key#61L]
                              +- *(1) Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L))
                                 +- *(1) ColumnarToRow
                                    +- FileScan parquet default.t2[key#61L,value2#62] Batched: true, DataFilters: [isnotnull(value2#62), (value2#62 > 0.01), isnotnull(key#61L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(value2), GreaterThan(value2,0.01), IsNotNull(key)], ReadSchema: struct<key:bigint,value2:double>
               +- == Initial Plan ==
                  ObjectHashAggregate(keys=[], functions=[bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0)], output=[bloomFilter#72])
                  +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=55]
                     +- ObjectHashAggregate(keys=[], functions=[partial_bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0)], output=[buf#75])
                        +- Project [key#61L]
                           +- Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L))
                              +- FileScan parquet default.t2[key#61L,value2#62] Batched: true, DataFilters: [isnotnull(value2#62), (value2#62 > 0.01), isnotnull(key#61L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(value2), GreaterThan(value2,0.01), IsNotNull(key)], ReadSchema: struct<key:bigint,value2:double>
      +- *(1) ColumnarToRow
         +- FileScan parquet default.t1[key#59L,value1#60] Batched: true, DataFilters: [isnotnull(key#59L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(key)], ReadSchema: struct<key:bigint,value1:double>
】

SparkPlanner(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2@125ace20) - org.apache.spark.sql.execution.adaptive.LogicalQueryStageStrategy$ -->【
LogicalQueryStage Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L)), ShuffleQueryStage 1
】 --> 【
ShuffleQueryStage 1
+- Exchange hashpartitioning(key#61L, 3), ENSURE_REQUIREMENTS, [plan_id=117]
   +- *(2) Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L))
      +- *(2) ColumnarToRow
         +- FileScan parquet default.t2[key#61L,value2#62] Batched: true, DataFilters: [isnotnull(value2#62), (value2#62 > 0.01), isnotnull(key#61L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(value2), GreaterThan(value2,0.01), IsNotNull(key)], ReadSchema: struct<key:bigint,value2:double>
】

21:16:27.490 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] DEBUG org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport - Going to read the following fields from the Parquet file with the following schema:
Parquet file schema:
message spark_schema {
  required int64 key;
  required double value2;
}

Parquet clipped schema:
message spark_schema {
  required int64 key;
  required double value2;
}

Parquet requested schema:
message spark_schema {
  required int64 key;
  required double value2;
}

Catalyst requested schema:
root
-- key: long (nullable = true)
-- value2: double (nullable = true)


org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec$ - applyPhysicalRules - org.apache.spark.sql.execution.exchange.EnsureRequirements -->【
Project [key#59L, value1#60, value2#62]
+- SortMergeJoin [key#59L], [key#61L], Inner
   :- ShuffleQueryStage 0
   :  +- Exchange hashpartitioning(key#59L, 3), ENSURE_REQUIREMENTS, [plan_id=95]
   :     +- *(1) Filter (isnotnull(key#59L) AND might_contain(Subquery subquery#73, [id=#57], xxhash64(key#59L, 42)))
   :        :  +- Subquery subquery#73, [id=#57]
   :        :     +- AdaptiveSparkPlan isFinalPlan=true
                     +- == Final Plan ==
                        ObjectHashAggregate(keys=[], functions=[bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0)], output=[bloomFilter#72])
                        +- ShuffleQueryStage 0
                           +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=141]
                              +- ObjectHashAggregate(keys=[], functions=[partial_bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0)], output=[buf#75])
                                 +- *(1) Project [key#61L]
                                    +- *(1) Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L))
                                       +- *(1) ColumnarToRow
                                          +- FileScan parquet default.t2[key#61L,value2#62] Batched: true, DataFilters: [isnotnull(value2#62), (value2#62 > 0.01), isnotnull(key#61L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(value2), GreaterThan(value2,0.01), IsNotNull(key)], ReadSchema: struct<key:bigint,value2:double>
                     +- == Initial Plan ==
                        ObjectHashAggregate(keys=[], functions=[bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0)], output=[bloomFilter#72])
                        +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=55]
                           +- ObjectHashAggregate(keys=[], functions=[partial_bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0)], output=[buf#75])
                              +- Project [key#61L]
                                 +- Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L))
                                    +- FileScan parquet default.t2[key#61L,value2#62] Batched: true, DataFilters: [isnotnull(value2#62), (value2#62 > 0.01), isnotnull(key#61L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(value2), GreaterThan(value2,0.01), IsNotNull(key)], ReadSchema: struct<key:bigint,value2:double>
   :        +- *(1) ColumnarToRow
   :           +- FileScan parquet default.t1[key#59L,value1#60] Batched: true, DataFilters: [isnotnull(key#59L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(key)], ReadSchema: struct<key:bigint,value1:double>
   +- ShuffleQueryStage 1
      +- Exchange hashpartitioning(key#61L, 3), ENSURE_REQUIREMENTS, [plan_id=117]
         +- *(2) Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L))
            +- *(2) ColumnarToRow
               +- FileScan parquet default.t2[key#61L,value2#62] Batched: true, DataFilters: [isnotnull(value2#62), (value2#62 > 0.01), isnotnull(key#61L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(value2), GreaterThan(value2,0.01), IsNotNull(key)], ReadSchema: struct<key:bigint,value2:double>
】AQE --> 【
Project [key#59L, value1#60, value2#62]
+- SortMergeJoin [key#59L], [key#61L], Inner
   :- Sort [key#59L ASC NULLS FIRST], false, 0
   :  +- ShuffleQueryStage 0
   :     +- Exchange hashpartitioning(key#59L, 3), ENSURE_REQUIREMENTS, [plan_id=95]
   :        +- *(1) Filter (isnotnull(key#59L) AND might_contain(Subquery subquery#73, [id=#57], xxhash64(key#59L, 42)))
   :           :  +- Subquery subquery#73, [id=#57]
   :           :     +- AdaptiveSparkPlan isFinalPlan=true
                        +- == Final Plan ==
                           ObjectHashAggregate(keys=[], functions=[bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0)], output=[bloomFilter#72])
                           +- ShuffleQueryStage 0
                              +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=141]
                                 +- ObjectHashAggregate(keys=[], functions=[partial_bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0)], output=[buf#75])
                                    +- *(1) Project [key#61L]
                                       +- *(1) Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L))
                                          +- *(1) ColumnarToRow
                                             +- FileScan parquet default.t2[key#61L,value2#62] Batched: true, DataFilters: [isnotnull(value2#62), (value2#62 > 0.01), isnotnull(key#61L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(value2), GreaterThan(value2,0.01), IsNotNull(key)], ReadSchema: struct<key:bigint,value2:double>
                        +- == Initial Plan ==
                           ObjectHashAggregate(keys=[], functions=[bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0)], output=[bloomFilter#72])
                           +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=55]
                              +- ObjectHashAggregate(keys=[], functions=[partial_bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0)], output=[buf#75])
                                 +- Project [key#61L]
                                    +- Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L))
                                       +- FileScan parquet default.t2[key#61L,value2#62] Batched: true, DataFilters: [isnotnull(value2#62), (value2#62 > 0.01), isnotnull(key#61L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(value2), GreaterThan(value2,0.01), IsNotNull(key)], ReadSchema: struct<key:bigint,value2:double>
   :           +- *(1) ColumnarToRow
   :              +- FileScan parquet default.t1[key#59L,value1#60] Batched: true, DataFilters: [isnotnull(key#59L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(key)], ReadSchema: struct<key:bigint,value1:double>
   +- Sort [key#61L ASC NULLS FIRST], false, 0
      +- ShuffleQueryStage 1
         +- Exchange hashpartitioning(key#61L, 3), ENSURE_REQUIREMENTS, [plan_id=117]
            +- *(2) Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L))
               +- *(2) ColumnarToRow
                  +- FileScan parquet default.t2[key#61L,value2#62] Batched: true, DataFilters: [isnotnull(value2#62), (value2#62 > 0.01), isnotnull(key#61L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(value2), GreaterThan(value2,0.01), IsNotNull(key)], ReadSchema: struct<key:bigint,value2:double>
】

21:16:27.491 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] DEBUG org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat - Appending StructType() [empty row]
org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec$ final result ==>【
Project [key#59L, value1#60, value2#62]
+- SortMergeJoin [key#59L], [key#61L], Inner
   :- ShuffleQueryStage 0
   :  +- Exchange hashpartitioning(key#59L, 3), ENSURE_REQUIREMENTS, [plan_id=95]
   :     +- *(1) Filter (isnotnull(key#59L) AND might_contain(Subquery subquery#73, [id=#57], xxhash64(key#59L, 42)))
   :        :  +- Subquery subquery#73, [id=#57]
   :        :     +- AdaptiveSparkPlan isFinalPlan=true
                     +- == Final Plan ==
                        ObjectHashAggregate(keys=[], functions=[bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0)], output=[bloomFilter#72])
                        +- ShuffleQueryStage 0
                           +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=141]
                              +- ObjectHashAggregate(keys=[], functions=[partial_bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0)], output=[buf#75])
                                 +- *(1) Project [key#61L]
                                    +- *(1) Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L))
                                       +- *(1) ColumnarToRow
                                          +- FileScan parquet default.t2[key#61L,value2#62] Batched: true, DataFilters: [isnotnull(value2#62), (value2#62 > 0.01), isnotnull(key#61L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(value2), GreaterThan(value2,0.01), IsNotNull(key)], ReadSchema: struct<key:bigint,value2:double>
                     +- == Initial Plan ==
                        ObjectHashAggregate(keys=[], functions=[bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0)], output=[bloomFilter#72])
                        +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=55]
                           +- ObjectHashAggregate(keys=[], functions=[partial_bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0)], output=[buf#75])
                              +- Project [key#61L]
                                 +- Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L))
                                    +- FileScan parquet default.t2[key#61L,value2#62] Batched: true, DataFilters: [isnotnull(value2#62), (value2#62 > 0.01), isnotnull(key#61L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(value2), GreaterThan(value2,0.01), IsNotNull(key)], ReadSchema: struct<key:bigint,value2:double>
   :        +- *(1) ColumnarToRow
   :           +- FileScan parquet default.t1[key#59L,value1#60] Batched: true, DataFilters: [isnotnull(key#59L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(key)], ReadSchema: struct<key:bigint,value1:double>
   +- ShuffleQueryStage 1
      +- Exchange hashpartitioning(key#61L, 3), ENSURE_REQUIREMENTS, [plan_id=117]
         +- *(2) Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L))
            +- *(2) ColumnarToRow
               +- FileScan parquet default.t2[key#61L,value2#62] Batched: true, DataFilters: [isnotnull(value2#62), (value2#62 > 0.01), isnotnull(key#61L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(value2), GreaterThan(value2,0.01), IsNotNull(key)], ReadSchema: struct<key:bigint,value2:double>
】AQE ==> 【
Project [key#59L, value1#60, value2#62]
+- SortMergeJoin [key#59L], [key#61L], Inner
   :- Sort [key#59L ASC NULLS FIRST], false, 0
   :  +- ShuffleQueryStage 0
   :     +- Exchange hashpartitioning(key#59L, 3), ENSURE_REQUIREMENTS, [plan_id=95]
   :        +- *(1) Filter (isnotnull(key#59L) AND might_contain(Subquery subquery#73, [id=#57], xxhash64(key#59L, 42)))
   :           :  +- Subquery subquery#73, [id=#57]
   :           :     +- AdaptiveSparkPlan isFinalPlan=true
                        +- == Final Plan ==
                           ObjectHashAggregate(keys=[], functions=[bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0)], output=[bloomFilter#72])
                           +- ShuffleQueryStage 0
                              +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=141]
                                 +- ObjectHashAggregate(keys=[], functions=[partial_bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0)], output=[buf#75])
                                    +- *(1) Project [key#61L]
                                       +- *(1) Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L))
                                          +- *(1) ColumnarToRow
                                             +- FileScan parquet default.t2[key#61L,value2#62] Batched: true, DataFilters: [isnotnull(value2#62), (value2#62 > 0.01), isnotnull(key#61L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(value2), GreaterThan(value2,0.01), IsNotNull(key)], ReadSchema: struct<key:bigint,value2:double>
                        +- == Initial Plan ==
                           ObjectHashAggregate(keys=[], functions=[bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0)], output=[bloomFilter#72])
                           +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=55]
                              +- ObjectHashAggregate(keys=[], functions=[partial_bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0)], output=[buf#75])
                                 +- Project [key#61L]
                                    +- Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L))
                                       +- FileScan parquet default.t2[key#61L,value2#62] Batched: true, DataFilters: [isnotnull(value2#62), (value2#62 > 0.01), isnotnull(key#61L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(value2), GreaterThan(value2,0.01), IsNotNull(key)], ReadSchema: struct<key:bigint,value2:double>
   :           +- *(1) ColumnarToRow
   :              +- FileScan parquet default.t1[key#59L,value1#60] Batched: true, DataFilters: [isnotnull(key#59L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(key)], ReadSchema: struct<key:bigint,value1:double>
   +- Sort [key#61L ASC NULLS FIRST], false, 0
      +- ShuffleQueryStage 1
         +- Exchange hashpartitioning(key#61L, 3), ENSURE_REQUIREMENTS, [plan_id=117]
            +- *(2) Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L))
               +- *(2) ColumnarToRow
                  +- FileScan parquet default.t2[key#61L,value2#62] Batched: true, DataFilters: [isnotnull(value2#62), (value2#62 > 0.01), isnotnull(key#61L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(value2), GreaterThan(value2,0.01), IsNotNull(key)], ReadSchema: struct<key:bigint,value2:double>
】

21:16:27.494 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] DEBUG org.apache.spark.shuffle.sort.io.LocalDiskShuffleMapOutputWriter - Writing shuffle index file for mapId 3 with length 3
21:16:27.495 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] DEBUG org.apache.spark.shuffle.IndexShuffleBlockResolver - Shuffle index for mapId 3: [4587,4642,4441]
21:16:27.496 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 4.0 (TID 3). 2050 bytes result sent to driver
21:16:27.496 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] DEBUG org.apache.spark.executor.ExecutorMetricsPoller - stageTCMP: (4, 0) -> 0
21:16:27.496 [dispatcher-event-loop-1] DEBUG org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_4.0, runningTasks: 0
21:16:27.497 [dispatcher-event-loop-1] DEBUG org.apache.spark.scheduler.TaskSetManager - No tasks for locality level NO_PREF, so moving to locality level ANY
21:16:27.497 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 4.0 (TID 3) in 31 ms on 192.168.31.69 (executor driver) (1/1)
21:16:27.497 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 4.0, whose tasks have all completed, from pool
21:16:27.497 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - ShuffleMapTask finished on driver
21:16:27.498 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 4 (saveAsTable at PhysicalPlanSpec.scala:372) finished in 0.217 s
21:16:27.498 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
21:16:27.498 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - running: Set()
21:16:27.498 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - waiting: Set()
21:16:27.498 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - failed: Set()
21:16:27.498 [dag-scheduler-event-loop] DEBUG org.apache.spark.MapOutputTrackerMaster - Increasing epoch to 3
21:16:27.498 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - After removal of stage 4, remaining stages = 0
21:16:27.499 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - Considering join on: Some((key#59L = key#61L))
21:16:27.499 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - leftKeys:List(key#59L) | rightKeys:List(key#61L)
SparkPlanner(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2@125ace20) - org.apache.spark.sql.execution.SparkStrategies$SpecialLimits$ -->【
ReturnAnswer
+- Project [key#59L, value1#60, value2#62]
   +- Join Inner, (key#59L = key#61L)
      :- LogicalQueryStage Filter (isnotnull(key#59L) AND might_contain(scalar-subquery#73 [], xxhash64(key#59L, 42))), ShuffleQueryStage 0
      +- LogicalQueryStage Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L)), ShuffleQueryStage 1
】 --> 【
PlanLater Project [key#59L, value1#60, value2#62]
】

SparkPlanner(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2@125ace20) - org.apache.spark.sql.execution.SparkStrategies$BasicOperators$ -->【
Project [key#59L, value1#60, value2#62]
+- Join Inner, (key#59L = key#61L)
   :- LogicalQueryStage Filter (isnotnull(key#59L) AND might_contain(scalar-subquery#73 [], xxhash64(key#59L, 42))), ShuffleQueryStage 0
   +- LogicalQueryStage Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L)), ShuffleQueryStage 1
】 --> 【
Project [key#59L, value1#60, value2#62]
+- PlanLater Join Inner, (key#59L = key#61L)
】

21:16:27.501 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - Considering join on: Some((key#59L = key#61L))
21:16:27.501 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - leftKeys:List(key#59L) | rightKeys:List(key#61L)
21:16:27.501 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - Considering join on: Some((key#59L = key#61L))
21:16:27.502 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys - leftKeys:List(key#59L) | rightKeys:List(key#61L)
21:16:27.503 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.execution.adaptive.LogicalQueryStage - Physical stats available as Statistics(sizeInBytes=23.3 KiB, rowCount=994) for plan: ShuffleQueryStage 0
+- Exchange hashpartitioning(key#59L, 3), ENSURE_REQUIREMENTS, [plan_id=95]
   +- *(1) Filter (isnotnull(key#59L) AND might_contain(Subquery subquery#73, [id=#57], xxhash64(key#59L, 42)))
      :  +- Subquery subquery#73, [id=#57]
      :     +- AdaptiveSparkPlan isFinalPlan=true
               +- == Final Plan ==
                  ObjectHashAggregate(keys=[], functions=[bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0)], output=[bloomFilter#72])
                  +- ShuffleQueryStage 0
                     +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=141]
                        +- ObjectHashAggregate(keys=[], functions=[partial_bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0)], output=[buf#75])
                           +- *(1) Project [key#61L]
                              +- *(1) Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L))
                                 +- *(1) ColumnarToRow
                                    +- FileScan parquet default.t2[key#61L,value2#62] Batched: true, DataFilters: [isnotnull(value2#62), (value2#62 > 0.01), isnotnull(key#61L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(value2), GreaterThan(value2,0.01), IsNotNull(key)], ReadSchema: struct<key:bigint,value2:double>
               +- == Initial Plan ==
                  ObjectHashAggregate(keys=[], functions=[bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0)], output=[bloomFilter#72])
                  +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=55]
                     +- ObjectHashAggregate(keys=[], functions=[partial_bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0)], output=[buf#75])
                        +- Project [key#61L]
                           +- Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L))
                              +- FileScan parquet default.t2[key#61L,value2#62] Batched: true, DataFilters: [isnotnull(value2#62), (value2#62 > 0.01), isnotnull(key#61L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(value2), GreaterThan(value2,0.01), IsNotNull(key)], ReadSchema: struct<key:bigint,value2:double>
      +- *(1) ColumnarToRow
         +- FileScan parquet default.t1[key#59L,value1#60] Batched: true, DataFilters: [isnotnull(key#59L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(key)], ReadSchema: struct<key:bigint,value1:double>

21:16:27.503 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.execution.adaptive.LogicalQueryStage - Physical stats available as Statistics(sizeInBytes=23.3 KiB, rowCount=994) for plan: ShuffleQueryStage 1
+- Exchange hashpartitioning(key#61L, 3), ENSURE_REQUIREMENTS, [plan_id=117]
   +- *(2) Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L))
      +- *(2) ColumnarToRow
         +- FileScan parquet default.t2[key#61L,value2#62] Batched: true, DataFilters: [isnotnull(value2#62), (value2#62 > 0.01), isnotnull(key#61L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(value2), GreaterThan(value2,0.01), IsNotNull(key)], ReadSchema: struct<key:bigint,value2:double>

SparkPlanner(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2@125ace20) - org.apache.spark.sql.execution.SparkStrategies$JoinSelection$ -->【
Join Inner, (key#59L = key#61L)
:- LogicalQueryStage Filter (isnotnull(key#59L) AND might_contain(scalar-subquery#73 [], xxhash64(key#59L, 42))), ShuffleQueryStage 0
+- LogicalQueryStage Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L)), ShuffleQueryStage 1
】 --> 【
SortMergeJoin [key#59L], [key#61L], Inner
:- PlanLater LogicalQueryStage Filter (isnotnull(key#59L) AND might_contain(scalar-subquery#73 [], xxhash64(key#59L, 42))), ShuffleQueryStage 0
+- PlanLater LogicalQueryStage Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L)), ShuffleQueryStage 1
】

SparkPlanner(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2@125ace20) - org.apache.spark.sql.execution.adaptive.LogicalQueryStageStrategy$ -->【
LogicalQueryStage Filter (isnotnull(key#59L) AND might_contain(scalar-subquery#73 [], xxhash64(key#59L, 42))), ShuffleQueryStage 0
】 --> 【
ShuffleQueryStage 0
+- Exchange hashpartitioning(key#59L, 3), ENSURE_REQUIREMENTS, [plan_id=95]
   +- *(1) Filter (isnotnull(key#59L) AND might_contain(Subquery subquery#73, [id=#57], xxhash64(key#59L, 42)))
      :  +- Subquery subquery#73, [id=#57]
      :     +- AdaptiveSparkPlan isFinalPlan=true
               +- == Final Plan ==
                  ObjectHashAggregate(keys=[], functions=[bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0)], output=[bloomFilter#72])
                  +- ShuffleQueryStage 0
                     +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=141]
                        +- ObjectHashAggregate(keys=[], functions=[partial_bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0)], output=[buf#75])
                           +- *(1) Project [key#61L]
                              +- *(1) Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L))
                                 +- *(1) ColumnarToRow
                                    +- FileScan parquet default.t2[key#61L,value2#62] Batched: true, DataFilters: [isnotnull(value2#62), (value2#62 > 0.01), isnotnull(key#61L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(value2), GreaterThan(value2,0.01), IsNotNull(key)], ReadSchema: struct<key:bigint,value2:double>
               +- == Initial Plan ==
                  ObjectHashAggregate(keys=[], functions=[bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0)], output=[bloomFilter#72])
                  +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=55]
                     +- ObjectHashAggregate(keys=[], functions=[partial_bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0)], output=[buf#75])
                        +- Project [key#61L]
                           +- Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L))
                              +- FileScan parquet default.t2[key#61L,value2#62] Batched: true, DataFilters: [isnotnull(value2#62), (value2#62 > 0.01), isnotnull(key#61L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(value2), GreaterThan(value2,0.01), IsNotNull(key)], ReadSchema: struct<key:bigint,value2:double>
      +- *(1) ColumnarToRow
         +- FileScan parquet default.t1[key#59L,value1#60] Batched: true, DataFilters: [isnotnull(key#59L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(key)], ReadSchema: struct<key:bigint,value1:double>
】

SparkPlanner(org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$2@125ace20) - org.apache.spark.sql.execution.adaptive.LogicalQueryStageStrategy$ -->【
LogicalQueryStage Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L)), ShuffleQueryStage 1
】 --> 【
ShuffleQueryStage 1
+- Exchange hashpartitioning(key#61L, 3), ENSURE_REQUIREMENTS, [plan_id=117]
   +- *(2) Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L))
      +- *(2) ColumnarToRow
         +- FileScan parquet default.t2[key#61L,value2#62] Batched: true, DataFilters: [isnotnull(value2#62), (value2#62 > 0.01), isnotnull(key#61L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(value2), GreaterThan(value2,0.01), IsNotNull(key)], ReadSchema: struct<key:bigint,value2:double>
】

org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec$ - applyPhysicalRules - org.apache.spark.sql.execution.exchange.EnsureRequirements -->【
Project [key#59L, value1#60, value2#62]
+- SortMergeJoin [key#59L], [key#61L], Inner
   :- ShuffleQueryStage 0
   :  +- Exchange hashpartitioning(key#59L, 3), ENSURE_REQUIREMENTS, [plan_id=95]
   :     +- *(1) Filter (isnotnull(key#59L) AND might_contain(Subquery subquery#73, [id=#57], xxhash64(key#59L, 42)))
   :        :  +- Subquery subquery#73, [id=#57]
   :        :     +- AdaptiveSparkPlan isFinalPlan=true
                     +- == Final Plan ==
                        ObjectHashAggregate(keys=[], functions=[bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0)], output=[bloomFilter#72])
                        +- ShuffleQueryStage 0
                           +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=141]
                              +- ObjectHashAggregate(keys=[], functions=[partial_bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0)], output=[buf#75])
                                 +- *(1) Project [key#61L]
                                    +- *(1) Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L))
                                       +- *(1) ColumnarToRow
                                          +- FileScan parquet default.t2[key#61L,value2#62] Batched: true, DataFilters: [isnotnull(value2#62), (value2#62 > 0.01), isnotnull(key#61L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(value2), GreaterThan(value2,0.01), IsNotNull(key)], ReadSchema: struct<key:bigint,value2:double>
                     +- == Initial Plan ==
                        ObjectHashAggregate(keys=[], functions=[bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0)], output=[bloomFilter#72])
                        +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=55]
                           +- ObjectHashAggregate(keys=[], functions=[partial_bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0)], output=[buf#75])
                              +- Project [key#61L]
                                 +- Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L))
                                    +- FileScan parquet default.t2[key#61L,value2#62] Batched: true, DataFilters: [isnotnull(value2#62), (value2#62 > 0.01), isnotnull(key#61L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(value2), GreaterThan(value2,0.01), IsNotNull(key)], ReadSchema: struct<key:bigint,value2:double>
   :        +- *(1) ColumnarToRow
   :           +- FileScan parquet default.t1[key#59L,value1#60] Batched: true, DataFilters: [isnotnull(key#59L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(key)], ReadSchema: struct<key:bigint,value1:double>
   +- ShuffleQueryStage 1
      +- Exchange hashpartitioning(key#61L, 3), ENSURE_REQUIREMENTS, [plan_id=117]
         +- *(2) Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L))
            +- *(2) ColumnarToRow
               +- FileScan parquet default.t2[key#61L,value2#62] Batched: true, DataFilters: [isnotnull(value2#62), (value2#62 > 0.01), isnotnull(key#61L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(value2), GreaterThan(value2,0.01), IsNotNull(key)], ReadSchema: struct<key:bigint,value2:double>
】AQE --> 【
Project [key#59L, value1#60, value2#62]
+- SortMergeJoin [key#59L], [key#61L], Inner
   :- Sort [key#59L ASC NULLS FIRST], false, 0
   :  +- ShuffleQueryStage 0
   :     +- Exchange hashpartitioning(key#59L, 3), ENSURE_REQUIREMENTS, [plan_id=95]
   :        +- *(1) Filter (isnotnull(key#59L) AND might_contain(Subquery subquery#73, [id=#57], xxhash64(key#59L, 42)))
   :           :  +- Subquery subquery#73, [id=#57]
   :           :     +- AdaptiveSparkPlan isFinalPlan=true
                        +- == Final Plan ==
                           ObjectHashAggregate(keys=[], functions=[bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0)], output=[bloomFilter#72])
                           +- ShuffleQueryStage 0
                              +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=141]
                                 +- ObjectHashAggregate(keys=[], functions=[partial_bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0)], output=[buf#75])
                                    +- *(1) Project [key#61L]
                                       +- *(1) Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L))
                                          +- *(1) ColumnarToRow
                                             +- FileScan parquet default.t2[key#61L,value2#62] Batched: true, DataFilters: [isnotnull(value2#62), (value2#62 > 0.01), isnotnull(key#61L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(value2), GreaterThan(value2,0.01), IsNotNull(key)], ReadSchema: struct<key:bigint,value2:double>
                        +- == Initial Plan ==
                           ObjectHashAggregate(keys=[], functions=[bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0)], output=[bloomFilter#72])
                           +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=55]
                              +- ObjectHashAggregate(keys=[], functions=[partial_bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0)], output=[buf#75])
                                 +- Project [key#61L]
                                    +- Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L))
                                       +- FileScan parquet default.t2[key#61L,value2#62] Batched: true, DataFilters: [isnotnull(value2#62), (value2#62 > 0.01), isnotnull(key#61L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(value2), GreaterThan(value2,0.01), IsNotNull(key)], ReadSchema: struct<key:bigint,value2:double>
   :           +- *(1) ColumnarToRow
   :              +- FileScan parquet default.t1[key#59L,value1#60] Batched: true, DataFilters: [isnotnull(key#59L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(key)], ReadSchema: struct<key:bigint,value1:double>
   +- Sort [key#61L ASC NULLS FIRST], false, 0
      +- ShuffleQueryStage 1
         +- Exchange hashpartitioning(key#61L, 3), ENSURE_REQUIREMENTS, [plan_id=117]
            +- *(2) Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L))
               +- *(2) ColumnarToRow
                  +- FileScan parquet default.t2[key#61L,value2#62] Batched: true, DataFilters: [isnotnull(value2#62), (value2#62 > 0.01), isnotnull(key#61L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(value2), GreaterThan(value2,0.01), IsNotNull(key)], ReadSchema: struct<key:bigint,value2:double>
】

21:16:27.511 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.execution.adaptive.OptimizeSkewedJoin -
Optimizing skewed join.
Left side partitions size info:
median size: 4830, max size: 4830, min size: 4390, avg size: 4683
Right side partitions size info:
median size: 4830, max size: 4830, min size: 4830, avg size: 4830

21:16:27.515 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.execution.adaptive.OptimizeSkewedJoin - number of skewed partitions: left 0, right 0
org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec$ final result ==>【
Project [key#59L, value1#60, value2#62]
+- SortMergeJoin [key#59L], [key#61L], Inner
   :- ShuffleQueryStage 0
   :  +- Exchange hashpartitioning(key#59L, 3), ENSURE_REQUIREMENTS, [plan_id=95]
   :     +- *(1) Filter (isnotnull(key#59L) AND might_contain(Subquery subquery#73, [id=#57], xxhash64(key#59L, 42)))
   :        :  +- Subquery subquery#73, [id=#57]
   :        :     +- AdaptiveSparkPlan isFinalPlan=true
                     +- == Final Plan ==
                        ObjectHashAggregate(keys=[], functions=[bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0)], output=[bloomFilter#72])
                        +- ShuffleQueryStage 0
                           +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=141]
                              +- ObjectHashAggregate(keys=[], functions=[partial_bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0)], output=[buf#75])
                                 +- *(1) Project [key#61L]
                                    +- *(1) Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L))
                                       +- *(1) ColumnarToRow
                                          +- FileScan parquet default.t2[key#61L,value2#62] Batched: true, DataFilters: [isnotnull(value2#62), (value2#62 > 0.01), isnotnull(key#61L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(value2), GreaterThan(value2,0.01), IsNotNull(key)], ReadSchema: struct<key:bigint,value2:double>
                     +- == Initial Plan ==
                        ObjectHashAggregate(keys=[], functions=[bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0)], output=[bloomFilter#72])
                        +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=55]
                           +- ObjectHashAggregate(keys=[], functions=[partial_bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0)], output=[buf#75])
                              +- Project [key#61L]
                                 +- Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L))
                                    +- FileScan parquet default.t2[key#61L,value2#62] Batched: true, DataFilters: [isnotnull(value2#62), (value2#62 > 0.01), isnotnull(key#61L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(value2), GreaterThan(value2,0.01), IsNotNull(key)], ReadSchema: struct<key:bigint,value2:double>
   :        +- *(1) ColumnarToRow
   :           +- FileScan parquet default.t1[key#59L,value1#60] Batched: true, DataFilters: [isnotnull(key#59L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(key)], ReadSchema: struct<key:bigint,value1:double>
   +- ShuffleQueryStage 1
      +- Exchange hashpartitioning(key#61L, 3), ENSURE_REQUIREMENTS, [plan_id=117]
         +- *(2) Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L))
            +- *(2) ColumnarToRow
               +- FileScan parquet default.t2[key#61L,value2#62] Batched: true, DataFilters: [isnotnull(value2#62), (value2#62 > 0.01), isnotnull(key#61L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(value2), GreaterThan(value2,0.01), IsNotNull(key)], ReadSchema: struct<key:bigint,value2:double>
】AQE ==> 【
Project [key#59L, value1#60, value2#62]
+- SortMergeJoin [key#59L], [key#61L], Inner
   :- Sort [key#59L ASC NULLS FIRST], false, 0
   :  +- ShuffleQueryStage 0
   :     +- Exchange hashpartitioning(key#59L, 3), ENSURE_REQUIREMENTS, [plan_id=95]
   :        +- *(1) Filter (isnotnull(key#59L) AND might_contain(Subquery subquery#73, [id=#57], xxhash64(key#59L, 42)))
   :           :  +- Subquery subquery#73, [id=#57]
   :           :     +- AdaptiveSparkPlan isFinalPlan=true
                        +- == Final Plan ==
                           ObjectHashAggregate(keys=[], functions=[bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0)], output=[bloomFilter#72])
                           +- ShuffleQueryStage 0
                              +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=141]
                                 +- ObjectHashAggregate(keys=[], functions=[partial_bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0)], output=[buf#75])
                                    +- *(1) Project [key#61L]
                                       +- *(1) Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L))
                                          +- *(1) ColumnarToRow
                                             +- FileScan parquet default.t2[key#61L,value2#62] Batched: true, DataFilters: [isnotnull(value2#62), (value2#62 > 0.01), isnotnull(key#61L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(value2), GreaterThan(value2,0.01), IsNotNull(key)], ReadSchema: struct<key:bigint,value2:double>
                        +- == Initial Plan ==
                           ObjectHashAggregate(keys=[], functions=[bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0)], output=[bloomFilter#72])
                           +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=55]
                              +- ObjectHashAggregate(keys=[], functions=[partial_bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0)], output=[buf#75])
                                 +- Project [key#61L]
                                    +- Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L))
                                       +- FileScan parquet default.t2[key#61L,value2#62] Batched: true, DataFilters: [isnotnull(value2#62), (value2#62 > 0.01), isnotnull(key#61L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(value2), GreaterThan(value2,0.01), IsNotNull(key)], ReadSchema: struct<key:bigint,value2:double>
   :           +- *(1) ColumnarToRow
   :              +- FileScan parquet default.t1[key#59L,value1#60] Batched: true, DataFilters: [isnotnull(key#59L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(key)], ReadSchema: struct<key:bigint,value1:double>
   +- Sort [key#61L ASC NULLS FIRST], false, 0
      +- ShuffleQueryStage 1
         +- Exchange hashpartitioning(key#61L, 3), ENSURE_REQUIREMENTS, [plan_id=117]
            +- *(2) Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L))
               +- *(2) ColumnarToRow
                  +- FileScan parquet default.t2[key#61L,value2#62] Batched: true, DataFilters: [isnotnull(value2#62), (value2#62 > 0.01), isnotnull(key#61L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(value2), GreaterThan(value2,0.01), IsNotNull(key)], ReadSchema: struct<key:bigint,value2:double>
】

org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec$ - applyPhysicalRules - org.apache.spark.sql.execution.CollapseCodegenStages -->【
Project [key#59L, value1#60, value2#62]
+- SortMergeJoin [key#59L], [key#61L], Inner
   :- Sort [key#59L ASC NULLS FIRST], false, 0
   :  +- ShuffleQueryStage 0
   :     +- Exchange hashpartitioning(key#59L, 3), ENSURE_REQUIREMENTS, [plan_id=95]
   :        +- *(1) Filter (isnotnull(key#59L) AND might_contain(Subquery subquery#73, [id=#57], xxhash64(key#59L, 42)))
   :           :  +- Subquery subquery#73, [id=#57]
   :           :     +- AdaptiveSparkPlan isFinalPlan=true
                        +- == Final Plan ==
                           ObjectHashAggregate(keys=[], functions=[bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0)], output=[bloomFilter#72])
                           +- ShuffleQueryStage 0
                              +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=141]
                                 +- ObjectHashAggregate(keys=[], functions=[partial_bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0)], output=[buf#75])
                                    +- *(1) Project [key#61L]
                                       +- *(1) Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L))
                                          +- *(1) ColumnarToRow
                                             +- FileScan parquet default.t2[key#61L,value2#62] Batched: true, DataFilters: [isnotnull(value2#62), (value2#62 > 0.01), isnotnull(key#61L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(value2), GreaterThan(value2,0.01), IsNotNull(key)], ReadSchema: struct<key:bigint,value2:double>
                        +- == Initial Plan ==
                           ObjectHashAggregate(keys=[], functions=[bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0)], output=[bloomFilter#72])
                           +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=55]
                              +- ObjectHashAggregate(keys=[], functions=[partial_bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0)], output=[buf#75])
                                 +- Project [key#61L]
                                    +- Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L))
                                       +- FileScan parquet default.t2[key#61L,value2#62] Batched: true, DataFilters: [isnotnull(value2#62), (value2#62 > 0.01), isnotnull(key#61L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(value2), GreaterThan(value2,0.01), IsNotNull(key)], ReadSchema: struct<key:bigint,value2:double>
   :           +- *(1) ColumnarToRow
   :              +- FileScan parquet default.t1[key#59L,value1#60] Batched: true, DataFilters: [isnotnull(key#59L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(key)], ReadSchema: struct<key:bigint,value1:double>
   +- Sort [key#61L ASC NULLS FIRST], false, 0
      +- ShuffleQueryStage 1
         +- Exchange hashpartitioning(key#61L, 3), ENSURE_REQUIREMENTS, [plan_id=117]
            +- *(2) Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L))
               +- *(2) ColumnarToRow
                  +- FileScan parquet default.t2[key#61L,value2#62] Batched: true, DataFilters: [isnotnull(value2#62), (value2#62 > 0.01), isnotnull(key#61L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(value2), GreaterThan(value2,0.01), IsNotNull(key)], ReadSchema: struct<key:bigint,value2:double>
】AQE --> 【
*(5) Project [key#59L, value1#60, value2#62]
+- *(5) SortMergeJoin [key#59L], [key#61L], Inner
   :- *(3) Sort [key#59L ASC NULLS FIRST], false, 0
   :  +- ShuffleQueryStage 0
   :     +- Exchange hashpartitioning(key#59L, 3), ENSURE_REQUIREMENTS, [plan_id=95]
   :        +- *(1) Filter (isnotnull(key#59L) AND might_contain(Subquery subquery#73, [id=#57], xxhash64(key#59L, 42)))
   :           :  +- Subquery subquery#73, [id=#57]
   :           :     +- AdaptiveSparkPlan isFinalPlan=true
                        +- == Final Plan ==
                           ObjectHashAggregate(keys=[], functions=[bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0)], output=[bloomFilter#72])
                           +- ShuffleQueryStage 0
                              +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=141]
                                 +- ObjectHashAggregate(keys=[], functions=[partial_bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0)], output=[buf#75])
                                    +- *(1) Project [key#61L]
                                       +- *(1) Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L))
                                          +- *(1) ColumnarToRow
                                             +- FileScan parquet default.t2[key#61L,value2#62] Batched: true, DataFilters: [isnotnull(value2#62), (value2#62 > 0.01), isnotnull(key#61L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(value2), GreaterThan(value2,0.01), IsNotNull(key)], ReadSchema: struct<key:bigint,value2:double>
                        +- == Initial Plan ==
                           ObjectHashAggregate(keys=[], functions=[bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0)], output=[bloomFilter#72])
                           +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=55]
                              +- ObjectHashAggregate(keys=[], functions=[partial_bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0)], output=[buf#75])
                                 +- Project [key#61L]
                                    +- Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L))
                                       +- FileScan parquet default.t2[key#61L,value2#62] Batched: true, DataFilters: [isnotnull(value2#62), (value2#62 > 0.01), isnotnull(key#61L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(value2), GreaterThan(value2,0.01), IsNotNull(key)], ReadSchema: struct<key:bigint,value2:double>
   :           +- *(1) ColumnarToRow
   :              +- FileScan parquet default.t1[key#59L,value1#60] Batched: true, DataFilters: [isnotnull(key#59L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(key)], ReadSchema: struct<key:bigint,value1:double>
   +- *(4) Sort [key#61L ASC NULLS FIRST], false, 0
      +- ShuffleQueryStage 1
         +- Exchange hashpartitioning(key#61L, 3), ENSURE_REQUIREMENTS, [plan_id=117]
            +- *(2) Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L))
               +- *(2) ColumnarToRow
                  +- FileScan parquet default.t2[key#61L,value2#62] Batched: true, DataFilters: [isnotnull(value2#62), (value2#62 > 0.01), isnotnull(key#61L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(value2), GreaterThan(value2,0.01), IsNotNull(key)], ReadSchema: struct<key:bigint,value2:double>
】

org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec$ final result ==>【
Project [key#59L, value1#60, value2#62]
+- SortMergeJoin [key#59L], [key#61L], Inner
   :- Sort [key#59L ASC NULLS FIRST], false, 0
   :  +- ShuffleQueryStage 0
   :     +- Exchange hashpartitioning(key#59L, 3), ENSURE_REQUIREMENTS, [plan_id=95]
   :        +- *(1) Filter (isnotnull(key#59L) AND might_contain(Subquery subquery#73, [id=#57], xxhash64(key#59L, 42)))
   :           :  +- Subquery subquery#73, [id=#57]
   :           :     +- AdaptiveSparkPlan isFinalPlan=true
                        +- == Final Plan ==
                           ObjectHashAggregate(keys=[], functions=[bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0)], output=[bloomFilter#72])
                           +- ShuffleQueryStage 0
                              +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=141]
                                 +- ObjectHashAggregate(keys=[], functions=[partial_bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0)], output=[buf#75])
                                    +- *(1) Project [key#61L]
                                       +- *(1) Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L))
                                          +- *(1) ColumnarToRow
                                             +- FileScan parquet default.t2[key#61L,value2#62] Batched: true, DataFilters: [isnotnull(value2#62), (value2#62 > 0.01), isnotnull(key#61L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(value2), GreaterThan(value2,0.01), IsNotNull(key)], ReadSchema: struct<key:bigint,value2:double>
                        +- == Initial Plan ==
                           ObjectHashAggregate(keys=[], functions=[bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0)], output=[bloomFilter#72])
                           +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=55]
                              +- ObjectHashAggregate(keys=[], functions=[partial_bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0)], output=[buf#75])
                                 +- Project [key#61L]
                                    +- Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L))
                                       +- FileScan parquet default.t2[key#61L,value2#62] Batched: true, DataFilters: [isnotnull(value2#62), (value2#62 > 0.01), isnotnull(key#61L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(value2), GreaterThan(value2,0.01), IsNotNull(key)], ReadSchema: struct<key:bigint,value2:double>
   :           +- *(1) ColumnarToRow
   :              +- FileScan parquet default.t1[key#59L,value1#60] Batched: true, DataFilters: [isnotnull(key#59L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(key)], ReadSchema: struct<key:bigint,value1:double>
   +- Sort [key#61L ASC NULLS FIRST], false, 0
      +- ShuffleQueryStage 1
         +- Exchange hashpartitioning(key#61L, 3), ENSURE_REQUIREMENTS, [plan_id=117]
            +- *(2) Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L))
               +- *(2) ColumnarToRow
                  +- FileScan parquet default.t2[key#61L,value2#62] Batched: true, DataFilters: [isnotnull(value2#62), (value2#62 > 0.01), isnotnull(key#61L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(value2), GreaterThan(value2,0.01), IsNotNull(key)], ReadSchema: struct<key:bigint,value2:double>
】AQE ==> 【
*(5) Project [key#59L, value1#60, value2#62]
+- *(5) SortMergeJoin [key#59L], [key#61L], Inner
   :- *(3) Sort [key#59L ASC NULLS FIRST], false, 0
   :  +- ShuffleQueryStage 0
   :     +- Exchange hashpartitioning(key#59L, 3), ENSURE_REQUIREMENTS, [plan_id=95]
   :        +- *(1) Filter (isnotnull(key#59L) AND might_contain(Subquery subquery#73, [id=#57], xxhash64(key#59L, 42)))
   :           :  +- Subquery subquery#73, [id=#57]
   :           :     +- AdaptiveSparkPlan isFinalPlan=true
                        +- == Final Plan ==
                           ObjectHashAggregate(keys=[], functions=[bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0)], output=[bloomFilter#72])
                           +- ShuffleQueryStage 0
                              +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=141]
                                 +- ObjectHashAggregate(keys=[], functions=[partial_bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0)], output=[buf#75])
                                    +- *(1) Project [key#61L]
                                       +- *(1) Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L))
                                          +- *(1) ColumnarToRow
                                             +- FileScan parquet default.t2[key#61L,value2#62] Batched: true, DataFilters: [isnotnull(value2#62), (value2#62 > 0.01), isnotnull(key#61L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(value2), GreaterThan(value2,0.01), IsNotNull(key)], ReadSchema: struct<key:bigint,value2:double>
                        +- == Initial Plan ==
                           ObjectHashAggregate(keys=[], functions=[bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0)], output=[bloomFilter#72])
                           +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=55]
                              +- ObjectHashAggregate(keys=[], functions=[partial_bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0)], output=[buf#75])
                                 +- Project [key#61L]
                                    +- Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L))
                                       +- FileScan parquet default.t2[key#61L,value2#62] Batched: true, DataFilters: [isnotnull(value2#62), (value2#62 > 0.01), isnotnull(key#61L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(value2), GreaterThan(value2,0.01), IsNotNull(key)], ReadSchema: struct<key:bigint,value2:double>
   :           +- *(1) ColumnarToRow
   :              +- FileScan parquet default.t1[key#59L,value1#60] Batched: true, DataFilters: [isnotnull(key#59L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(key)], ReadSchema: struct<key:bigint,value1:double>
   +- *(4) Sort [key#61L ASC NULLS FIRST], false, 0
      +- ShuffleQueryStage 1
         +- Exchange hashpartitioning(key#61L, 3), ENSURE_REQUIREMENTS, [plan_id=117]
            +- *(2) Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L))
               +- *(2) ColumnarToRow
                  +- FileScan parquet default.t2[key#61L,value2#62] Batched: true, DataFilters: [isnotnull(value2#62), (value2#62 > 0.01), isnotnull(key#61L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(value2), GreaterThan(value2,0.01), IsNotNull(key)], ReadSchema: struct<key:bigint,value2:double>
】

21:16:27.530 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec - withFinalPlanUpdate.doExecute ==> after get final plan:
*(5) Project [key#59L, value1#60, value2#62]
+- *(5) SortMergeJoin [key#59L], [key#61L], Inner
   :- *(3) Sort [key#59L ASC NULLS FIRST], false, 0
   :  +- ShuffleQueryStage 0
   :     +- Exchange hashpartitioning(key#59L, 3), ENSURE_REQUIREMENTS, [plan_id=95]
   :        +- *(1) Filter (isnotnull(key#59L) AND might_contain(Subquery subquery#73, [id=#57], xxhash64(key#59L, 42)))
   :           :  +- Subquery subquery#73, [id=#57]
   :           :     +- AdaptiveSparkPlan isFinalPlan=true
                        +- == Final Plan ==
                           ObjectHashAggregate(keys=[], functions=[bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0)], output=[bloomFilter#72])
                           +- ShuffleQueryStage 0
                              +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=141]
                                 +- ObjectHashAggregate(keys=[], functions=[partial_bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0)], output=[buf#75])
                                    +- *(1) Project [key#61L]
                                       +- *(1) Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L))
                                          +- *(1) ColumnarToRow
                                             +- FileScan parquet default.t2[key#61L,value2#62] Batched: true, DataFilters: [isnotnull(value2#62), (value2#62 > 0.01), isnotnull(key#61L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(value2), GreaterThan(value2,0.01), IsNotNull(key)], ReadSchema: struct<key:bigint,value2:double>
                        +- == Initial Plan ==
                           ObjectHashAggregate(keys=[], functions=[bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0)], output=[bloomFilter#72])
                           +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=55]
                              +- ObjectHashAggregate(keys=[], functions=[partial_bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0)], output=[buf#75])
                                 +- Project [key#61L]
                                    +- Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L))
                                       +- FileScan parquet default.t2[key#61L,value2#62] Batched: true, DataFilters: [isnotnull(value2#62), (value2#62 > 0.01), isnotnull(key#61L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(value2), GreaterThan(value2,0.01), IsNotNull(key)], ReadSchema: struct<key:bigint,value2:double>
   :           +- *(1) ColumnarToRow
   :              +- FileScan parquet default.t1[key#59L,value1#60] Batched: true, DataFilters: [isnotnull(key#59L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(key)], ReadSchema: struct<key:bigint,value1:double>
   +- *(4) Sort [key#61L ASC NULLS FIRST], false, 0
      +- ShuffleQueryStage 1
         +- Exchange hashpartitioning(key#61L, 3), ENSURE_REQUIREMENTS, [plan_id=117]
            +- *(2) Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L))
               +- *(2) ColumnarToRow
                  +- FileScan parquet default.t2[key#61L,value2#62] Batched: true, DataFilters: [isnotnull(value2#62), (value2#62 > 0.01), isnotnull(key#61L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(value2), GreaterThan(value2,0.01), IsNotNull(key)], ReadSchema: struct<key:bigint,value2:double>

21:16:27.571 [ScalaTest-run-running-PhysicalPlanSpec] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 29.794137 ms
21:16:27.593 [ScalaTest-run-running-PhysicalPlanSpec] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 16.281912 ms
21:16:27.594 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.util.ClosureCleaner - Cleaning indylambda closure: $anonfun$doExecute$4$adapted
21:16:27.595 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.util.ClosureCleaner -  +++ indylambda closure ($anonfun$doExecute$4$adapted) is now cleaned +++
21:16:27.632 [ScalaTest-run-running-PhysicalPlanSpec] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 12.351437 ms
21:16:27.633 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.util.ClosureCleaner - Cleaning indylambda closure: $anonfun$doExecute$4$adapted
21:16:27.634 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.util.ClosureCleaner -  +++ indylambda closure ($anonfun$doExecute$4$adapted) is now cleaned +++
21:16:27.642 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.util.ClosureCleaner - Cleaning indylambda closure: $anonfun$doExecute$5
21:16:27.643 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.util.ClosureCleaner -  +++ indylambda closure ($anonfun$doExecute$5) is now cleaned +++
21:16:27.647 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.util.ClosureCleaner - Cleaning indylambda closure: $anonfun$doExecute$6$adapted
21:16:27.647 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.util.ClosureCleaner -  +++ indylambda closure ($anonfun$doExecute$6$adapted) is now cleaned +++
21:16:27.657 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec - withFinalPlanUpdate.doExecute was finished.
21:16:27.659 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec - Final plan: *(5) Project [key#59L, value1#60, value2#62]
+- *(5) SortMergeJoin [key#59L], [key#61L], Inner
   :- *(3) Sort [key#59L ASC NULLS FIRST], false, 0
   :  +- ShuffleQueryStage 0
   :     +- Exchange hashpartitioning(key#59L, 3), ENSURE_REQUIREMENTS, [plan_id=95]
   :        +- *(1) Filter (isnotnull(key#59L) AND might_contain(Subquery subquery#73, [id=#57], xxhash64(key#59L, 42)))
   :           :  +- Subquery subquery#73, [id=#57]
   :           :     +- AdaptiveSparkPlan isFinalPlan=true
                        +- == Final Plan ==
                           ObjectHashAggregate(keys=[], functions=[bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0)], output=[bloomFilter#72])
                           +- ShuffleQueryStage 0
                              +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=141]
                                 +- ObjectHashAggregate(keys=[], functions=[partial_bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0)], output=[buf#75])
                                    +- *(1) Project [key#61L]
                                       +- *(1) Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L))
                                          +- *(1) ColumnarToRow
                                             +- FileScan parquet default.t2[key#61L,value2#62] Batched: true, DataFilters: [isnotnull(value2#62), (value2#62 > 0.01), isnotnull(key#61L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(value2), GreaterThan(value2,0.01), IsNotNull(key)], ReadSchema: struct<key:bigint,value2:double>
                        +- == Initial Plan ==
                           ObjectHashAggregate(keys=[], functions=[bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0)], output=[bloomFilter#72])
                           +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=55]
                              +- ObjectHashAggregate(keys=[], functions=[partial_bloom_filter_agg(xxhash64(key#61L, 42), 1000000, 8388608, 0, 0)], output=[buf#75])
                                 +- Project [key#61L]
                                    +- Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L))
                                       +- FileScan parquet default.t2[key#61L,value2#62] Batched: true, DataFilters: [isnotnull(value2#62), (value2#62 > 0.01), isnotnull(key#61L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(value2), GreaterThan(value2,0.01), IsNotNull(key)], ReadSchema: struct<key:bigint,value2:double>
   :           +- *(1) ColumnarToRow
   :              +- FileScan parquet default.t1[key#59L,value1#60] Batched: true, DataFilters: [isnotnull(key#59L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(key)], ReadSchema: struct<key:bigint,value1:double>
   +- *(4) Sort [key#61L ASC NULLS FIRST], false, 0
      +- ShuffleQueryStage 1
         +- Exchange hashpartitioning(key#61L, 3), ENSURE_REQUIREMENTS, [plan_id=117]
            +- *(2) Filter ((isnotnull(value2#62) AND (value2#62 > 0.01)) AND isnotnull(key#61L))
               +- *(2) ColumnarToRow
                  +- FileScan parquet default.t2[key#61L,value2#62] Batched: true, DataFilters: [isnotnull(value2#62), (value2#62 > 0.01), isnotnull(key#61L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/juntao/src/github.com/apache/spark-v3.3.1-study/spark-ware..., PartitionFilters: [], PushedFilters: [IsNotNull(value2), GreaterThan(value2,0.01), IsNotNull(key)], ReadSchema: struct<key:bigint,value2:double>

21:16:27.659 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec - withFinalPlanUpdate.doExecute.finalPlanUpdate was finished.
21:16:27.668 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.util.ClosureCleaner - Cleaning indylambda closure: $anonfun$write$21
21:16:27.670 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.util.ClosureCleaner -  +++ indylambda closure ($anonfun$write$21) is now cleaned +++
21:16:27.706 [ScalaTest-run-running-PhysicalPlanSpec] INFO  org.apache.spark.SparkContext - Starting job: saveAsTable at PhysicalPlanSpec.scala:372
21:16:27.706 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.scheduler.DAGScheduler - eagerlyComputePartitionsForRddAndAncestors for RDD 21 took 0.000152 seconds
21:16:27.706 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - Merging stage rdd profiles: Set()
21:16:27.706 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - Merging stage rdd profiles: Set()
21:16:27.707 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - Merging stage rdd profiles: Set()
21:16:27.707 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 4 (saveAsTable at PhysicalPlanSpec.scala:372) with 3 output partitions
21:16:27.707 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 7 (saveAsTable at PhysicalPlanSpec.scala:372)
21:16:27.707 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 5, ShuffleMapStage 6)
21:16:27.708 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
21:16:27.708 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - submitStage(ResultStage 7 (name=saveAsTable at PhysicalPlanSpec.scala:372;jobs=4))
21:16:27.709 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - missing: List()
21:16:27.709 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 7 (MapPartitionsRDD[21] at saveAsTable at PhysicalPlanSpec.scala:372), which has no missing parents
21:16:27.709 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - submitMissingTasks(ResultStage 7)
21:16:27.748 [dag-scheduler-event-loop] WARN  org.apache.spark.scheduler.DAGScheduler - Broadcasting large task binary with size 1274.3 KiB
21:16:27.748 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_7 stored as values in memory (estimated size 1274.3 KiB, free 4.1 GiB)
21:16:27.749 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager - Put block broadcast_7 locally took 0 ms
21:16:27.749 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager - Putting block broadcast_7 without replication took 0 ms
21:16:27.751 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_7_piece0 stored as bytes in memory (estimated size 128.4 KiB, free 4.1 GiB)
21:16:27.751 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_7_piece0 for BlockManagerId(driver, 192.168.31.69, 58425, None)
21:16:27.752 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_7_piece0 in memory on 192.168.31.69:58425 (size: 128.4 KiB, free: 4.1 GiB)
21:16:27.752 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_7_piece0
21:16:27.752 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager - Told master about block broadcast_7_piece0
21:16:27.752 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager - Put block broadcast_7_piece0 locally took 1 ms
21:16:27.752 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager - Putting block broadcast_7_piece0 without replication took 1 ms
21:16:27.752 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 7 from broadcast at DAGScheduler.scala:1513
21:16:27.752 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 3 missing tasks from ResultStage 7 (MapPartitionsRDD[21] at saveAsTable at PhysicalPlanSpec.scala:372) (first 15 tasks are for partitions Vector(0, 1, 2))
21:16:27.752 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 7.0 with 3 tasks resource profile 0
21:16:27.753 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.TaskSetManager - Epoch for TaskSet 7.0: 3
21:16:27.753 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.TaskSetManager - Adding pending tasks took 0 ms
21:16:27.753 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.TaskSetManager - Valid locality levels for TaskSet 7.0: NODE_LOCAL, ANY
21:16:27.753 [dispatcher-event-loop-0] DEBUG org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_7.0, runningTasks: 0
21:16:27.755 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 7.0 (TID 4) (192.168.31.69, executor driver, partition 0, NODE_LOCAL, 4735 bytes) taskResourceAssignments Map()
21:16:27.755 [Executor task launch worker for task 0.0 in stage 7.0 (TID 4)] INFO  org.apache.spark.executor.Executor - Running task 0.0 in stage 7.0 (TID 4)
21:16:27.756 [Executor task launch worker for task 0.0 in stage 7.0 (TID 4)] DEBUG org.apache.spark.executor.ExecutorMetricsPoller - stageTCMP: (7, 0) -> 1
21:16:27.756 [Executor task launch worker for task 0.0 in stage 7.0 (TID 4)] DEBUG org.apache.spark.storage.BlockManager - Getting local block broadcast_7
21:16:27.756 [Executor task launch worker for task 0.0 in stage 7.0 (TID 4)] DEBUG org.apache.spark.storage.BlockManager - Level for block broadcast_7 is StorageLevel(disk, memory, deserialized, 1 replicas)
21:16:27.803 [Executor task launch worker for task 0.0 in stage 7.0 (TID 4)] DEBUG org.apache.spark.MapOutputTrackerMaster - Fetching outputs for shuffle 1
21:16:27.803 [Executor task launch worker for task 0.0 in stage 7.0 (TID 4)] DEBUG org.apache.spark.MapOutputTrackerMaster - Convert map statuses for shuffle 1, mappers 0-1, partitions 0-1
21:16:27.803 [Executor task launch worker for task 0.0 in stage 7.0 (TID 4)] DEBUG org.apache.spark.storage.ShuffleBlockFetcherIterator - maxBytesInFlight: 50331648, targetRemoteRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
21:16:27.804 [Executor task launch worker for task 0.0 in stage 7.0 (TID 4)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 (4.7 KiB) non-empty blocks including 1 (4.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
21:16:27.804 [Executor task launch worker for task 0.0 in stage 7.0 (TID 4)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
21:16:27.804 [Executor task launch worker for task 0.0 in stage 7.0 (TID 4)] DEBUG org.apache.spark.storage.ShuffleBlockFetcherIterator - Start fetching local blocks: (shuffle_1_2_0,0)
21:16:27.804 [Executor task launch worker for task 0.0 in stage 7.0 (TID 4)] DEBUG org.apache.spark.storage.BlockManager - Getting local shuffle block shuffle_1_2_0
21:16:27.804 [Executor task launch worker for task 0.0 in stage 7.0 (TID 4)] DEBUG org.apache.spark.storage.ShuffleBlockFetcherIterator - Got local blocks in 0 ms
21:16:27.836 [Executor task launch worker for task 0.0 in stage 7.0 (TID 4)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 23.146863 ms
21:16:27.864 [Executor task launch worker for task 0.0 in stage 7.0 (TID 4)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 8.28232 ms
21:16:27.880 [Executor task launch worker for task 0.0 in stage 7.0 (TID 4)] DEBUG org.apache.spark.memory.TaskMemoryManager - Task 4 acquired 64.0 KiB for org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@4bace373
21:16:27.883 [Executor task launch worker for task 0.0 in stage 7.0 (TID 4)] DEBUG org.apache.spark.MapOutputTrackerMaster - Fetching outputs for shuffle 2
21:16:27.883 [Executor task launch worker for task 0.0 in stage 7.0 (TID 4)] DEBUG org.apache.spark.MapOutputTrackerMaster - Convert map statuses for shuffle 2, mappers 0-1, partitions 0-1
21:16:27.884 [Executor task launch worker for task 0.0 in stage 7.0 (TID 4)] DEBUG org.apache.spark.storage.ShuffleBlockFetcherIterator - maxBytesInFlight: 50331648, targetRemoteRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
21:16:27.885 [Executor task launch worker for task 0.0 in stage 7.0 (TID 4)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 (4.7 KiB) non-empty blocks including 1 (4.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
21:16:27.885 [Executor task launch worker for task 0.0 in stage 7.0 (TID 4)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
21:16:27.885 [Executor task launch worker for task 0.0 in stage 7.0 (TID 4)] DEBUG org.apache.spark.storage.ShuffleBlockFetcherIterator - Start fetching local blocks: (shuffle_2_3_0,0)
21:16:27.885 [Executor task launch worker for task 0.0 in stage 7.0 (TID 4)] DEBUG org.apache.spark.storage.BlockManager - Getting local shuffle block shuffle_2_3_0
21:16:27.885 [Executor task launch worker for task 0.0 in stage 7.0 (TID 4)] DEBUG org.apache.spark.storage.ShuffleBlockFetcherIterator - Got local blocks in 1 ms
21:16:27.891 [Executor task launch worker for task 0.0 in stage 7.0 (TID 4)] DEBUG org.apache.spark.memory.TaskMemoryManager - Task 4 acquired 64.0 KiB for org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@c0fa0a
21:16:27.898 [Executor task launch worker for task 0.0 in stage 7.0 (TID 4)] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21:16:27.898 [Executor task launch worker for task 0.0 in stage 7.0 (TID 4)] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21:16:27.941 [Executor task launch worker for task 0.0 in stage 7.0 (TID 4)] INFO  org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "key",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "value1",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "value2",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int64 key;
  optional double value1;
  optional double value2;
}


21:16:28.032 [Executor task launch worker for task 0.0 in stage 7.0 (TID 4)] DEBUG org.apache.spark.memory.TaskMemoryManager - Task 4 acquired 64.0 MiB for org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@4bace373
21:16:28.053 [Executor task launch worker for task 0.0 in stage 7.0 (TID 4)] DEBUG org.apache.spark.memory.TaskMemoryManager - Task 4 acquired 64.0 MiB for org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@c0fa0a
21:16:28.082 [Executor task launch worker for task 0.0 in stage 7.0 (TID 4)] DEBUG org.apache.spark.memory.TaskMemoryManager - Task 4 release 64.0 MiB from org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@c0fa0a
21:16:28.083 [Executor task launch worker for task 0.0 in stage 7.0 (TID 4)] DEBUG org.apache.spark.memory.TaskMemoryManager - Task 4 release 64.0 KiB from org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@c0fa0a
21:16:28.083 [Executor task launch worker for task 0.0 in stage 7.0 (TID 4)] DEBUG org.apache.spark.memory.TaskMemoryManager - Task 4 release 64.0 MiB from org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@4bace373
21:16:28.083 [Executor task launch worker for task 0.0 in stage 7.0 (TID 4)] DEBUG org.apache.spark.memory.TaskMemoryManager - Task 4 release 64.0 KiB from org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@4bace373
21:16:28.134 [dispatcher-event-loop-0] DEBUG org.apache.spark.scheduler.OutputCommitCoordinator - Commit allowed for stage=7.0, partition=0, task attempt 0
21:16:28.137 [Executor task launch worker for task 0.0 in stage 7.0 (TID 4)] INFO  org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_202512182116275020037948209086300_0007_m_000000_4: Committed. Elapsed time: 1 ms.
21:16:28.145 [Executor task launch worker for task 0.0 in stage 7.0 (TID 4)] INFO  org.apache.spark.executor.Executor - Finished task 0.0 in stage 7.0 (TID 4). 8189 bytes result sent to driver
21:16:28.146 [Executor task launch worker for task 0.0 in stage 7.0 (TID 4)] DEBUG org.apache.spark.executor.ExecutorMetricsPoller - stageTCMP: (7, 0) -> 0
21:16:28.146 [dispatcher-event-loop-1] DEBUG org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_7.0, runningTasks: 0
21:16:28.146 [dispatcher-event-loop-1] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 7.0 (TID 5) (192.168.31.69, executor driver, partition 1, NODE_LOCAL, 4735 bytes) taskResourceAssignments Map()
21:16:28.146 [Executor task launch worker for task 1.0 in stage 7.0 (TID 5)] INFO  org.apache.spark.executor.Executor - Running task 1.0 in stage 7.0 (TID 5)
21:16:28.147 [Executor task launch worker for task 1.0 in stage 7.0 (TID 5)] DEBUG org.apache.spark.executor.ExecutorMetricsPoller - stageTCMP: (7, 0) -> 1
21:16:28.147 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 7.0 (TID 4) in 394 ms on 192.168.31.69 (executor driver) (1/3)
21:16:28.149 [dag-scheduler-event-loop] DEBUG org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - onTaskCommit(org.apache.spark.internal.io.FileCommitProtocol$TaskCommitMessage@552f487b)
21:16:28.167 [Executor task launch worker for task 1.0 in stage 7.0 (TID 5)] DEBUG org.apache.spark.MapOutputTrackerMaster - Fetching outputs for shuffle 1
21:16:28.167 [Executor task launch worker for task 1.0 in stage 7.0 (TID 5)] DEBUG org.apache.spark.MapOutputTrackerMaster - Convert map statuses for shuffle 1, mappers 0-1, partitions 1-2
21:16:28.168 [Executor task launch worker for task 1.0 in stage 7.0 (TID 5)] DEBUG org.apache.spark.storage.ShuffleBlockFetcherIterator - maxBytesInFlight: 50331648, targetRemoteRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
21:16:28.168 [Executor task launch worker for task 1.0 in stage 7.0 (TID 5)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 (4.7 KiB) non-empty blocks including 1 (4.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
21:16:28.168 [Executor task launch worker for task 1.0 in stage 7.0 (TID 5)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
21:16:28.168 [Executor task launch worker for task 1.0 in stage 7.0 (TID 5)] DEBUG org.apache.spark.storage.ShuffleBlockFetcherIterator - Start fetching local blocks: (shuffle_1_2_1,0)
21:16:28.168 [Executor task launch worker for task 1.0 in stage 7.0 (TID 5)] DEBUG org.apache.spark.storage.BlockManager - Getting local shuffle block shuffle_1_2_1
21:16:28.169 [Executor task launch worker for task 1.0 in stage 7.0 (TID 5)] DEBUG org.apache.spark.storage.ShuffleBlockFetcherIterator - Got local blocks in 1 ms
21:16:28.171 [Executor task launch worker for task 1.0 in stage 7.0 (TID 5)] DEBUG org.apache.spark.memory.TaskMemoryManager - Task 5 acquired 64.0 KiB for org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@6ffc0a95
21:16:28.171 [Executor task launch worker for task 1.0 in stage 7.0 (TID 5)] DEBUG org.apache.spark.MapOutputTrackerMaster - Fetching outputs for shuffle 2
21:16:28.171 [Executor task launch worker for task 1.0 in stage 7.0 (TID 5)] DEBUG org.apache.spark.MapOutputTrackerMaster - Convert map statuses for shuffle 2, mappers 0-1, partitions 1-2
21:16:28.172 [Executor task launch worker for task 1.0 in stage 7.0 (TID 5)] DEBUG org.apache.spark.storage.ShuffleBlockFetcherIterator - maxBytesInFlight: 50331648, targetRemoteRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
21:16:28.172 [Executor task launch worker for task 1.0 in stage 7.0 (TID 5)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 (4.7 KiB) non-empty blocks including 1 (4.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
21:16:28.172 [Executor task launch worker for task 1.0 in stage 7.0 (TID 5)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
21:16:28.172 [Executor task launch worker for task 1.0 in stage 7.0 (TID 5)] DEBUG org.apache.spark.storage.ShuffleBlockFetcherIterator - Start fetching local blocks: (shuffle_2_3_1,0)
21:16:28.172 [Executor task launch worker for task 1.0 in stage 7.0 (TID 5)] DEBUG org.apache.spark.storage.BlockManager - Getting local shuffle block shuffle_2_3_1
21:16:28.172 [Executor task launch worker for task 1.0 in stage 7.0 (TID 5)] DEBUG org.apache.spark.storage.ShuffleBlockFetcherIterator - Got local blocks in 0 ms
21:16:28.176 [Executor task launch worker for task 1.0 in stage 7.0 (TID 5)] DEBUG org.apache.spark.memory.TaskMemoryManager - Task 5 acquired 64.0 KiB for org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@497a4db5
21:16:28.177 [Executor task launch worker for task 1.0 in stage 7.0 (TID 5)] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21:16:28.177 [Executor task launch worker for task 1.0 in stage 7.0 (TID 5)] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21:16:28.178 [Executor task launch worker for task 1.0 in stage 7.0 (TID 5)] DEBUG org.apache.spark.memory.TaskMemoryManager - Task 5 acquired 64.0 MiB for org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@6ffc0a95
21:16:28.179 [Executor task launch worker for task 1.0 in stage 7.0 (TID 5)] DEBUG org.apache.spark.memory.TaskMemoryManager - Task 5 acquired 64.0 MiB for org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@497a4db5
21:16:28.182 [Executor task launch worker for task 1.0 in stage 7.0 (TID 5)] INFO  org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "key",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "value1",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "value2",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int64 key;
  optional double value1;
  optional double value2;
}


21:16:28.218 [Executor task launch worker for task 1.0 in stage 7.0 (TID 5)] DEBUG org.apache.spark.memory.TaskMemoryManager - Task 5 release 64.0 MiB from org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@497a4db5
21:16:28.219 [Executor task launch worker for task 1.0 in stage 7.0 (TID 5)] DEBUG org.apache.spark.memory.TaskMemoryManager - Task 5 release 64.0 KiB from org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@497a4db5
21:16:28.219 [Executor task launch worker for task 1.0 in stage 7.0 (TID 5)] DEBUG org.apache.spark.memory.TaskMemoryManager - Task 5 release 64.0 MiB from org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@6ffc0a95
21:16:28.219 [Executor task launch worker for task 1.0 in stage 7.0 (TID 5)] DEBUG org.apache.spark.memory.TaskMemoryManager - Task 5 release 64.0 KiB from org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@6ffc0a95
21:16:28.222 [dispatcher-event-loop-1] DEBUG org.apache.spark.scheduler.OutputCommitCoordinator - Commit allowed for stage=7.0, partition=1, task attempt 0
21:16:28.223 [Executor task launch worker for task 1.0 in stage 7.0 (TID 5)] INFO  org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_202512182116279102424829962825832_0007_m_000001_5: Committed. Elapsed time: 0 ms.
21:16:28.224 [Executor task launch worker for task 1.0 in stage 7.0 (TID 5)] INFO  org.apache.spark.executor.Executor - Finished task 1.0 in stage 7.0 (TID 5). 8146 bytes result sent to driver
21:16:28.224 [Executor task launch worker for task 1.0 in stage 7.0 (TID 5)] DEBUG org.apache.spark.executor.ExecutorMetricsPoller - stageTCMP: (7, 0) -> 0
21:16:28.224 [dispatcher-event-loop-0] DEBUG org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_7.0, runningTasks: 0
21:16:28.225 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 7.0 (TID 6) (192.168.31.69, executor driver, partition 2, NODE_LOCAL, 4735 bytes) taskResourceAssignments Map()
21:16:28.225 [Executor task launch worker for task 2.0 in stage 7.0 (TID 6)] INFO  org.apache.spark.executor.Executor - Running task 2.0 in stage 7.0 (TID 6)
21:16:28.225 [Executor task launch worker for task 2.0 in stage 7.0 (TID 6)] DEBUG org.apache.spark.executor.ExecutorMetricsPoller - stageTCMP: (7, 0) -> 1
21:16:28.225 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 7.0 (TID 5) in 79 ms on 192.168.31.69 (executor driver) (2/3)
21:16:28.226 [dag-scheduler-event-loop] DEBUG org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - onTaskCommit(org.apache.spark.internal.io.FileCommitProtocol$TaskCommitMessage@3e0c2d7b)
21:16:28.240 [Executor task launch worker for task 2.0 in stage 7.0 (TID 6)] DEBUG org.apache.spark.MapOutputTrackerMaster - Fetching outputs for shuffle 1
21:16:28.240 [Executor task launch worker for task 2.0 in stage 7.0 (TID 6)] DEBUG org.apache.spark.MapOutputTrackerMaster - Convert map statuses for shuffle 1, mappers 0-1, partitions 2-3
21:16:28.240 [Executor task launch worker for task 2.0 in stage 7.0 (TID 6)] DEBUG org.apache.spark.storage.ShuffleBlockFetcherIterator - maxBytesInFlight: 50331648, targetRemoteRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
21:16:28.240 [Executor task launch worker for task 2.0 in stage 7.0 (TID 6)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 (4.3 KiB) non-empty blocks including 1 (4.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
21:16:28.240 [Executor task launch worker for task 2.0 in stage 7.0 (TID 6)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
21:16:28.241 [Executor task launch worker for task 2.0 in stage 7.0 (TID 6)] DEBUG org.apache.spark.storage.ShuffleBlockFetcherIterator - Start fetching local blocks: (shuffle_1_2_2,0)
21:16:28.241 [Executor task launch worker for task 2.0 in stage 7.0 (TID 6)] DEBUG org.apache.spark.storage.BlockManager - Getting local shuffle block shuffle_1_2_2
21:16:28.241 [Executor task launch worker for task 2.0 in stage 7.0 (TID 6)] DEBUG org.apache.spark.storage.ShuffleBlockFetcherIterator - Got local blocks in 1 ms
21:16:28.244 [Executor task launch worker for task 2.0 in stage 7.0 (TID 6)] DEBUG org.apache.spark.memory.TaskMemoryManager - Task 6 acquired 64.0 KiB for org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@714f06ae
21:16:28.244 [Executor task launch worker for task 2.0 in stage 7.0 (TID 6)] DEBUG org.apache.spark.MapOutputTrackerMaster - Fetching outputs for shuffle 2
21:16:28.244 [Executor task launch worker for task 2.0 in stage 7.0 (TID 6)] DEBUG org.apache.spark.MapOutputTrackerMaster - Convert map statuses for shuffle 2, mappers 0-1, partitions 2-3
21:16:28.244 [Executor task launch worker for task 2.0 in stage 7.0 (TID 6)] DEBUG org.apache.spark.storage.ShuffleBlockFetcherIterator - maxBytesInFlight: 50331648, targetRemoteRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
21:16:28.245 [Executor task launch worker for task 2.0 in stage 7.0 (TID 6)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 (4.7 KiB) non-empty blocks including 1 (4.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
21:16:28.245 [Executor task launch worker for task 2.0 in stage 7.0 (TID 6)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
21:16:28.245 [Executor task launch worker for task 2.0 in stage 7.0 (TID 6)] DEBUG org.apache.spark.storage.ShuffleBlockFetcherIterator - Start fetching local blocks: (shuffle_2_3_2,0)
21:16:28.245 [Executor task launch worker for task 2.0 in stage 7.0 (TID 6)] DEBUG org.apache.spark.storage.BlockManager - Getting local shuffle block shuffle_2_3_2
21:16:28.245 [Executor task launch worker for task 2.0 in stage 7.0 (TID 6)] DEBUG org.apache.spark.storage.ShuffleBlockFetcherIterator - Got local blocks in 0 ms
21:16:28.248 [Executor task launch worker for task 2.0 in stage 7.0 (TID 6)] DEBUG org.apache.spark.memory.TaskMemoryManager - Task 6 acquired 64.0 KiB for org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@183cf901
21:16:28.250 [Executor task launch worker for task 2.0 in stage 7.0 (TID 6)] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21:16:28.251 [Executor task launch worker for task 2.0 in stage 7.0 (TID 6)] INFO  org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
21:16:28.252 [Executor task launch worker for task 2.0 in stage 7.0 (TID 6)] DEBUG org.apache.spark.memory.TaskMemoryManager - Task 6 acquired 64.0 MiB for org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@714f06ae
21:16:28.254 [Executor task launch worker for task 2.0 in stage 7.0 (TID 6)] DEBUG org.apache.spark.memory.TaskMemoryManager - Task 6 acquired 64.0 MiB for org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@183cf901
21:16:28.258 [Executor task launch worker for task 2.0 in stage 7.0 (TID 6)] INFO  org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "key",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "value1",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "value2",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int64 key;
  optional double value1;
  optional double value2;
}


21:16:28.293 [Executor task launch worker for task 2.0 in stage 7.0 (TID 6)] DEBUG org.apache.spark.memory.TaskMemoryManager - Task 6 release 64.0 MiB from org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@183cf901
21:16:28.293 [Executor task launch worker for task 2.0 in stage 7.0 (TID 6)] DEBUG org.apache.spark.memory.TaskMemoryManager - Task 6 release 64.0 KiB from org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@183cf901
21:16:28.293 [Executor task launch worker for task 2.0 in stage 7.0 (TID 6)] DEBUG org.apache.spark.memory.TaskMemoryManager - Task 6 release 64.0 MiB from org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@714f06ae
21:16:28.293 [Executor task launch worker for task 2.0 in stage 7.0 (TID 6)] DEBUG org.apache.spark.memory.TaskMemoryManager - Task 6 release 64.0 KiB from org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@714f06ae
21:16:28.296 [dispatcher-event-loop-0] DEBUG org.apache.spark.scheduler.OutputCommitCoordinator - Commit allowed for stage=7.0, partition=2, task attempt 0
21:16:28.297 [Executor task launch worker for task 2.0 in stage 7.0 (TID 6)] INFO  org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_202512182116278883709035151187827_0007_m_000002_6: Committed. Elapsed time: 0 ms.
21:16:28.298 [Executor task launch worker for task 2.0 in stage 7.0 (TID 6)] INFO  org.apache.spark.executor.Executor - Finished task 2.0 in stage 7.0 (TID 6). 8146 bytes result sent to driver
21:16:28.298 [Executor task launch worker for task 2.0 in stage 7.0 (TID 6)] DEBUG org.apache.spark.executor.ExecutorMetricsPoller - stageTCMP: (7, 0) -> 0
21:16:28.298 [dispatcher-event-loop-1] DEBUG org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_7.0, runningTasks: 0
21:16:28.299 [dispatcher-event-loop-1] DEBUG org.apache.spark.scheduler.TaskSetManager - No tasks for locality level NODE_LOCAL, so moving to locality level ANY
21:16:28.299 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 7.0 (TID 6) in 75 ms on 192.168.31.69 (executor driver) (3/3)
21:16:28.299 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 7.0, whose tasks have all completed, from pool
21:16:28.300 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 7 (saveAsTable at PhysicalPlanSpec.scala:372) finished in 0.589 s
21:16:28.301 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - After removal of stage 5, remaining stages = 2
21:16:28.301 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - After removal of stage 7, remaining stages = 1
21:16:28.301 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - After removal of stage 6, remaining stages = 0
21:16:28.301 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
21:16:28.301 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 7: Stage finished
21:16:28.301 [dag-scheduler-event-loop] DEBUG org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - onTaskCommit(org.apache.spark.internal.io.FileCommitProtocol$TaskCommitMessage@706add55)
21:16:28.301 [ScalaTest-run-running-PhysicalPlanSpec] INFO  org.apache.spark.scheduler.DAGScheduler - Job 4 finished: saveAsTable at PhysicalPlanSpec.scala:372, took 0.595372 s
21:16:28.304 [ScalaTest-run-running-PhysicalPlanSpec] INFO  org.apache.spark.sql.execution.datasources.FileFormatWriter - Start to commit write Job 58c200c4-0637-49bd-9cc2-47021bf22b99.
21:16:28.332 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Committing files staged for absolute locations Map()
21:16:28.333 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Create absolute parent directories: Set()
21:16:28.335 [ScalaTest-run-running-PhysicalPlanSpec] INFO  org.apache.spark.sql.execution.datasources.FileFormatWriter - Write Job 58c200c4-0637-49bd-9cc2-47021bf22b99 committed. Elapsed time: 30 ms.
21:16:28.341 [ScalaTest-run-running-PhysicalPlanSpec] INFO  org.apache.spark.sql.execution.datasources.FileFormatWriter - Finished processing stats for write job 58c200c4-0637-49bd-9cc2-47021bf22b99.
21:16:28.353 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.execution.datasources.DataSource - Some paths were ignored:

21:16:28.355 [ScalaTest-run-running-PhysicalPlanSpec] INFO  org.apache.spark.sql.execution.datasources.InMemoryFileIndex - It took 2 ms to list leaf files for 1 paths.
21:16:28.379 [ScalaTest-run-running-PhysicalPlanSpec] INFO  org.apache.spark.sql.hive.HiveExternalCatalog - Persisting file based data source table `default`.`t3` into Hive metastore in Hive compatible format.
21:16:28.386 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
org.apache.spark.sql.catalyst.parser.SqlBaseParser$SingleDataTypeContext =>  bigint <EOF>
org.apache.spark.sql.catalyst.parser.SqlBaseParser$PrimitiveDataTypeContext =>  bigint
21:16:28.387 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: double
org.apache.spark.sql.catalyst.parser.SqlBaseParser$SingleDataTypeContext =>  double <EOF>
org.apache.spark.sql.catalyst.parser.SqlBaseParser$PrimitiveDataTypeContext =>  double
21:16:28.387 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: double
org.apache.spark.sql.catalyst.parser.SqlBaseParser$SingleDataTypeContext =>  double <EOF>
org.apache.spark.sql.catalyst.parser.SqlBaseParser$PrimitiveDataTypeContext =>  double
21:16:28.465 [ScalaTest-run-running-PhysicalPlanSpec] WARN  org.apache.hadoop.hive.ql.session.SessionState - METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.
21:16:28.466 [ScalaTest-run-running-PhysicalPlanSpec] INFO  hive.metastore - Mestastore configuration hive.metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
21:16:28.466 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG hive.ql.metadata.Hive - Creating new db. db = org.apache.hadoop.hive.ql.metadata.Hive@5a362dc7, needsRefresh = false, db.isCurrentUserOwner = true
21:16:28.466 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG hive.ql.metadata.Hive - Closing current thread's connection to Hive Metastore.
21:16:28.469 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG hive.ql.metadata.Hive - Closing current thread's connection to Hive Metastore.
21:16:28.506 [ScalaTest-run-running-PhysicalPlanSpec] DEBUG hive.log - DDL: struct t3 { i64 key, double value1, double value2}
21:16:28.598 [ScalaTest-run-running-PhysicalPlanSpec] WARN  org.apache.hadoop.hive.conf.HiveConf - HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist
21:16:28.598 [ScalaTest-run-running-PhysicalPlanSpec] WARN  org.apache.hadoop.hive.conf.HiveConf - HiveConf of name hive.stats.jdbc.timeout does not exist
21:16:28.598 [ScalaTest-run-running-PhysicalPlanSpec] WARN  org.apache.hadoop.hive.conf.HiveConf - HiveConf of name hive.stats.retries.wait does not exist
21:16:28.608 [ScalaTest-run-running-PhysicalPlanSpec] INFO  hive.log - Updating table stats fast for t3
21:16:28.608 [ScalaTest-run-running-PhysicalPlanSpec] INFO  hive.log - Updated size of table t3 to 23156
21:16:28.773 [Spark Context Cleaner] DEBUG org.apache.spark.broadcast.TorrentBroadcast - Unpersisting TorrentBroadcast 7
21:16:28.773 [block-manager-storage-async-thread-pool-3] DEBUG org.apache.spark.storage.BlockManagerStorageEndpoint - removing broadcast 7
21:16:28.773 [block-manager-storage-async-thread-pool-3] DEBUG org.apache.spark.storage.BlockManager - Removing broadcast 7
21:16:28.774 [block-manager-storage-async-thread-pool-3] DEBUG org.apache.spark.storage.BlockManager - Removing block broadcast_7_piece0
21:16:28.774 [block-manager-storage-async-thread-pool-3] DEBUG org.apache.spark.storage.memory.MemoryStore - Block broadcast_7_piece0 of size 131474 dropped from memory (free 4388999023)
21:16:28.774 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_7_piece0 for BlockManagerId(driver, 192.168.31.69, 58425, None)
21:16:28.774 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_7_piece0 on 192.168.31.69:58425 in memory (size: 128.4 KiB, free: 4.1 GiB)
21:16:28.774 [block-manager-storage-async-thread-pool-3] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_7_piece0
21:16:28.774 [block-manager-storage-async-thread-pool-3] DEBUG org.apache.spark.storage.BlockManager - Told master about block broadcast_7_piece0
21:16:28.774 [block-manager-storage-async-thread-pool-3] DEBUG org.apache.spark.storage.BlockManager - Removing block broadcast_7
21:16:28.775 [block-manager-storage-async-thread-pool-3] DEBUG org.apache.spark.storage.memory.MemoryStore - Block broadcast_7 of size 1304920 dropped from memory (free 4390303943)
21:16:28.775 [block-manager-storage-async-thread-pool-5] DEBUG org.apache.spark.storage.BlockManagerStorageEndpoint - Done removing broadcast 7, response is 0
21:16:28.776 [block-manager-storage-async-thread-pool-5] DEBUG org.apache.spark.storage.BlockManagerStorageEndpoint - Sent response: 0 to 192.168.31.69:58424
21:16:28.776 [Spark Context Cleaner] DEBUG org.apache.spark.broadcast.TorrentBroadcast - Unpersisting TorrentBroadcast 4
21:16:28.777 [block-manager-storage-async-thread-pool-6] DEBUG org.apache.spark.storage.BlockManagerStorageEndpoint - removing broadcast 4
21:16:28.777 [block-manager-storage-async-thread-pool-6] DEBUG org.apache.spark.storage.BlockManager - Removing broadcast 4
21:16:28.777 [block-manager-storage-async-thread-pool-6] DEBUG org.apache.spark.storage.BlockManager - Removing block broadcast_4_piece0
21:16:28.777 [block-manager-storage-async-thread-pool-6] DEBUG org.apache.spark.storage.memory.MemoryStore - Block broadcast_4_piece0 of size 50001 dropped from memory (free 4390353944)
21:16:28.778 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_4_piece0 for BlockManagerId(driver, 192.168.31.69, 58425, None)
21:16:28.778 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_4_piece0 on 192.168.31.69:58425 in memory (size: 48.8 KiB, free: 4.1 GiB)
21:16:28.778 [block-manager-storage-async-thread-pool-6] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_4_piece0
21:16:28.778 [block-manager-storage-async-thread-pool-6] DEBUG org.apache.spark.storage.BlockManager - Told master about block broadcast_4_piece0
21:16:28.778 [block-manager-storage-async-thread-pool-6] DEBUG org.apache.spark.storage.BlockManager - Removing block broadcast_4
21:16:28.778 [block-manager-storage-async-thread-pool-6] DEBUG org.apache.spark.storage.memory.MemoryStore - Block broadcast_4 of size 1071112 dropped from memory (free 4391425056)
21:16:28.778 [block-manager-storage-async-thread-pool-8] DEBUG org.apache.spark.storage.BlockManagerStorageEndpoint - Done removing broadcast 4, response is 0
21:16:28.778 [block-manager-storage-async-thread-pool-8] DEBUG org.apache.spark.storage.BlockManagerStorageEndpoint - Sent response: 0 to 192.168.31.69:58424
21:16:28.779 [Spark Context Cleaner] DEBUG org.apache.spark.broadcast.TorrentBroadcast - Unpersisting TorrentBroadcast 6
21:16:28.779 [block-manager-storage-async-thread-pool-9] DEBUG org.apache.spark.storage.BlockManagerStorageEndpoint - removing broadcast 6
21:16:28.779 [block-manager-storage-async-thread-pool-9] DEBUG org.apache.spark.storage.BlockManager - Removing broadcast 6
21:16:28.779 [block-manager-storage-async-thread-pool-9] DEBUG org.apache.spark.storage.BlockManager - Removing block broadcast_6
21:16:28.779 [block-manager-storage-async-thread-pool-9] DEBUG org.apache.spark.storage.memory.MemoryStore - Block broadcast_6 of size 17128 dropped from memory (free 4391442184)
21:16:28.780 [block-manager-storage-async-thread-pool-9] DEBUG org.apache.spark.storage.BlockManager - Removing block broadcast_6_piece0
21:16:28.780 [block-manager-storage-async-thread-pool-9] DEBUG org.apache.spark.storage.memory.MemoryStore - Block broadcast_6_piece0 of size 7893 dropped from memory (free 4391450077)
21:16:28.780 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_6_piece0 for BlockManagerId(driver, 192.168.31.69, 58425, None)
21:16:28.780 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_6_piece0 on 192.168.31.69:58425 in memory (size: 7.7 KiB, free: 4.1 GiB)
21:16:28.780 [block-manager-storage-async-thread-pool-9] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_6_piece0
21:16:28.780 [block-manager-storage-async-thread-pool-9] DEBUG org.apache.spark.storage.BlockManager - Told master about block broadcast_6_piece0
21:16:28.781 [block-manager-storage-async-thread-pool-11] DEBUG org.apache.spark.storage.BlockManagerStorageEndpoint - Done removing broadcast 6, response is 0
21:16:28.781 [block-manager-storage-async-thread-pool-11] DEBUG org.apache.spark.storage.BlockManagerStorageEndpoint - Sent response: 0 to 192.168.31.69:58424
21:16:28.782 [Spark Context Cleaner] DEBUG org.apache.spark.broadcast.TorrentBroadcast - Unpersisting TorrentBroadcast 2
21:16:28.783 [block-manager-storage-async-thread-pool-12] DEBUG org.apache.spark.storage.BlockManagerStorageEndpoint - removing broadcast 2
21:16:28.783 [block-manager-storage-async-thread-pool-12] DEBUG org.apache.spark.storage.BlockManager - Removing broadcast 2
21:16:28.783 [block-manager-storage-async-thread-pool-12] DEBUG org.apache.spark.storage.BlockManager - Removing block broadcast_2_piece0
21:16:28.783 [block-manager-storage-async-thread-pool-12] DEBUG org.apache.spark.storage.memory.MemoryStore - Block broadcast_2_piece0 of size 13783 dropped from memory (free 4391463860)
21:16:28.783 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.BlockManagerMasterEndpoint - Updating block info on master broadcast_2_piece0 for BlockManagerId(driver, 192.168.31.69, 58425, None)
21:16:28.783 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_2_piece0 on 192.168.31.69:58425 in memory (size: 13.5 KiB, free: 4.1 GiB)
21:16:28.784 [block-manager-storage-async-thread-pool-12] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_2_piece0
21:16:28.784 [block-manager-storage-async-thread-pool-12] DEBUG org.apache.spark.storage.BlockManager - Told master about block broadcast_2_piece0
21:16:28.784 [block-manager-storage-async-thread-pool-12] DEBUG org.apache.spark.storage.BlockManager - Removing block broadcast_2
21:16:28.784 [block-manager-storage-async-thread-pool-12] DEBUG org.apache.spark.storage.memory.MemoryStore - Block broadcast_2 of size 29264 dropped from memory (free 4391493124)
21:16:28.784 [block-manager-storage-async-thread-pool-14] DEBUG org.apache.spark.storage.BlockManagerStorageEndpoint - Done removing broadcast 2, response is 0
21:16:28.785 [block-manager-storage-async-thread-pool-14] DEBUG org.apache.spark.storage.BlockManagerStorageEndpoint - Sent response: 0 to 192.168.31.69:58424
