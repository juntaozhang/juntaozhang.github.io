# RDD

## RDD ç®€è¿°

**1. compute(split: Partition, context: TaskContext): Iterator[T]**
- **ä½œç”¨**: RDDæœ€æ ¸å¿ƒçš„æ–¹æ³•ï¼Œå®šä¹‰å¦‚ä½•è®¡ç®—æ¯ä¸ªåˆ†åŒºçš„æ•°æ®
- **å®ç°**: ç”±å…·ä½“RDDå­ç±»å®ç°ï¼Œå¦‚MapPartitionsRDDã€ShuffledRDDç­‰
- **è°ƒç”¨**: é€šè¿‡iterator()é—´æ¥è°ƒç”¨ï¼Œå®é™…æ‰§è¡Œæ•°æ®è®¡ç®—é€»è¾‘

**2. getPartitions: Array[Partition]**
- **ä½œç”¨**: è¿”å›RDDçš„æ‰€æœ‰åˆ†åŒºä¿¡æ¯ï¼Œå®šä¹‰æ•°æ®å¦‚ä½•åˆ†å‰²
- **å®ç°**: å„RDDå­ç±»æ ¹æ®æ•°æ®æºç‰¹æ€§å®ç°ï¼ˆå¦‚æ–‡ä»¶åˆ†ç‰‡ã€èŒƒå›´åˆ†åŒºç­‰ï¼‰
- **è°ƒç”¨**: åœ¨ä½œä¸šè§„åˆ’é˜¶æ®µç¡®å®šä»»åŠ¡æ•°é‡ï¼Œ`rdd.partitions.length`å³ä¸ºä»»åŠ¡æ•°

**3. getDependencies: Seq[Dependency[_]]**
- **ä½œç”¨**: è¿”å›å½“å‰RDDå¯¹çˆ¶RDDçš„ä¾èµ–å…³ç³»ï¼ˆçª„ä¾èµ–æˆ–å®½ä¾èµ–ï¼‰
- **å®ç°**: å®šä¹‰è¡€ç»Ÿå…³ç³»ï¼Œç”¨äºDAGæ„å»ºå’Œæ•…éšœæ¢å¤
- **è°ƒç”¨**: DAGScheduleræ ¹æ®ä¾èµ–å…³ç³»åˆ’åˆ†Stageè¾¹ç•Œ

**4. iterator(split: Partition, context: TaskContext): Iterator[T]**
- **ä½œç”¨**: è·å–æŒ‡å®šåˆ†åŒºçš„æ•°æ®è¿­ä»£å™¨ï¼Œå¤„ç†ç¼“å­˜å’Œcheckpointé€»è¾‘
- **å®ç°**: å…ˆæ£€æŸ¥ç¼“å­˜ï¼Œå†æ£€æŸ¥checkpointï¼Œæœ€åè°ƒç”¨compute()
- **è°ƒç”¨**: åœ¨Executorç«¯å®é™…è·å–æ•°æ®æ—¶ä½¿ç”¨

**æ–¹æ³•è°ƒç”¨å…³ç³»**ï¼š
```mermaid
graph TD
    subgraph Driver["Driver ç«¯"]
        A[Action æ“ä½œè§¦å‘] --> B[SparkContext.runJob]
        B --> C[DAGScheduler.submitJob]
        C --> D[éå† RDD Graph]

        subgraph StageGraph["ğŸ“Š Stage åˆ’åˆ†é˜¶æ®µ"]
            D --> E[getDependencies<br/>åˆ†æä¾èµ–å…³ç³»]
            E --> F{ä¾èµ–ç±»å‹åˆ¤æ–­}
            F -->|çª„ä¾èµ–| G[åˆå¹¶åˆ°åŒä¸€ Stage]
            F -->|å®½ä¾èµ–| H[åˆ›å»ºæ–° Stage è¾¹ç•Œ]
            G --> I[getPartitions<br/>ç¡®å®š Stage å†…ä»»åŠ¡æ•°]
            H --> I
        end

        I --> J[ç”Ÿæˆ TaskSet]
        J --> K[TaskScheduler æäº¤ä»»åŠ¡]
    end

    subgraph Executors["Executor ç«¯"]
        K --> L[æ¥æ”¶ Task æ‰§è¡Œ]

        subgraph TaskExec["Task æ‰§è¡Œé˜¶æ®µ"]
            L --> M[iterator è·å–åˆ†åŒºæ•°æ®]
            M --> N{æ£€æŸ¥ç¼“å­˜}
            N -->|æœ‰ç¼“å­˜| O[ä» MemoryStore è¯»å–]
            N -->|æ— ç¼“å­˜| P{æ£€æŸ¥ Checkpoint}
            P -->|æœ‰ Checkpoint| Q[ä»æŒä¹…åŒ–å­˜å‚¨è¯»å–]
            P -->|æ—  Checkpoint| R[compute è®¡ç®—åˆ†åŒº]
            R --> S[é€’å½’è°ƒç”¨çˆ¶ RDD.iterator]
            S --> M
        end

        O --> T[è¿”å›æ•°æ®è¿­ä»£å™¨]
        Q --> T
        R --> T
    end

    style A fill:#e1f5fe
    style E fill:#ffeb3b
    style I fill:#ffeb3b
    style R fill:#fff3e0
    style T fill:#e8f5e8

    classDef driverStyle fill:#e3f2fd,stroke:#1976d2,stroke-width:2px
    classDef executorStyle fill:#f1f8e9,stroke:#388e3c,stroke-width:2px
    classDef stageStyle fill:#fce4ec,stroke:#c2185b,stroke-width:2px

    class Driver driverStyle
    class Executors executorStyle
    class StageGraph stageStyle
```
RDD çš„ç”Ÿå‘½å‘¨æœŸä¸è°ƒåº¦è¿‡ç¨‹ï¼š
- æ„å»º DAGï¼šç”± transformations æ„å»ºä¾èµ–å›¾ï¼›
- åˆ’åˆ† Stageï¼šæ ¹æ®å®½ä¾èµ–åˆ† stageï¼›
- ç”Ÿæˆ TaskSetï¼šå¯¹æ¯ä¸ª partition ç”Ÿæˆ ä¸€ä¸ªtaskï¼› 
- Task è°ƒåº¦æ‰§è¡Œï¼šDAGScheduler è´Ÿè´£ Stage è°ƒåº¦ï¼ŒTaskScheduler è´Ÿè´£ Task åˆ†é…ç»™ Executorï¼Œ
SchedulerBackend è´Ÿè´£ Task ä¸‹å‘ç»™ Executor
- ç¼“å­˜ç®¡ç†ï¼ˆcache/persistï¼‰ï¼šé€šè¿‡å†…å­˜ç®¡ç†æ¨¡å—ä¿å­˜ä¸­é—´æ•°æ®ï¼›
- å¤±è´¥æ¢å¤ï¼šTask å¤±è´¥æ—¶é‡æ–°æ‰§è¡Œï¼Œæ•°æ®ä¸¢å¤±æ—¶æ ¹æ® lineage é‡æ–°è®¡ç®—ï¼›

## RDD ç±»å‹
- **Transformation**ï¼šå¦‚mapã€filterã€reduceByKeyç­‰ï¼Œå®šä¹‰RDDçš„è½¬æ¢æ“ä½œ
- **Action**ï¼šå¦‚collectã€countã€saveAsTextFileç­‰ï¼Œè§¦å‘å®é™…è®¡ç®—å¹¶è¿”å›ç»“æœ

## å®½ä¾èµ– vs çª„ä¾èµ–
- **å®½ä¾èµ–ï¼ˆWide Dependencyï¼‰**: ä¸€ä¸ª RDD çš„åˆ†åŒºä¾èµ–äºå¤šä¸ªçˆ¶ RDD çš„åˆ†åŒºï¼Œé€šå¸¸ä¼šå¼•å‘ Shuffle æ“ä½œ
- **çª„ä¾èµ–ï¼ˆNarrow Dependencyï¼‰**: ä¸€ä¸ª RDD çš„åˆ†åŒºä»…ä¾èµ–äºå•ä¸ªçˆ¶ RDD çš„åˆ†åŒºï¼Œå¯ä»¥åœ¨åŒä¸€ä¸ª Stage å†…å®Œæˆè®¡ç®—


# Q&A

## 1. RDD æ ¸å¿ƒæ¦‚å¿µä¸åŸºç¡€

### RDD æ‡’æ‰§è¡Œæœºåˆ¶çš„ä¼˜åŠ¿æ˜¯ä»€ä¹ˆï¼Ÿ

**æ ¸å¿ƒä¼˜åŠ¿**ï¼š
- **ä¼˜åŒ–æœºä¼š**: å»¶è¿Ÿåˆ° Action æ—¶æ‰æ‰§è¡Œï¼Œå¯ä»¥è¿›è¡Œå…¨å±€ä¼˜åŒ–ï¼ˆå¦‚ç®¡é“åŒ–ã€è°“è¯ä¸‹æ¨ï¼‰
- **èµ„æºèŠ‚çº¦**: é¿å…ä¸å¿…è¦çš„ä¸­é—´ç»“æœå­˜å‚¨
- **å®¹é”™èƒ½åŠ›**: é€šè¿‡è¡€ç»Ÿé‡æ–°è®¡ç®—ï¼Œæ— éœ€æŒä¹…åŒ–ä¸­é—´çŠ¶æ€

**ç®¡é“åŒ–ä¼˜åŒ–**: å¤šä¸ªçª„ä¾èµ–è½¬æ¢ï¼ˆmapâ†’filterâ†’mapï¼‰åœ¨åŒä¸€ Task ä¸­ä¸²è¡Œæ‰§è¡Œï¼Œæ•°æ®æµå¼å¤„ç†ã€‚

### RDD æ˜¯å¦çº¿ç¨‹å®‰å…¨ï¼Ÿ

| ç»„ä»¶ | çº¿ç¨‹å®‰å…¨æ€§        | é£é™©            |
|-----|--------------|---------------|
| **RDD å…ƒæ•°æ®è¯»å–** | âœ… å®‰å…¨         | ä¸å¯å˜å¯¹è±¡         |
| **Action æ“ä½œ** | âš ï¸ä½¿ç”¨AsyncRDDActions | checkpoint å¤±æ•ˆ |
| **Transformation æ“ä½œ** | âœ… å®‰å…¨         | åˆ›å»ºæ–° RDDï¼Œæ— å‰¯ä½œç”¨  |
| **ç´¯åŠ å™¨æ“ä½œ** | âœ… å®‰å…¨         | å†…éƒ¨æœ‰åŒæ­¥æœºåˆ¶       |
| **å¹¿æ’­å˜é‡è¯»å–** | âœ… å®‰å…¨         | åªè¯»è®¿é—®          |

```scala
// RDD æ˜¯ä¸å¯å˜å¯¹è±¡ï¼Œå¤šçº¿ç¨‹è¯»å–å®Œå…¨å®‰å…¨
val rdd = sc.parallelize(1 to 1000)
val threads = (1 to 10).map { i =>
  new Thread(() => {
    println(s"Thread $i: partitions = ${rdd.getNumPartitions}")  // å®‰å…¨
    println(s"Thread $i: dependencies = ${rdd.dependencies}")   // å®‰å…¨
  })
}
threads.foreach(_.start())  // å®Œå…¨å®‰å…¨
```

### SparkContext éçº¿ç¨‹å®‰å…¨å—ï¼Ÿåœ¨å¤šçº¿ç¨‹ç¯å¢ƒä¸­å¹¶å‘æ“ä½œåŒä¸€ä¸ª RDD ä¼šæœ‰ä»€ä¹ˆé—®é¢˜ï¼Ÿ
SparkContext æœ¬èº«è®¾è®¡ä¸Šä¸æ˜¯çº¿ç¨‹å®‰å…¨çš„ï¼Œä½†æ˜¯å¤šçº¿ç¨‹å¹¶å‘actionä¸å½±å“ç»“æœã€‚

æºç åˆ†æç»“è®ºï¼š
**runJob vs submitJob**ï¼š
```text
runJob = submitJob + progressBar.finishAll() + doCheckpoint()
progressBar.finishAll() å†…éƒ¨æœ‰ synchronized ä¿æŠ¤
```

ä½¿ç”¨runJobï¼š
```scala
rdd1.checkpoint()
val thread1 = new Thread(() => rdd1.collect())
val thread2 = new Thread(() => rdd1.count())
thread1.start()
thread2.start()
```

ä½¿ç”¨ AsyncRDDActionsï¼Œç›´æ¥ä½¿ç”¨submitJobï¼Œä¼šå¿½ç•¥ doCheckpointï¼š
```scala
import org.apache.spark.rdd.AsyncRDDActions
rdd1.checkpoint()
val future1 = rdd1.collectAsync()  // åº•å±‚ä»ç”¨ submitJobï¼Œä½†åŒ…è£…ä¸º Future
val future2 = rdd1.countAsync()
val results = Seq(future1.get(), future2.get())
```

## 2. RDD è½¬æ¢æ“ä½œ (Transformations)

### groupByKey() å’Œ reduceByKey() éƒ½ä¼šäº§ç”Ÿå®½ä¾èµ–ï¼Œä½†ä¸ºä»€ä¹ˆé€šå¸¸æ¨èä½¿ç”¨ reduceByKey() è€Œä¸æ˜¯ groupByKey()ï¼Ÿ
 
æ ¸å¿ƒå·®å¼‚ï¼š\
è™½ç„¶ä¸¤è€…éƒ½äº§ç”Ÿå®½ä¾èµ–å¹¶è§¦å‘ Shuffleï¼Œä½†å…³é”®åŒºåˆ«åœ¨äº `reduceByKey` æ”¯æŒ **Map-side Combine**ï¼ˆåœ¨ Map é˜¶æ®µè¿›è¡Œé¢„èšåˆï¼Œå‡å°‘ Shuffle æ•°æ®é‡çš„ä¼˜åŒ–æŠ€æœ¯ï¼‰ é¢„èšåˆï¼Œè€Œ `groupByKey` é»˜è®¤ç¦ç”¨æ­¤ä¼˜åŒ–ã€‚

æŠ€æœ¯åŸç†ï¼š
- `reduceByKey`: èšåˆæ“ä½œï¼ˆå¦‚æ±‚å’Œã€å–æœ€å¤§å€¼ï¼‰ï¼ŒMap ç«¯é¢„èšåˆå¯å°† 1000ä¸‡æ¡è®°å½•å‹ç¼©åˆ° 100ä¸‡æ¡
- `groupByKey`: æ”¶é›†æ“ä½œï¼ˆcollect_listï¼‰ï¼Œé¢„èšåˆæ— æ³•å‡å°‘æ•°æ®é‡ï¼Œåè€Œå¢åŠ  CompactBuffer å¼€é”€

æ€§èƒ½å¯¹æ¯”ï¼š\
åœ¨å…¸å‹åœºæ™¯ä¸‹ï¼Œ`reduceByKey` æ¯” `groupByKey` å¿« 5-10 å€ï¼Œä¸»è¦ä½“ç°åœ¨ Shuffle é˜¶æ®µçš„ç½‘ç»œ I/O å’Œå†…å­˜ä½¿ç”¨ä¸Šã€‚

å®è·µå»ºè®®ï¼š\
é¿å… `groupByKey().mapValues(_.sum)` è¿™ç§åæ¨¡å¼ï¼Œåº”ç›´æ¥ä½¿ç”¨ `reduceByKey(_ + _)`ã€‚

### reduceByKey vs reduceByKeyLocally æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ
`rdd.reduceByKeyLocally(_ + _)` ç­‰ä»·äº `rdd.reduceByKey(_ + _).collect().toMap`ï¼Œå®é™…ä¸Šå°±æ˜¯ä¸ªè¯­æ³•ç³–ï¼Œ\
æœ€ç»ˆç»“æœé›†è¦è¾ƒå°ï¼Œéœ€è¦åœ¨ Driver ç«¯ç›´æ¥ä½¿ç”¨ï¼Œä¸¤ç§æƒ…å†µå¦‚æœç»“æœé›†è¿‡å¤§éƒ½ä¼šå¯¼è‡´ OOMã€‚
**æ ¸å¿ƒå·®å¼‚ï¼š**

| å¯¹æ¯”ç»´åº¦ | reduceByKey | reduceByKeyLocally |
|---------|-------------|-------------------|
| **è¿”å›ç±»å‹** | `RDD[(K, V)]` | `Map[K, V]` |
| **æ“ä½œç±»å‹** | Transformationï¼ˆæ‡’æ‰§è¡Œï¼‰ | Actionï¼ˆç«‹å³æ‰§è¡Œï¼‰ |

### map vs mapPartitions æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ

**æ ¸å¿ƒå·®å¼‚**ï¼š`map` é€æ¡å¤„ç†ï¼Œ`mapPartitions` æŒ‰åˆ†åŒºæ‰¹é‡å¤„ç†ã€‚

**ä½¿ç”¨åœºæ™¯**ï¼š
- **mapPartitions**: IO å¯†é›†å‹ï¼ˆæ•°æ®åº“å†™å…¥ã€æ–‡ä»¶æ“ä½œï¼‰ï¼Œæ¯ä¸ªåˆ†åŒºåªå»ºç«‹ä¸€æ¬¡è¿æ¥
- **map**: CPU å¯†é›†å‹ï¼ˆæ•°å­¦è®¡ç®—ã€ç®€å•è½¬æ¢ï¼‰ï¼Œé¿å…åˆ†åŒºè¿‡å¤§ OOM

**å…¸å‹ä¾‹å­**ï¼š
```scala
// é”™è¯¯ï¼šæ¯æ¡è®°å½•éƒ½åˆ›å»ºæ•°æ®åº“è¿æ¥
rdd.map { record =>
  val conn = DriverManager.getConnection(url)  // åˆ›å»ºè¿æ¥å¼€é”€å¤§
  conn.execute(s"INSERT INTO table VALUES($record)")
}

// æ­£ç¡®ï¼šæ¯ä¸ªåˆ†åŒºåªåˆ›å»ºä¸€æ¬¡è¿æ¥
rdd.mapPartitions { partition =>
  val conn = DriverManager.getConnection(url)  // åˆ†åŒºçº§åˆ«å¤ç”¨
  partition.map(record => conn.execute(s"INSERT INTO table VALUES($record)"))
}
```

### mapPartitions å¯èƒ½ä¼šå¯¼è‡´ä»€ä¹ˆé—®é¢˜ï¼Ÿå¦‚ä½•é¿å…ï¼Ÿ

**ä¸»è¦é—®é¢˜**ï¼š
- **OOM**: æ•´ä¸ªåˆ†åŒºæ•°æ®åŠ è½½åˆ°å†…å­˜
- **èµ„æºæ³„éœ²**: æ•°æ®åº“è¿æ¥ç­‰èµ„æºä¸åˆ†åŒºç”Ÿå‘½å‘¨æœŸç»‘å®šï¼Œå¤„ç†æ—¶é—´é•¿å¯¼è‡´èµ„æºè€—å°½
- **æ•°æ®å€¾æ–œ**: å¤§åˆ†åŒºä¼šæ”¾å¤§ä¸Šè¿°é—®é¢˜

**è§£å†³æ–¹æ¡ˆ**ï¼š
```scala
// 1. æ§åˆ¶åˆ†åŒºå¤§å°
rdd.repartition(partitionCount)  // é¿å…å•åˆ†åŒºè¿‡å¤§

// 2. èµ„æºå®‰å…¨é‡Šæ”¾
rdd.mapPartitions { partition =>
  val conn = DriverManager.getConnection(url)
  try {
    partition.map(processRecord)
  } finally {
    conn.close()  // ç¡®ä¿èµ„æºé‡Šæ”¾
  }
}

// 3. åˆ†å‰²æˆå°æ‰¹
rdd.mapPartitions { partition =>
  partition.grouped(batchSize).flatMap { batch =>
    val conn = connectionPool.getConnection()  // æ¯æ‰¹æ¬¡è·å–è¿æ¥
    try {
      processBatch(batch, conn)
    } finally {
      connectionPool.returnConnection(conn)  // åŠæ—¶å½’è¿˜è¿æ¥æ± 
    }
  }
}
```

### coalesce vs repartition æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ
- `repartition` å°±æ˜¯ `coalesce(numPartitions, shuffle = true)` çš„ç®€å†™ã€‚
- `coalesce` é»˜è®¤ä¸ shuffleï¼Œåªèƒ½å‡å°‘åˆ†åŒºæ•°ï¼›`repartition` å¼ºåˆ¶ shuffleï¼Œå¯å¢å¯å‡åˆ†åŒºæ•°ã€‚
- å‡å°‘åˆ†åŒºç”¨ `coalesce`ï¼ˆæ€§èƒ½å¥½ï¼‰ï¼Œå¢åŠ åˆ†åŒºæˆ–éœ€è¦æ•°æ®å‡è¡¡ç”¨ `repartition`ã€‚
  - rdd.coalesce(2000)     // âŒ æ— æ•ˆï¼Œåˆ†åŒºæ•°ä¸ä¼šå¢åŠ 
  - rdd.repartition(2000)  // âœ… æœ‰æ•ˆï¼Œå¼ºåˆ¶å¢åŠ åˆ†åŒº

### sortByKey vs repartitionAndSortWithinPartitions æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ

**çŸ¥è¯†ç‚¹ï¼š**
- **sortByKey**: æ€»æ˜¯åˆ›å»ºæ–°çš„ RangePartitionerï¼Œå¼ºåˆ¶è§¦å‘ Shuffle
- **repartitionAndSortWithinPartitions**: æ™ºèƒ½åˆ¤æ–­åˆ†åŒºå™¨ï¼Œç›¸åŒæ—¶é¿å… Shuffle
- **ExternalSorter**: Spark å†…éƒ¨æ’åºå™¨ï¼Œæ”¯æŒå†…å­˜+ç£ç›˜çš„å¤–éƒ¨æ’åº

**æ ¸å¿ƒå·®å¼‚ï¼š**
```scala
// sortByKey æºç ï¼šæ€»æ˜¯åˆ›å»º ShuffledRDD
def sortByKey(): RDD[(K, V)] = {
  val part = new RangePartitioner(numPartitions, self, ascending)
  new ShuffledRDD[K, V, V](self, part)  // ğŸš¨ ä¸€å®šä¼š Shuffle
}

// repartitionAndSortWithinPartitions æºç ï¼šæ™ºèƒ½ä¼˜åŒ–
def repartitionAndSortWithinPartitions(partitioner: Partitioner): RDD[(K, V)] = {
  if (self.partitioner == Some(partitioner)) {
    // âœ… åˆ†åŒºå™¨ç›¸åŒï¼Œåªåšåˆ†åŒºå†…æ’åºï¼Œæ—  Shuffle
    self.mapPartitions(iter => new ExternalSorter(...).insertAll(iter))
  } else {
    // âŒ åˆ†åŒºå™¨ä¸åŒï¼Œéœ€è¦ Shuffle
    new ShuffledRDD[K, V, V](self, partitioner)
  }
}
```

**Spark 1.6 ä¼˜åŒ–å®è·µï¼š**
```scala
// HBase Bulk Load åœºæ™¯ä¼˜åŒ–
val regions = getHBaseRegionBoundaries()  // é¢„å…ˆè·å–åˆ†åŒºè¾¹ç•Œ
val hbasePartitioner = new CustomHBasePartitioner(regions)

// é”™è¯¯åšæ³•ï¼šé‡å¤ Shuffle
val step1 = rawRDD.partitionBy(hbasePartitioner)  // Shuffle 1
val step2 = step1.sortByKey()                     // Shuffle 2 (é‡æ–°åˆ†åŒº)

// æ­£ç¡®åšæ³•ï¼šé¿å…é‡å¤ Shuffle
val step1 = rawRDD.partitionBy(hbasePartitioner)                    // Shuffle 1
val step2 = step1.repartitionAndSortWithinPartitions(hbasePartitioner) // æ—  Shuffle
```

## 3. RDD è¡ŒåŠ¨æ“ä½œ (Actions)

### countAsync() vs count()

**æ ¸å¿ƒåŒºåˆ«**ï¼šåŒæ­¥é˜»å¡ vs å¼‚æ­¥éé˜»å¡æ‰§è¡Œ

```scala
import scala.concurrent.Future
import org.apache.spark.FutureAction

// åŒæ­¥æ‰§è¡Œ - é˜»å¡å½“å‰çº¿ç¨‹
val count1: Long = rdd.count()  // ç­‰å¾…å®Œæˆåè¿”å›ç»“æœ

// å¼‚æ­¥æ‰§è¡Œ - ç«‹å³è¿”å› Future
val futureCount: FutureAction[Long] = rdd.countAsync()  // ç«‹å³è¿”å›
val count2: Long = futureCount.get()  // æ‰‹åŠ¨ç­‰å¾…ç»“æœ
```

**ä½¿ç”¨åœºæ™¯å¯¹æ¯”**ï¼š

| å¯¹æ¯”ç»´åº¦ | count() | countAsync() |
|---------|---------|--------------|
| **æ‰§è¡Œæ–¹å¼** | åŒæ­¥é˜»å¡ | å¼‚æ­¥éé˜»å¡ |
| **è¿”å›ç±»å‹** | `Long` | `FutureAction[Long]` |
| **çº¿ç¨‹å½±å“** | é˜»å¡è°ƒç”¨çº¿ç¨‹ | ä¸é˜»å¡è°ƒç”¨çº¿ç¨‹ |
| **é€‚ç”¨åœºæ™¯** | ç®€å•é¡ºåºæ‰§è¡Œ | å¹¶å‘æ‰§è¡Œå¤šä¸ªæ“ä½œ |

**å®é™…åº”ç”¨ç¤ºä¾‹**ï¼š
```scala
// count1 ä¸ count2 ä¸²è¡Œæ‰§è¡Œï¼Œæ€»è€—æ—¶ â‰ˆ å„RDDæ‰§è¡Œæ—¶é—´ä¹‹å’Œ
val count1 = rdd1.count()
val count2 = rdd2.count()

// å¹¶å‘æ‰§è¡Œï¼Œæ€»è€—æ—¶ â‰ˆ max(å„RDDæ‰§è¡Œæ—¶é—´)
val future1 = rdd1.countAsync()  // ç«‹å³å¯åŠ¨
val future2 = rdd2.countAsync()  // ç«‹å³å¯åŠ¨

// ç­‰å¾…æ‰€æœ‰ç»“æœ
val count1 = future1.get()
val count2 = future2.get()
```

**æ³¨æ„äº‹é¡¹**ï¼š
- `countAsync()` åŸºäº Scala Futureï¼Œéœ€è¦éšå¼ ExecutionContext
- é€‚ç”¨äºéœ€è¦å¹¶å‘æ‰§è¡Œå¤šä¸ªç‹¬ç«‹ RDD æ“ä½œçš„åœºæ™¯
- å¯ä»¥ä½¿ç”¨ `futureAction.cancel()` å–æ¶ˆæ­£åœ¨æ‰§è¡Œçš„å¼‚æ­¥æ“ä½œ

### å¤šçº¿ç¨‹ç¯å¢ƒä¸­å¹¶å‘æ“ä½œåŒä¸€ä¸ª RDD ä¼šæœ‰ä»€ä¹ˆé—®é¢˜ï¼Ÿ

SparkContext æœ¬èº«è®¾è®¡ä¸Šä¸æ˜¯çº¿ç¨‹å®‰å…¨çš„ï¼Œä½†æ˜¯å¤šçº¿ç¨‹å¹¶å‘actionä¸å½±å“ç»“æœã€‚

#### æºç åˆ†æç»“è®ºï¼š
**runJob vs submitJob**ï¼š
```text
runJob = submitJob + progressBar.finishAll() + doCheckpoint()
progressBar.finishAll() å†…éƒ¨æœ‰ synchronized ä¿æŠ¤
```

ä½¿ç”¨runJobï¼š
```scala
rdd1.checkpoint()
val thread1 = new Thread(() => rdd1.collect())
val thread2 = new Thread(() => rdd1.count())
thread1.start()
thread2.start()
```

ä½¿ç”¨ AsyncRDDActionsï¼Œç›´æ¥ä½¿ç”¨submitJobï¼Œä¼šå¿½ç•¥ doCheckpointï¼š
```scala
import org.apache.spark.rdd.AsyncRDDActions
rdd1.checkpoint()
val future1 = rdd1.collectAsync()  // åº•å±‚ä»ç”¨ submitJobï¼Œä½†åŒ…è£…ä¸º Future
val future2 = rdd1.countAsync()
val results = Seq(future1.get(), future2.get())
```

## 4. ç¼“å­˜ä¸æŒä¹…åŒ–

### cache vs persist æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ

`cache()` æ˜¯ `persist()` çš„ç®€åŒ–ç‰ˆæœ¬ï¼Œä½¿ç”¨é»˜è®¤å­˜å‚¨çº§åˆ«ï¼š
- **RDD**: `cache() = persist(MEMORY_ONLY)`
- **DataFrame**: `cache() = persist(MEMORY_AND_DISK)`

### Cache ä¼šå½±å“ Job æ‰§è¡Œæ—¶é—´å—ï¼Ÿ

**ä¼šå½±å“ï¼Œä½†åˆ†æƒ…å†µ**ï¼š

- **ç¬¬ä¸€æ¬¡æ‰§è¡Œ**ï¼šæ¯”æœª cache **ç¨æ…¢**ï¼ˆéœ€è¦é¢å¤–å†™å…¥ MemoryStoreï¼‰
- **ç¬¬äºŒæ¬¡æ‰§è¡Œ**ï¼š**æ˜¾è‘—åŠ é€Ÿ**ï¼ˆç›´æ¥è¯»ç¼“å­˜ï¼Œè·³è¿‡ä¸Šæ¸¸è®¡ç®—ï¼‰

**ç»“è®º**ï¼šcache æ˜¯ä¸ºå¤šæ¬¡ä½¿ç”¨è€Œè®¾è®¡çš„ï¼Œå•æ¬¡ä½¿ç”¨åè€Œæœ‰æ€§èƒ½æŸå¤±ã€‚

### checkpoint vs cache æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ

**æœ¬è´¨å·®å¼‚**ï¼šcheckpoint æ˜¯**å®¹é”™æœºåˆ¶**ï¼Œcache æ˜¯**æ€§èƒ½ä¼˜åŒ–**ã€‚

**æ ¸å¿ƒåŒºåˆ«**ï¼š
- **checkpoint**: å†™å…¥å¯é å­˜å‚¨ï¼ˆHDFSï¼‰ï¼Œ**åˆ‡æ–­lineage**ï¼ŒExecutor æŒ‚æ‰ä¹Ÿèƒ½æ¢å¤
- **cache**: å­˜å‚¨åœ¨ Executor å†…å­˜/ç£ç›˜ï¼Œ**ä¿ç•™lineage**ï¼ŒExecutor æŒ‚æ‰éœ€è¦é‡ç®—
- **localCheckpoint**: `persist(MEMORY_AND_DISK)` + **æˆªæ–­è¡€ç»Ÿ**ï¼Œexecutor å¤±è´¥æ—¶æ— æ³•æ¢å¤

**ä½¿ç”¨åœºæ™¯**ï¼š
- **checkpoint**: ML è¿­ä»£è®­ç»ƒï¼ˆè¡€ç»Ÿè¿‡é•¿ã€å¼‚å¸¸é£é™©é«˜ï¼‰ã€å…³é”®ä¸­é—´ç»“æœä¿æŠ¤
- **cache**: å¤šæ¬¡ä½¿ç”¨çš„ RDDï¼ˆå¦‚ç‰¹å¾å·¥ç¨‹åçš„æ•°æ®é›†ï¼‰ã€çŸ­æœŸé‡å¤è®¡ç®—ä¼˜åŒ–

**å…¸å‹ä¾‹å­**ï¼š
```scala
// ML åœºæ™¯ï¼šè¿­ä»£è®­ç»ƒ
val features = rawData.map(preprocess).filter(validate)
features.checkpoint()  // é¿å…æ¯æ¬¡è¿­ä»£é‡æ–°é¢„å¤„ç†ï¼Œè¡€ç»Ÿè¿‡é•¿é£é™©

// ETL åœºæ™¯ï¼šå¤šæ¬¡èšåˆ
val cleanData = rawData.filter(isValid).map(transform)
cleanData.cache()      // åç»­å¤šä¸ªç»Ÿè®¡åˆ†æä¼šé‡å¤ä½¿ç”¨
val stat1 = cleanData.groupBy("region").count()
val stat2 = cleanData.groupBy("category").sum("amount")
```

### æœ‰å‡ ç§ Checkpoint æœºåˆ¶ï¼Ÿ`ReliableCheckpointRDD` å’Œ `LocalCheckpointRDD` çš„é€‚ç”¨åœºæ™¯å’Œå®ç°å·®å¼‚æ˜¯ä»€ä¹ˆï¼Ÿ

**ä¸¤ç§ Checkpoint æœºåˆ¶**ï¼š

**RDD API**ï¼š
```scala
// 1. Reliable Checkpoint - å†™å…¥å¯é å­˜å‚¨ï¼ˆHDFSï¼‰ï¼Œåˆ‡æ–­è¡€ç»Ÿ
rdd.checkpoint()

// 2. Local Checkpoint - persist(MEMORY_AND_DISK) + æˆªæ–­è¡€ç»Ÿ
rdd.localCheckpoint()

// æ‰‹åŠ¨è§¦å‘ Checkpoint, private æ–¹æ³•
rdd.doCheckpoint()
```

**Dataset API**ï¼ˆæ›´ä¸°å¯Œï¼‰ï¼š
```scala
// 1. Reliable Checkpoint
df.checkpoint()                    // eager=trueï¼ˆç«‹å³æ‰§è¡Œï¼‰
df.checkpoint(eager = false)       // å»¶è¿Ÿåˆ°ä¸‹æ¬¡actionæ‰§è¡Œ

// 2. Local Checkpoint
df.localCheckpoint()               // eager=trueï¼ˆç«‹å³æ‰§è¡Œï¼‰
df.localCheckpoint(eager = false)  // å»¶è¿Ÿåˆ°ä¸‹æ¬¡actionæ‰§è¡Œ
```

**å®ç°å·®å¼‚**ï¼š
- **Reliable**: æ•°æ®å®‰å…¨æ€§é«˜ï¼Œä½† I/O å¼€é”€å¤§ï¼Œå®Œå…¨åˆ‡æ–­è¡€ç»Ÿ
- **Local**: persist + æˆªæ–­è¡€ç»Ÿï¼Œexecutor å¤±è´¥æ—¶æ— æ³•æ¢å¤ï¼ˆä¸å¯é ï¼‰
- **eager å‚æ•°**ï¼šåªåœ¨ Dataset API ä¸­æœ‰ï¼Œæ§åˆ¶æ˜¯å¦ç«‹å³æ‰§è¡Œ `doCheckpoint()`

## 5. åˆ†åŒºä¸æ•°æ®å€¾æ–œ

### HashPartitioner vs RangePartitioner é€‚ç”¨åœºæ™¯ï¼Ÿ

**HashPartitioner**:
- é€‚ç”¨äºå‡åŒ€åˆ†å¸ƒçš„ keyï¼Œç®€å•å¿«é€Ÿ
- é£é™©ï¼šçƒ­ç‚¹ key å¯¼è‡´æ•°æ®å€¾æ–œ

**RangePartitioner**:
- é€šè¿‡é‡‡æ ·ç¡®å®šåˆ†åŒºè¾¹ç•Œï¼Œæ•°æ®åˆ†å¸ƒæ›´å‡åŒ€
- é€‚ç”¨äºå¯æ’åºçš„ keyï¼Œé¿å…å€¾æ–œ

### `HashPartitioner` å’Œ `RangePartitioner` åœ¨å¤„ç†æ•°æ®å€¾æ–œæ—¶çš„è¡¨ç°å¦‚ä½•ï¼Ÿ

**HashPartitioner å€¾æ–œé—®é¢˜**ï¼š
```scala
// HashPartitioner: key.hashCode() % numPartitions
def getPartition(key: Any): Int = Utils.nonNegativeMod(key.hashCode, numPartitions)

// é—®é¢˜ï¼šçƒ­ç‚¹keyå¯¼è‡´ä¸¥é‡å€¾æ–œ
// ç¤ºä¾‹ï¼škey="user123" æ€»æ˜¯æ˜ å°„åˆ°åŒä¸€åˆ†åŒºï¼Œé€ æˆè¯¥åˆ†åŒºæ•°æ®é‡å·¨å¤§
```

**RangePartitioner å€¾æ–œè§£å†³æœºåˆ¶**ï¼š
```scala
// 1. æ•°æ®é‡‡æ ·é˜¶æ®µ
def sketch[K](rdd: RDD[K], sampleSizePerPartition: Int): (Long, Array[(Int, Long, Array[K])])
// æ¯åˆ†åŒºé‡‡æ ·sampleSizePerPartitionHintä¸ªæ•°æ®ç‚¹(é»˜è®¤20)

// 2. åˆ†åŒºè¾¹ç•Œè®¡ç®—
// åŸºäºé‡‡æ ·æ•°æ®çš„åˆ†ä½æ•°ç¡®å®šè¾¹ç•Œï¼Œä½¿æ¯ä¸ªåˆ†åŒºæ•°æ®é‡å°½å¯èƒ½å‡åŒ€
val rangeBounds = determineBounds(sketched, partitions)

// 3. åˆ†åŒºåˆ†é…
def getPartition(key: K): Int = {
  // é€šè¿‡äºŒåˆ†æŸ¥æ‰¾ç¡®å®škeyåº”è¯¥åˆ†é…åˆ°å“ªä¸ªåˆ†åŒº
  val partition = binarySearch(rangeBounds, key)
}
```

**æ€§èƒ½å¯¹æ¯”**ï¼š
- **HashPartitioner**: å¿«é€Ÿä½†æ˜“å€¾æ–œï¼Œé€‚ç”¨äºkeyåˆ†å¸ƒå‡åŒ€çš„åœºæ™¯
- **RangePartitioner**: å€¾æ–œå‹å¥½ä½†æœ‰é‡‡æ ·å¼€é”€ï¼Œé€‚ç”¨äºsortByKeyç­‰éœ€è¦æ•°æ®æ’åºçš„åœºæ™¯

### å¦‚ä½•è§£å†³ RDD æ•°æ®å€¾æ–œé—®é¢˜ï¼Ÿ

**è¯†åˆ«æ–¹æ³•**: è§‚å¯Ÿä»»åŠ¡æ‰§è¡Œæ—¶é—´å·®å¼‚å·¨å¤§ï¼ŒæŸäº›åˆ†åŒºæ•°æ®é‡è¿‡å¤§ã€‚

**è§£å†³ç­–ç•¥**:
- **é‡æ–°åˆ†åŒº**: `repartition(n)` æˆ–è‡ªå®šä¹‰åˆ†åŒºå™¨
- **åŠ ç›æŠ€æœ¯**: ç»™çƒ­ç‚¹ key æ·»åŠ éšæœºå‰ç¼€ï¼Œæ‹†åˆ†åå†èšåˆ
- **é¢„èšåˆ**: å…ˆå±€éƒ¨èšåˆå‡å°‘æ•°æ®é‡ï¼Œå†å…¨å±€èšåˆ

### Spark ä¸­å¦‚ä½•å®ç°åˆ†åŒºçº§åˆ«çš„æ•°æ®æœ¬åœ°æ€§ä¼˜åŒ–ï¼Ÿ

**æœ¬åœ°æ€§çº§åˆ«ä¼˜å…ˆçº§**ï¼š
æšä¸¾é¡ºåºï¼šPROCESS_LOCAL(0) < NODE_LOCAL(1) < NO_PREF(2) < RACK_LOCAL(3) < ANY(4)

- PROCESS_LOCAL (0) - æœ€ä¼˜ï¼šè¿›ç¨‹å†…ç¼“å­˜ï¼Œé›¶å»¶è¿Ÿ
- NODE_LOCAL (1) - æ¬¡ä¼˜ï¼šèŠ‚ç‚¹å†…ä¼ è¾“ï¼Œæœ€å°ç½‘ç»œå¼€é”€
- NO_PREF (2) - ä¸­ç­‰ï¼šæ— æ•°æ®ä¾èµ–ï¼Œé›¶ç½‘ç»œä¼ è¾“æˆæœ¬
- RACK_LOCAL (3) - è¾ƒå·®ï¼šè·¨æœºæ¶ä¼ è¾“ï¼Œä¸­ç­‰ç½‘ç»œæˆæœ¬
- ANY (4) - æœ€å·®ï¼šä»»æ„ä½ç½®ï¼Œå¯èƒ½è·¨æ•°æ®ä¸­å¿ƒï¼Œæœ€é«˜ç½‘ç»œæˆæœ¬

NO_PREF ä¸æ˜¯ä¸ºäº†æ€§èƒ½ä¼˜åŒ–ï¼Œè€Œæ˜¯ä¸ºäº†èµ„æºåˆ†é…å…¬å¹³æ€§â€”â€”ç¡®ä¿æœ‰æ•°æ®æœ¬åœ°æ€§éœ€æ±‚çš„ä»»åŠ¡èƒ½å¤Ÿè·å¾—åº”æœ‰çš„èµ„æºï¼Œé¿å…è¢«æ— å…³ä»»åŠ¡æŠ¢å ã€‚

**åœºæ™¯**ï¼šhost1 æœ‰ç©ºé—² Executor Eï¼ŒTaskAï¼ˆNODE_LOCALï¼Œåå¥½ host1ï¼‰ï¼ŒTaskBï¼ˆNO_PREFï¼Œæ— åå¥½ï¼‰
- **ç¬¬1è½®**ï¼šmaxLocality=PROCESS_LOCALï¼ŒTaskB æƒ³è¦è°ƒåº¦ä½†è¢«é˜»æ­¢
- **ç¬¬2è½®**ï¼šmaxLocality=NODE_LOCALï¼ŒTaskA æˆåŠŸè°ƒåº¦åˆ° host1ï¼Œè·å¾—æ•°æ®æœ¬åœ°æ€§
- **ç¬¬3è½®**ï¼šmaxLocality=NO_PREFï¼ŒTaskB åœ¨å…¶ä»– executor ä¸Šè°ƒåº¦
- **ç»“æœ**ï¼šé¿å…äº† TaskB æŠ¢å æœ‰æœ¬åœ°æ€§ä»·å€¼çš„ executorï¼Œç¡®ä¿ Task A è·å¾—æœ€ä¼˜æ€§èƒ½

**å®é™…è°ƒåº¦æµç¨‹**ï¼š
- **ä¼˜å…ˆçº§è°ƒåº¦**: å…ˆå°è¯• PROCESS_LOCALï¼Œå¤±è´¥åé€çº§é™çº§
- **ç­‰å¾…è¶…æ—¶**: æ¯ä¸ªçº§åˆ«ç­‰å¾… `spark.locality.wait` æ—¶é—´ï¼ˆé»˜è®¤3sï¼‰
- **åŠ¨æ€è°ƒæ•´**: executor å¢å‡æ—¶é‡æ–°è®¡ç®—æœ¬åœ°æ€§çº§åˆ«

## 6. æ€§èƒ½ä¼˜åŒ–ä¸æœºåˆ¶

### ä¸ºä»€ä¹ˆ DataFrame/Dataset é€šå¸¸æ¯” RDD æ€§èƒ½æ›´å¥½ï¼ŸCatalyst ä¼˜åŒ–å™¨æ— æ³•ä¼˜åŒ– RDD æ“ä½œçš„æ ¹æœ¬åŸå› æ˜¯ä»€ä¹ˆï¼Ÿ

**RDD é™åˆ¶**ï¼š
```scala
// RDD æ˜¯é»‘ç›’æ“ä½œï¼ŒCatalyst æ— æ³•æ„ŸçŸ¥å‡½æ•°å†…éƒ¨é€»è¾‘
rdd.map(row => complexTransformation(row))  // æ— æ³•ä¼˜åŒ–
rdd.filter(row => row.age > 30)            // æ— æ³•ä¸‹æ¨
```

**DataFrame/Dataset ä¼˜åŠ¿**ï¼š
```scala
// ç»“æ„åŒ– APIï¼ŒCatalyst å¯ä»¥è¿›è¡Œå…¨é¢ä¼˜åŒ–
df.select("name", "age").filter($"age" > 30)
// è‡ªåŠ¨ä¼˜åŒ–ï¼šè°“è¯ä¸‹æ¨ã€åˆ—è£å‰ªã€ä»£ç ç”Ÿæˆç­‰
```

**æ€§èƒ½å·®å¼‚æ ¹æº**ï¼š
1. **ä»£ç ç”Ÿæˆ**: Catalyst ç”Ÿæˆé«˜æ•ˆçš„ Java ä»£ç ï¼Œé¿å…è™šå‡½æ•°è°ƒç”¨
2. **å†…å­˜å¸ƒå±€**: Tungsten ä½¿ç”¨å †å¤–å†…å­˜å’Œåˆ—å¼å­˜å‚¨
3. **ä¼˜åŒ–å™¨**: æˆæœ¬ä¼°ç®—ã€è§„åˆ™ä¼˜åŒ–ã€ç‰©ç†è®¡åˆ’é€‰æ‹©
4. **å‘é‡åŒ–**: æ‰¹é‡å¤„ç†å¤šè¡Œæ•°æ®ï¼Œæé«˜ CPU åˆ©ç”¨ç‡

**RDD ä»æœ‰ä»·å€¼çš„åœºæ™¯**ï¼š
- éç»“æ„åŒ–æ•°æ®å¤„ç†
- å¤æ‚çš„ç”¨æˆ·è‡ªå®šä¹‰é€»è¾‘
- éœ€è¦ç²¾ç¡®æ§åˆ¶åˆ†åŒºå’Œæœ¬åœ°æ€§

### RDD çš„æ‡’åŠ è½½æœºåˆ¶åœ¨ DAGScheduler ä¸­æ˜¯å¦‚ä½•å®ç°çš„ï¼Ÿå¤šä¸ªè¿ç»­çš„çª„ä¾èµ–è½¬æ¢ï¼ˆå¦‚ `map().filter().map()`ï¼‰æ˜¯å¦‚ä½•åœ¨è¿è¡Œæ—¶è¿›è¡Œç®¡é“åŒ–ä¼˜åŒ–çš„ï¼Ÿ

**æ‡’åŠ è½½è§¦å‘æµç¨‹**ï¼š
```scala
// 1. Transformationï¼šåªæ„å»º RDD Graphï¼Œä¸æ‰§è¡Œè®¡ç®—
rdd.map(f1).filter(f2).map(f3)  // åˆ›å»º3ä¸ª MapPartitionsRDDï¼Œæ„å»ºä¾èµ–é“¾

// 2. Actionï¼šè§¦å‘ DAGScheduler.submitJob()
rdd.collect() â†’ SparkContext.runJob() â†’ DAGScheduler.submitJob()
```

**ç®¡é“åŒ–ä¼˜åŒ–æ ¸å¿ƒ**ï¼š
```scala
// MapPartitionsRDD.compute() - ç®¡é“åŒ–çš„å…³é”®
override def compute(split: Partition, context: TaskContext): Iterator[U] = {
  f(context, split.index, firstParent[T].iterator(split, context))
}

// RDD.iterator() - é€’å½’è°ƒç”¨çˆ¶ RDD
final def iterator(split: Partition, context: TaskContext): Iterator[T] = {
  if (storageLevel != StorageLevel.NONE) {
    getOrCompute(split, context)  // ä»ç¼“å­˜è¯»å–
  } else {
    computeOrReadCheckpoint(split, context)  // é€’å½’è®¡ç®—
  }
}
```

**å®é™…æ‰§è¡Œæ•ˆæœ**ï¼š
- **çª„ä¾èµ–é“¾**ï¼š`ParallelCollectionRDD â†’ MapRDD(f1) â†’ FilterRDD(f2) â†’ MapRDD(f3)`
- **å• Task æ‰§è¡Œ**ï¼š`iter.map(f1).filter(f2).map(f3)`ï¼Œæ•°æ®æµå¼å¤„ç†ï¼Œæ— ä¸­é—´ç‰©åŒ–
- **å†…å­˜æ•ˆç‡**ï¼šæ¯æ¡è®°å½•é€ä¸€é€šè¿‡æ•´ä¸ªè½¬æ¢ç®¡é“ï¼Œå†…å­˜å ç”¨æœ€å°

### ç´¯åŠ å™¨å’Œå¹¿æ’­å˜é‡çš„çº¿ç¨‹å®‰å…¨æ€§ï¼Ÿ
```scala
val acc = sc.longAccumulator()
val broadcast = sc.broadcast(Map("key" -> "value"))

// âœ… å®‰å…¨ï¼šç´¯åŠ å™¨å†…éƒ¨æœ‰åŒæ­¥æœºåˆ¶
rdd.foreachPartition { iter =>
  iter.foreach(_ => acc.add(1))  // çº¿ç¨‹å®‰å…¨
}

// âœ… å®‰å…¨ï¼šå¹¿æ’­å˜é‡åªè¯»è®¿é—®
rdd.map { x =>
  val sharedData = broadcast.value  // çº¿ç¨‹å®‰å…¨
  x + sharedData.size
}
```

## 7. å®¹é”™ä¸æ•…éšœæ¢å¤

### å½“ Shuffle æ–‡ä»¶ä¸¢å¤±æ—¶ï¼ŒSpark å¦‚ä½•é€šè¿‡ RDD lineage è¿›è¡Œæ•…éšœæ¢å¤ï¼Ÿ

**è¡€ç»Ÿæ¢å¤æœºåˆ¶**ï¼š
1. **FetchFailedException** æŠ›å‡ºï¼šä¸‹æ¸¸ Task æ— æ³•è·å– Shuffle æ•°æ®
2. **DAGScheduler å¤„ç†**ï¼š`handleTaskCompletion()` æ£€æµ‹åˆ° FetchFailedException
3. **Stage é‡æäº¤**ï¼šæ ‡è®°ç›¸å…³ ShuffleMapStage ä¸ºå¤±è´¥ï¼Œé‡æ–°æäº¤
4. **è¡€ç»Ÿè¿½æº¯**ï¼šé€’å½’é‡æ–°è®¡ç®—ä¾èµ–çš„çˆ¶ RDDï¼Œç›´åˆ°æ‰¾åˆ°å¯ç”¨æ•°æ®æˆ–æºå¤´

**å¦‚æœæ˜¯ LocalCheckpointï¼ŒExecutor å¤±è´¥æ—¶æ— æ³•æ¢å¤ï¼**

**LocalCheckpoint æ¢å¤æµç¨‹å¯¹æ¯”**ï¼š

| åœºæ™¯ | å¯é  Checkpoint | LocalCheckpoint |
|------|----------------|-----------------|
| **Task å¤±è´¥ (Executorå­˜æ´»)** | âœ… ä» HDFS è¯»å– | âœ… ä»æœ¬åœ°å­˜å‚¨è¯»å– |
| **Executor å¤±è´¥** | âœ… ä» HDFS è¯»å– | âŒ **ç›´æ¥æŠ›å¼‚å¸¸** |
| **æ•°æ®ä¸¢å¤±** | âœ… ä»å¯é å­˜å‚¨æ¢å¤ | âŒ **æ— æ³•æ¢å¤** |

**å…³é”®åŒºåˆ«**ï¼š
- **Task å¤±è´¥**ï¼šå¦‚æœåªæ˜¯å•ä¸ª Task å¼‚å¸¸ï¼Œä½† Executor è¿›ç¨‹ä»å­˜æ´»ï¼ŒLocalCheckpoint æ•°æ®ä»åœ¨ BlockManager ä¸­ï¼Œä¸‹æ¸¸ä½¿ç”¨è€…å¯ä»¥æ­£å¸¸æ¢å¤
- **Executor å¤±è´¥**ï¼šå¦‚æœæ•´ä¸ª Executor è¿›ç¨‹å´©æºƒï¼ŒLocalCheckpoint æ•°æ®éšä¹‹ä¸¢å¤±ï¼Œç”±äºè¡€ç»Ÿå·²æˆªæ–­ï¼Œæ— æ³•é‡æ–°è®¡ç®—

**ä½¿ç”¨å»ºè®®**ï¼š
- **LocalCheckpoint**: ä»…ç”¨äºè¿­ä»£ç®—æ³•çš„æ€§èƒ½ä¼˜åŒ–ï¼Œä¸ä¾èµ–å®¹é”™æ€§
- **å¯é  Checkpoint**: ç”¨äºç”Ÿäº§ç¯å¢ƒå…³é”®æ•°æ®ä¿æŠ¤
- **æ³¨æ„**: LocalCheckpoint åå¦‚æœ Executor å¤±è´¥ï¼Œæ•´ä¸ª Job ä¼šå¤±è´¥

## 8. æ–°ç‰¹æ€§
### Spark 3.1+ å¼•å…¥äº† `ResourceProfile` æœºåˆ¶ï¼Œ`RDD.withResources()` æ–¹æ³•å¦‚ä½•å®ç° RDD çº§åˆ«çš„ç»†ç²’åº¦èµ„æºç®¡ç†ï¼Ÿè¿™ä¸ä¼ ç»Ÿçš„ spark-submit èµ„æºé…ç½®æœ‰ä»€ä¹ˆæœ¬è´¨åŒºåˆ«ï¼Ÿ

```scala
val cpuProfile = new ResourceProfileBuilder()
  .require(new ExecutorResourceRequests().cores(2).memory("4g"))
  .build()

// ETLç®¡é“
val preprocessed = rawData
  .withResources(cpuProfile)
  .map(preprocess)
```
**æ ¸å¿ƒæœºåˆ¶**ï¼š
- **RDD çº§åˆ«èµ„æºå®šä¹‰**ï¼šæ¯ä¸ª RDD å¯ä»¥æœ‰ç‹¬ç«‹çš„ ResourceProfileï¼ŒæŒ‡å®šä¸åŒçš„ CPUã€å†…å­˜ã€GPU éœ€æ±‚
- **åŠ¨æ€ Executor ç”³è¯·**ï¼šDAGScheduler æ ¹æ® Stage çš„ ResourceProfile åŠ¨æ€ç”³è¯·å…·æœ‰ç›¸åº”èµ„æºé…ç½®çš„ Executor
- **å‘åå…¼å®¹**ï¼šæœªæŒ‡å®š ResourceProfile çš„ RDD ä½¿ç”¨é»˜è®¤é…ç½®ï¼ˆæ¥è‡ª spark-submit å‚æ•°ï¼‰

**ä¸ä¼ ç»Ÿé…ç½®çš„åŒºåˆ«**ï¼š
- ä¼ ç»Ÿï¼šæ•´ä¸ªåº”ç”¨ä½¿ç”¨ç»Ÿä¸€èµ„æºé…ç½®ï¼ˆ--executor-cores, --executor-memoryï¼‰
- æ–°æœºåˆ¶ï¼šæ”¯æŒåŒä¸€åº”ç”¨å†…ä¸åŒ RDD ä½¿ç”¨ä¸åŒèµ„æºé…ç½®ï¼Œå®ç°çœŸæ­£çš„ workload-aware èµ„æºåˆ†é…

### RDDBarrieræ˜¯ Spark 2.4+ å¼•å…¥çš„æ–°ç‰¹æ€§ï¼Œå®ƒè§£å†³äº†ä»€ä¹ˆé—®é¢˜ï¼Ÿåœ¨ä»€ä¹ˆåœºæ™¯ä¸‹éœ€è¦ä½¿ç”¨ `rdd.barrier().mapPartitions()`ï¼Ÿ

**è§£å†³çš„é—®é¢˜**ï¼š
- **åˆ†å¸ƒå¼æ·±åº¦å­¦ä¹ **ï¼šéœ€è¦æ‰€æœ‰ worker åŒæ—¶å¯åŠ¨ï¼Œè¿›è¡Œå‚æ•°åŒæ­¥å’Œæ¢¯åº¦èšåˆ
- **é›†ä½“é€šä¿¡**ï¼šMPI é£æ ¼çš„ AllReduceã€AllGather ç­‰æ“ä½œéœ€è¦æ‰€æœ‰ä»»åŠ¡ååŒ
- **åŒæ­¥è®¡ç®—**ï¼šæŸäº›ç®—æ³•è¦æ±‚æ‰€æœ‰åˆ†åŒºå¿…é¡»åŒæ—¶å¤„ç†æ•°æ®


**æ ¸å¿ƒ API è¯´æ˜**ï¼š
- `context.allGather(message: String)`: æ”¶é›†æ‰€æœ‰ä»»åŠ¡çš„æ¶ˆæ¯åˆ°æ¯ä¸ªä»»åŠ¡
- `context.barrier()`: åŒæ­¥å±éšœï¼Œç­‰å¾…æ‰€æœ‰ä»»åŠ¡åˆ°è¾¾æ­¤ç‚¹
- `context.getTaskInfos()`: è·å–æ‰€æœ‰ä»»åŠ¡çš„ä¿¡æ¯ï¼ˆåœ°å€ã€ç«¯å£ç­‰ï¼‰
- `context.partitionId()`: è·å–å½“å‰åˆ†åŒºID

**API è¡Œä¸ºè¯¦è§£**ï¼š

1. **`context.allGather()` æ˜¯é˜»å¡æ“ä½œ**ï¼š
   - ç­‰å¾…æ‰€æœ‰ä»»åŠ¡éƒ½è°ƒç”¨ `allGather` åæ‰è¿”å›
   - æ¯ä¸ªä»»åŠ¡éƒ½ä¼šæ”¶åˆ°æ‰€æœ‰ä»»åŠ¡çš„æ¶ˆæ¯
   - æœ¬èº«å°±æœ‰åŒæ­¥æ•ˆæœ

2. **`context.barrier()` çš„é¢å¤–ä½œç”¨**ï¼š
   - ç¡®ä¿æ‰€æœ‰ä»»åŠ¡éƒ½å®Œæˆäº†æ¢¯åº¦æ›´æ–°æ“ä½œ
   - æä¾›æ˜¾å¼çš„åŒæ­¥ç‚¹ï¼Œå¢å¼ºä»£ç å¯è¯»æ€§
   - å¤„ç†å¼‚å¸¸æƒ…å†µï¼ˆå¦‚æŸä¸ªä»»åŠ¡åœ¨ allGather åå¼‚å¸¸é€€å‡ºï¼‰

**TensorFlow on Spark ç¤ºä¾‹**ï¼š
```scala
val trainingRDD = sc.parallelize(dataset, numWorkers)
val finalModels = trainingRDD.barrier().mapPartitions { batch =>
  val context = BarrierTaskContext.get()
  val workerId = context.partitionId()

  // å¯åŠ¨ TensorFlow worker
  val tf = TensorFlow.create(workerId, context.getTaskInfos())

  for (step <- 1 to maxSteps) {
    // 1. æœ¬åœ°è®­ç»ƒï¼šåŸºäºæœ¬partitionçš„æ•°æ®è®¡ç®—å±€éƒ¨æ¢¯åº¦
    tf.trainStep(batch)  // åªä½¿ç”¨å½“å‰partitionçš„æ•°æ®

    // 2. è·å–å±€éƒ¨æ¢¯åº¦ï¼ˆæ¯ä¸ªworkerç‹¬æœ‰çš„ï¼‰
    val localGradients = tf.getGradients()  // åŸºäºlocal batchè®¡ç®—çš„æ¢¯åº¦

    // 3. AllGather: æ”¶é›†æ‰€æœ‰workerçš„å±€éƒ¨æ¢¯åº¦ï¼ˆé˜»å¡ç­‰å¾…ï¼‰
    val allLocalGradients = context.allGather(serialize(localGradients))
    // allLocalGradients = [grad_worker0, grad_worker1, grad_worker2, grad_worker3]

    // 4. è®¡ç®—å…¨å±€å¹³å‡æ¢¯åº¦
    val globalAvgGradients = average(allLocalGradients.map(deserialize))

    // 5. ä½¿ç”¨å…¨å±€æ¢¯åº¦æ›´æ–°æœ¬åœ°æ¨¡å‹ï¼ˆæ‰€æœ‰workerç”¨ç›¸åŒæ¢¯åº¦æ›´æ–°ï¼‰
    tf.updateWeights(globalAvgGradients)

    // 6. ç¡®ä¿æ‰€æœ‰workeréƒ½å®Œæˆäº†æƒé‡æ›´æ–°ï¼ˆå¯é€‰ï¼Œå› ä¸ºallGatherå·²ç»åŒæ­¥äº†ï¼‰
    context.barrier()
  }

  // 7. åªè¿”å›ä¸€ä¸ªæ¨¡å‹å³å¯ï¼ˆæ‰€æœ‰æ¨¡å‹éƒ½ç›¸åŒï¼‰
  if (workerId == 0) Iterator(tf.getModel()) else Iterator.empty
}

// æ”¶é›†æœ€ç»ˆæ¨¡å‹ï¼ˆåªæœ‰ä¸€ä¸ªï¼Œå› ä¸ºæ‰€æœ‰workerçš„æ¨¡å‹éƒ½ç›¸åŒï¼‰
val finalModel = finalModels.collect().head
```

```
Epoch 1:
Worker 0: batch_0 â†’ local_grad_0 â”
Worker 1: batch_1 â†’ local_grad_1 â”œâ”€ allGather â†’ global_avg_grad â†’ æ›´æ–°æ‰€æœ‰worker
Worker 2: batch_2 â†’ local_grad_2 â”¤                                     â†“
Worker 3: batch_3 â†’ local_grad_3 â”˜                               æ‰€æœ‰æ¨¡å‹ä¿æŒä¸€è‡´
```
